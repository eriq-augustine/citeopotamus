

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:foaf="http://xmlns.com/foaf/0.1/"
      xmlns:dc="http://purl.org/dc/terms/"
      xmlns:doi="http://dx.doi.org/"
      xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
      xmlns:xsd="http://www.w3.org/2001/XMLSchema-datatypes#"
      lang="en" xml:lang="en"
      itemscope itemtype="http://schema.org/Article">

<head>
   <title>PLOS Computational Biology: Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories</title>



<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="X-UA-Compatible" content="IE=EmulateIE7; IE=EmulateIE9" />

<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />
<link rel="home" title="home" href="/home.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" />
<link rel="alternate" type="application/rss+xml"
  title="PLOS Computational Biology: New Articles"
  href="http://www.ploscompbiol.org/article/feed" />

<link rel="pingback" href="http://www.ploscompbiol.org/pingback" />


<!-- global_css.ftl -->
<link rel="stylesheet" type="text/css" media="screen" href="/css/dojo/dijit/themes/tundra/tundra.css?v=zwugcxLdpBWmb72qqlHgGyeNAGM" />
<link rel="stylesheet" type="text/css" media="all" href="/css/star_rating.css?v=Mm3HyaUPUahAJAq6xG5W3tdF7Hk" />
<link rel="stylesheet" type="text/css" media="all" href="/css/rating.css?v=AOT9rozE9sYVjGTdeiyxHf18Ykw" />
<link rel="stylesheet" type="text/css" media="all" href="/css/screen.css?v=KBNFqBmhD635U0NgiM5tf9dtxRE" />
<link rel="stylesheet" type="text/css" media="all" href="/css/iepc.css?v=FIjyR83BhSovq_5yNVk1Sih60AI" />
<link rel="stylesheet" type="text/css" media="all" href="/css/journal.css?v=SiREXztjNSUsOyrlkE9sv028Y+0" />
<link rel="stylesheet" type="text/css" media="all" href="/css/lightBox.css?v=sDgsUXamFoYAakEshdj0YWrZiiQ" />
<link rel="stylesheet" type="text/css" media="print" href="/css/print.css?v=0T+LTV7Gd0jtHEhqEvBfSrmHUGg" />

<!--chartbeat --> 
<script type="text/javascript">var _sf_startpt=(new Date()).getTime()</script>


<!--[if IE]><![endif]-->
<meta name="description" content="PLOS Computational Biology is an open-access" />

<meta name="keywords" content="plos computational biology" />

  <meta name="citation_publisher" content="Public Library of Science" />
  <meta name="citation_doi" content="10.1371/journal.pcbi.1002726" />
    <meta name="citation_title" content="Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories"/>
    <meta itemprop="name" content="Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories"/>

      <meta name="citation_author" content="Iris I. A. Groen" />
          <meta name="citation_author_institution" content="Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands" />
      <meta name="citation_author" content="Sennay Ghebreab" />
          <meta name="citation_author_institution" content="Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands" />
          <meta name="citation_author_institution" content="Intelligent Systems Lab Amsterdam, Institute of Informatics, University of Amsterdam, Amsterdam, The Netherlands" />
      <meta name="citation_author" content="Victor A. F. Lamme" />
          <meta name="citation_author_institution" content="Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands" />
      <meta name="citation_author" content="H. Steven Scholte" />
          <meta name="citation_author_institution" content="Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands" />

    <meta name="citation_date" content="2012/10/18"/>

  <meta name="citation_pdf_url" content="http://dx.plos.org/10.1371/journal.pcbi.1002726.pdf" />

    <meta name="citation_journal_title" content="PLOS Computational Biology" />
    <meta name="citation_firstpage" content="e1002726"/>
    <meta name="citation_issue" content="10"/>
    <meta name="citation_volume" content="8"/>
    <meta name="citation_issn" content="1553-7358"/>

    <meta name="citation_journal_abbrev" content="PLoS Comput Biol" />

      <meta name="citation_reference" content="citation_title=Meaning in visual search; citation_volume=187; citation_number=1; citation_pages=965-966; citation_date=1975; " />
      <meta name="citation_reference" content="citation_title=The briefest of glances: the time course of natural scene understanding; citation_volume=20; citation_number=2; citation_pages=464-472; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=Rapid natural scene categorization in the near absence of attention; citation_volume=99; citation_number=3; citation_pages=9596-9601; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Speed of processing in the human visual system; citation_volume=381; citation_number=4; citation_pages=520-522; citation_date=1996; " />
      <meta name="citation_reference" content="citation_title=Ultra-rapid object detection with saccadic eye movements: visual processing speed revisited; citation_volume=46; citation_number=5; citation_pages=1762-1776; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=The time course of visual processing: from early perception to decision-making; citation_volume=13; citation_number=6; citation_pages=454-461; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Parallel processing in high-level categorization of natural images; citation_volume=5; citation_number=7; citation_pages=629-630; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Neural mechanisms of rapid natural scene categorization in human visual cortex; citation_volume=460; citation_number=8; citation_pages=94-97; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=Relations between the statistics of natural images and the response properties of cortical cells; citation_volume=4; citation_number=9; citation_pages=2379-2394; citation_date=1987; " />
      <meta name="citation_reference" content="citation_title=Sparse coding and decorrelation in primary visual cortex during natural vision; citation_volume=287; citation_number=10; citation_pages=1273-1276; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Natural signal statistics and sensory gain control; citation_volume=4; citation_number=11; citation_pages=819-825; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Emergence of simple-cell receptive field properties by learning a sparse code for natural images; citation_volume=381; citation_number=12; citation_pages=607-610; citation_date=1996; " />
      <meta name="citation_reference" content="citation_title=Emergence of complex cell properties by learning to generalize in natural scenes; citation_volume=457; citation_number=13; citation_pages=83-86; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=Local luminance and contrast in natural images; citation_volume=46; citation_number=14; citation_pages=1585-1598; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Statistics of natural image categories; citation_volume=14; citation_number=15; citation_pages=391-412; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Does the brain perform a Fourier analysis of the visual scene?; citation_volume=2; citation_number=16; citation_pages=207-208; citation_date=1979; " />
      <meta name="citation_reference" content="citation_title=Brain responses strongly correlate with Weibull image statistics when processing natural images; citation_volume=9; citation_number=17; citation_pages=1-15; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=A biologically plausible model for rapid natural image identification; citation_number=18; citation_pages=1-9; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=Building the gist of a scene: the role of global image features in recognition; citation_volume=155; citation_number=19; citation_pages=23-36; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Dynamics of trimming the content of face representations for categorization in the brain; citation_volume=5; citation_number=20; citation_pages=e1000561; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=Quantifying the time course of visual object processing using ERPs: It's time to up the game; citation_volume=2; citation_number=21; citation_pages=1-6; citation_date=2011; " />
      <meta name="citation_reference" content="citation_title=Reliability of ERP and single-trial analyses; citation_volume=58; citation_number=22; citation_pages=620-629; citation_date=2011; " />
      <meta name="citation_reference" content="citation_title=Temporally distinct neural coding of perceptual similarity and prototype bias; citation_volume=10; citation_number=23; citation_pages=1-12; citation_date=2010; " />
      <meta name="citation_reference" content="citation_title=Low-level contrast statistics are diagnostic of invariance of natural textures; citation_volume=6; citation_number=24; citation_pages=34; citation_date=2012; " />
      <meta name="citation_reference" content="citation_title=Representational similarity analysis - connecting the branches of systems neuroscience; citation_volume=2; citation_number=25; citation_pages=4; citation_date=2008; " />
      <meta name="citation_reference" content="citation_title=Real-world scene representations in high-level visual cortex: it's the spaces more than the places; citation_volume=31; citation_number=26; citation_pages=7322-7333; citation_date=2011; " />
      <meta name="citation_reference" content="citation_title=Estimating perception of scene layout properties from global image features; citation_volume=10; citation_number=27; citation_pages=1-25; citation_date=2010; " />
      <meta name="citation_reference" content="citation_title=Effects of occlusion, edges, and scaling on the power spectra of natural images; citation_volume=22; citation_number=28; citation_pages=1789-1797; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Attention and the metric structure of the stimulus space; citation_volume=1; citation_number=29; citation_pages=54-87; citation_date=1964; " />
      <meta name="citation_reference" content="citation_title=Different spatial scales of shape similarity representation in lateral and ventral LOC; citation_volume=19; citation_number=30; citation_pages=2269-2280; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=The representation of perceived shape similarity and its role for category learning in monkeys: a modeling study; citation_volume=48; citation_number=31; citation_pages=598-610; citation_date=2008; " />
      <meta name="citation_reference" content="citation_title=Modeling the shape of the scene: A holistic representation of the spatial envelope; citation_volume=42; citation_number=32; citation_pages=145-175; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Calculating the contrasts that retinal ganglion cells and LGN neurones encounter in natural scenes; citation_volume=40; citation_number=33; citation_pages=3145-3157; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Local contrast in natural images: normalisation and coding efficiency; citation_volume=29; citation_number=34; citation_pages=1041-1055; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Sensitivity to contrast histogram differences in synthetic wavelet-textures; citation_volume=41; citation_number=35; citation_pages=585-598; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=The suppressive field of neurons in lateral geniculate nucleus; citation_volume=25; citation_number=36; citation_pages=10844-10856; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Receptive fields of P and M ganglion cells across the primate retina; citation_volume=35; citation_number=37; citation_pages=7-24; citation_date=1995; " />
      <meta name="citation_reference" content="citation_title=Local scale control for edge detection and blur estimation; citation_volume=20; citation_number=38; citation_pages=699-716; citation_date=1998; " />
      <meta name="citation_reference" content="citation_number=39; " />
      <meta name="citation_reference" content="citation_title=A new method for off-line removal of ocular artifact; citation_volume=55; citation_number=40; citation_pages=468-484; citation_date=1983; " />
      <meta name="citation_reference" content="citation_title=Spherical splines for scalp potential and current density mapping; citation_volume=72; citation_number=41; citation_pages=184-187; citation_date=1989; " />
      <meta name="citation_reference" content="citation_number=42; " />
      <meta name="citation_reference" content="citation_number=43; " />
      <meta name="citation_reference" content="citation_title=Multimodel Inference: Understanding AIC and BIC in Model Selection; citation_volume=33; citation_number=44; citation_pages=261-304; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Matching categorical object representations in inferior temporal cortex of man and monkey; citation_volume=60; citation_number=45; citation_pages=1126-1141; citation_date=2008; " />
      <meta name="citation_reference" content="citation_title=The relation between measures of correlation in the universe of sample permutations; citation_volume=33; citation_number=46; citation_pages=129-135; citation_date=1944; " />
      <meta name="citation_reference" content="citation_title=A technique of nonparametric multivariate analysis; citation_volume=26; citation_number=47; citation_pages=547-558; citation_date=1970; " />
      <meta name="citation_reference" content="citation_title=Confidence intervals from randomization tests; citation_volume=52; citation_number=48; citation_pages=1387-1393; citation_date=1996; " />
      <meta name="citation_reference" content="citation_title=The VideoToolbox software for visual psychophysics: Transforming numbers into movies; citation_volume=10; citation_number=49; citation_pages=437-442; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=The Psychophysics Toolbox; citation_volume=10; citation_number=50; citation_pages=433-436; citation_date=1997; " />
      <meta name="citation_reference" content="citation_title=The speed of categorization in the human visual system; citation_volume=62; citation_number=51; citation_pages=168-170; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=A feedforward architecture accounts for rapid categorization; citation_volume=104; citation_number=52; citation_pages=6424-6429; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=When is scene identification just texture recognition?; citation_volume=44; citation_number=53; citation_pages=2301-2311; citation_date=2004; " />
      <meta name="citation_reference" content="citation_title=Timecourse of neural signatures of object recognition; citation_volume=3; citation_number=54; citation_pages=499-512; citation_date=2003; " />
      <meta name="citation_reference" content="citation_title=Adaptation to statistical properties of visual scenes biases rapid categorization; citation_volume=15; citation_number=55; citation_pages=12-19; citation_date=2007; " />
      <meta name="citation_reference" content="citation_title=From spatial frequency contrast to edge preponderance: the differential modulation of early visual evoked potentials by natural scene stimuli; citation_volume=28; citation_number=56; citation_pages=221-237; citation_date=2011; " />
      <meta name="citation_reference" content="citation_title=Event-related potentials reveal an early advantage for luminance contours in the processing of objects; citation_volume=11; citation_number=57; citation_pages=1-15; citation_date=2011; " />
      <meta name="citation_reference" content="citation_title=Neural representation of task difficulty and decision making during perceptual categorization: A timing diagram; citation_volume=26; citation_number=58; citation_pages=8965-8975; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Time course and robustness of ERP object and face differences; citation_volume=8; citation_number=59; citation_pages=1-18; citation_date=2008; " />
      <meta name="citation_reference" content="citation_title=Spatial scaling factors explain eccentricity effects on face ERPs; citation_volume=5; citation_number=60; citation_pages=755-763; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Cracking the code of oscillatory activity; citation_volume=9; citation_number=61; citation_pages=e1001064; citation_date=2011; " />
      <meta name="citation_reference" content="citation_title=Faces in the cloud: Fourier power spectrum biases ultrarapid face detection; citation_volume=8; citation_number=62; citation_pages=1-13; citation_date=2008; " />
      <meta name="citation_reference" content="citation_title=The time course of visual processing: backward masking and natural scene categorisation; citation_volume=45; citation_number=63; citation_pages=1459-1469; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Temporal characterization of the neural correlates of perceptual decision making in the human brain; citation_volume=16; citation_number=64; citation_pages=509-518; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Parametric study of EEG sensitivity to phase noise during face processing; citation_volume=9; citation_number=65; citation_pages=98; citation_date=2008; " />
      <meta name="citation_reference" content="citation_title=Inverse mapping the neuronal substrates of face categorizations; citation_volume=19; citation_number=66; citation_pages=2428-2438; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=Texture segregation in the human visual cortex: A functional MRI study; citation_volume=83; citation_number=67; citation_pages=2453-2457; citation_date=2000; " />
      <meta name="citation_reference" content="citation_title=Feedforward and recurrent processing in scene segmentation: electroencephalography and functional magnetic resonance imaging; citation_volume=20; citation_number=68; citation_pages=2097-2109; citation_date=2008; " />
      <meta name="citation_reference" content="citation_title=Metamers of the ventral stream; citation_volume=14; citation_number=69; citation_pages=1195-1201; citation_date=2011; " />
      <meta name="citation_reference" content="citation_title=The statistics of natural images; citation_volume=5; citation_number=70; citation_pages=517-548; citation_date=1994; " />
      <meta name="citation_reference" content="citation_title=Preattentive texture discrimination with early vision mechanisms; citation_volume=7; citation_number=71; citation_pages=923-932; citation_date=1990; " />
      <meta name="citation_reference" content="citation_number=72; " />
      <meta name="citation_reference" content="citation_title=Phase noise and the classification of natural images; citation_volume=46; citation_number=73; citation_pages=1520-1529; citation_date=2006; " />
      <meta name="citation_reference" content="citation_title=Localized information is necessary for scene categorization, including the natural/man-made distinction; citation_volume=8; citation_number=74; citation_pages=1-9; citation_date=2008; " />
      <meta name="citation_reference" content="citation_title=How do amplitude spectra influence rapid animal detection?; citation_volume=49; citation_number=75; citation_pages=3001-3012; citation_date=2009; " />
      <meta name="citation_reference" content="citation_title=Rapid visual categorization of natural scene contexts with equalized amplitude spectrum and increasing phase noise; citation_volume=9; citation_number=76; citation_pages=2.1-16; citation_date=2009; " />
      <meta name="citation_reference" content="citation_number=77; " />
      <meta name="citation_reference" content="citation_title=A six-stimulus theory for stochastic texture; citation_volume=62; citation_number=78; citation_pages=7-16; citation_date=2005; " />
      <meta name="citation_reference" content="citation_title=Hierarchical models of object recognition in cortex; citation_volume=2; citation_number=79; citation_pages=1019-1025; citation_date=1999; " />
      <meta name="citation_reference" content="citation_title=Perceiving real world scenes; citation_volume=177; citation_number=80; citation_pages=77-80; citation_date=1972; " />
      <meta name="citation_reference" content="citation_title=Coarse blobs or fine edges? Evidence that information diagnosticity changes the perception of complex visual stimuli; citation_volume=34; citation_number=81; citation_pages=72-107; citation_date=1997; " />
      <meta name="citation_reference" content="citation_number=82; " />
      <meta name="citation_reference" content="citation_title=View from the top: Hierarchies and reverse hierarchies in the visual system; citation_volume=36; citation_number=83; citation_pages=791-804; citation_date=2002; " />
      <meta name="citation_reference" content="citation_title=Natural image statistics and neural representation; citation_volume=24; citation_number=84; citation_pages=1193-1216; citation_date=2001; " />
      <meta name="citation_reference" content="citation_title=Visual perception and the statistical properties of natural scenes; citation_volume=59; citation_number=85; citation_pages=167-192; citation_date=2008; " />
      <meta name="citation_reference" content="citation_title=A biologically inspired algorithm for the recovery of shading and reflectance images; citation_volume=33; citation_number=86; citation_pages=1463-1473; citation_date=2004; " />



<!--
<rdf:RDF xmlns="http://web.resource.org/cc/"
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<Work rdf:about="http://www.ploscompbiol.org/article/fetchArticle.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726">
   <license rdf:resource="http://creativecommons.org/licenses/by/2.5/" />
</Work>
<License rdf:about="http://creativecommons.org/licenses/by/2.5/">
   <permits rdf:resource="http://web.resource.org/cc/Reproduction" />
   <permits rdf:resource="http://web.resource.org/cc/Distribution" />
   <requires rdf:resource="http://web.resource.org/cc/Notice" />
   <requires rdf:resource="http://web.resource.org/cc/Attribution" />
   <permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" />
</License>
<rdf:Description
     rdf:about="http://www.ploscompbiol.org/article/fetchArticle.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726"
     dc:identifier="http://www.ploscompbiol.org/article/fetchArticle.action?articleURI=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726"
     dc:title="PLOS Computational Biology: Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories"
       trackback:ping="http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726/trackback;jsessionid=27798ADAA937AC4537C756CBCF05EBCC"
     />
</rdf:RDF>

-->
</head>
<body>
<span property="dc:language" content="en"></span>
<!-- begin : container -->
<div id="container">
  <!-- begin : top banner external ad space -->
  <div id="topBanner">


<!-- begin : left banner slot -->
<div class="left">
  <iframe id='affe494b' name='affe494b'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=84&amp;cb=7373'
    frameborder='0' scrolling='no' width='468' height='60'><a
    href='http://ads.plos.org/www/delivery/ck.php?n=affe494b&amp;cb=8440'
    target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=84&amp;cb=9392&amp;n=affe494b'
    border='0' alt='' /></a></iframe>
</div>
<!-- end : left banner slot -->
<!-- begin : right banner slot -->
<div class="right">
  <iframe id='af654bd5' name='af654bd5'
    src='http://ads.plos.org/www/delivery/afr.php?zoneid=85&amp;cb=229'
    frameborder='0' scrolling='no' width='468' height='60'><a
    href='http://ads.plos.org/www/delivery/ck.php?n=af654bd5&amp;cb=8198'
    target='_top'><img src='http://ads.plos.org/www/delivery/avw.php?zoneid=85&amp;cb=9703&amp;n=af654bd5'
    border='0' alt='' /></a></iframe>
</div>
<!-- end : right banner slot -->

  </div>
  <!-- end : top banner external ad space -->

  <!-- begin : header -->
  <div id="hdr" class="login">

  <!-- begin : logo -->
  <div id="logo" title="PLOS Computational Biology"><a href="/home.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" title="PLOS Computational Biology"><span>PLOS Computational Biology</span></a></div>
  <!-- end : logo -->
  <!-- begin : user controls -->





  <div id="user">
    <div>
      <ul>
        <li><a href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pcbi.1002726" class="feedback"><strong>Login</strong></a> | </li>
        <li><a href="https://register.plos.org:443/ambra-registration/register.action">Create Account</a> | </li>
        <li class="feedback"><a href="/feedbackCreate.action?page=%252Farticle%252FfetchArticle.action%253FarticleURI%253Dinfo%25253Adoi%25252F10.1371%25252Fjournal.pcbi.1002726" title="Send us your feedback">Feedback</a></li>
      </ul>
    </div>
  </div>


  <!-- end : user controls -->
  <!-- begin search links -->
  <ul id="links"><li class="browse"><a href="/article/browse.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC?field=date" title="Browse Articles">Browse</a></li><li class="rss"><a href="/static/rssFeeds.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" title="RSS Feeds">RSS</a></li></ul>
  <!-- end : search links -->
  <!-- begin : dashboard -->
  <div id="db">
    <form name="searchForm" action="/search/simpleSearch.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" method="get">
      <input type="hidden" name="from" value="globalSimpleSearch"/>
      <input type="hidden" name="filterJournals" value="PLoSCompBiol">
      <fieldset>
        <legend>Search</legend>
        <label for="search">Search</label>
        <div class="wrap"><input id="search" type="text" name="query" value="Search articles..." onfocus="if(this.value=='Search articles...')value='';" onblur="if(this.value=='')value='Search articles...';" class="searchField" alt="Search articles..."/></div>
        <input src="/images/search_btn1.gif" onclick="submit();" value="ftsearch" alt="SEARCH" tabindex="3" class="button" type="image" />
      </fieldset>
    </form>
    <form name="gasf" action="/search/advancedSearch.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" method="get">
<input type="hidden" name="noSearchFlag" value="true" id="noSearchFlag"/><input type="hidden" name="query" value="" id="query"/>    </form>
    <a id="advSearch" href="#" onclick="if(document.searchForm.query.value!='Search articles...')document.gasf.query.value=document.searchForm.query.value;document.gasf.submit();return false;">Advanced Search</a>
  </div>
  <!-- end : dashboard -->
  <!-- begin : navigation -->
  <ul id="nav">
    <li><a href="/home.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" tabindex="101">Home</a></li>
     <!-- This is a temporary action to link to th static toc page -->
    <li><a href="/static/browse.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" tabindex="102">Browse Articles</a>
        <ul>
          <li><a href="/article/browseIssue.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC?field=issue">Current Issue</a></li> <!-- Assuming dynamic TOC is in place -->
          <li><a href="/article/browseVolume.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC?field=volume">Journal Archive</a></li> 
          <li><a href="/article/browse.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC?field=date">By Publication Date</a></li>
          <li><a href="/article/browse.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">By Subject</a></li>
          <li><a href="http://www.ploscollections.org/static/pcbiCollections.action">Collections</a></li>
        </ul>
    </li>
    <li><a href="/static/about.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" tabindex="103">About</a>
        <ul>
          <li><a href="/static/information.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Journal Information</a></li>
          <li><a href="/static/edboard.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Editorial Board</a></li>
          <li><a href="/static/eic.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Editors-in-Chief</a></li>
          <li><a href="/static/almInfo.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Article-Level Metrics</a></li>
          <li><a href="/static/license.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Open-Access License</a></li>
          <li><a href="/static/contact.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Contact Us</a></li>
        </ul>
      </li>
    <li><a href="/static/users.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" tabindex="104">For Readers</a>
        <ul>
          <li><a href="/static/commentGuidelines.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Guidelines for Notes, Comments, and Corrections</a></li>
          <li><a href="/static/ratingGuidelines.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Guidelines for Rating</a></li>
          <li><a href="/static/help.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Help Using This Site</a></li>
          <li><a href="/static/downloads.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Media Downloads</a></li>
          <li><a href="/static/sitemap.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Site Map</a></li>
        </ul>
      </li>
    <li><a href="/static/authors.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" tabindex="105">For Authors and Reviewers</a>
        <ul>
          <li><a href="/static/policies.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Editorial and Publishing Policies</a></li>
          <li><a href="/static/guidelines.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Author Guidelines</a></li>
          <li><a href="/static/figureGuidelines.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Figure and Table Guidelines</a></li>
          <li><a href="/static/checklist.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Submit Your Paper</a></li>
          <li><a href="/static/reviewerGuidelines.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">Reviewer Guidelines</a></li>
	   <li><a href="http://www.plos.org/journals/index.php">Submit to Other PLOS Journals</a></li>
        </ul>
      </li>
      <li class="journalnav"><a href="http://www.plos.org" title="PLOS" tabindex="110" class="drop">PLOS.org</a>
        <ul>
          <li><a href="http://www.plos.org/oa/index.html" title="Open Access Statement">Open Access</a></li>
          <li><a href="http://www.plos.org/support/donate.php" title="Join PLOS: Show Your Support">Join PLOS</a></li>
          <li><a href="http://blogs.plos.org/" title="PLOS Blogs">PLOS Blogs</a></li>
	  <li><a href="http://www.zazzle.com/plos?rf=238434112415374154" title="PLOS Store">PLOS Store</a></li>
          <li><a href="http://currents.plos.org" title="PLOS Currents">PLOS Currents</a></li>
          <li><a href="http://www.plos.org/connect.html" title="PLOS.org | Stay Connected">Stay Connected</a></li>
        </ul>
      </li>
      <li class="journalnav"><a href="http://hubs.plos.org" tabindex="109">Hubs</a>
        <ul>
          <li><a href="http://hubs.plos.org/biodiversity" title="PLOS Hubs: Biodiversity">Biodiversity</a></li>
          <li><a href="http://clinicaltrials.ploshubs.org" title="PLOS Hub for Clinical Trials">Clinical Trials</a></li>
        </ul>
    </li>
      <li class="journalnav"><a href="http://www.plosjournals.org" tabindex="108">Journals</a>
        <ul>
          <li><a href="http://www.plosbiology.org/" title="PLOSBiology.org">PLOS Biology</a></li>
          <li><a href="http://www.plosmedicine.org/" title="PLOSMedicine.org">PLOS Medicine</a></li>
          <li><a href="http://www.ploscompbiol.org/" title="PLOSCompBiol.org">PLOS Computational Biology</a></li>
          <li><a href="http://www.plosgenetics.org/" title="PLOSGenetics.org">PLOS Genetics</a></li>
          <li><a href="http://www.plospathogens.org/" title="PLOSPathogens.org">PLOS Pathogens</a></li>
          <li><a href="http://www.plosone.org/" title="PLOSONE.org">PLOS ONE</a></li>
          <li><a href="http://www.plosntds.org/" title="PLOSNTDs.org">PLOS Neglected Tropical Diseases</a></li>
        </ul>
      </li>
    </ul>
  <!-- end : navigation -->
  </div>
  <!-- end : header --> 



<!-- begin : main content -->
<div id="content" class="article" style="visibility:visible;">

<!-- begin : right hand column -->

<div id="rhc" xpathLocation="noDialog">

  <div id="download" class="rhcBox_type1">
    <div class="wrap">
      <ul>
        <li class="download icon"><strong>Download:</strong>
          <a href="/article/fetchObjectAttachment.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC?uri=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726&amp;representation=PDF" title="Download article PDF">PDF</a> |
          <a href="/article/citationList.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC?articleURI=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726" title="Download citations">Citation</a> |
          <a href="/article/fetchObjectAttachment.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC?uri=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726&amp;representation=XML" title="Download Article XML">XML</a>
        </li>
        <li class="print icon"><a href="#" onclick="window.print();return false;" title="Print Article"><strong>Print article</strong></a></li>
        <li class="reprint icon"><a href="https://www.odysseypress.com/onlinehost/reprint_order.php?type=A&page=0&journal=3&doi=10.1371/journal.pcbi.1002726&volume=&issue=&title=Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories&author_name=Iris%20I.%20A.%20Groen%2C%20Sennay%20Ghebreab%2C%20Victor%20A.%20F.%20Lamme%2C%20H.%20Steven%20Scholte&start_page=1&end_page=16" title="Odyssey Press">EzReprint</a> New &amp; improved!</li>
      </ul>
    </div>
  </div>

  <div id="issues" class="rhcBox_type2">
    <p><strong>Published in the</strong>
    <a href="http://www.ploscompbiol.org/article/browseIssue.action?issue=info%3Adoi%2F10.1371%2Fissue.pcbi.v08.i10" title="Browse the Open-Access Issue">October 2012 Issue of <em>PLOS Computational Biology</em></a></p>
  </div>

  <div id="impact" class="rhcBox_type2">
    <h6>Metrics <a href="/static/help.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC#articleMetrics" class="replaced" id="info" title="More information"><span>info</span></a></h6>

    <div class="wrap">
      <img id="rhcDataSpinner" src="/images/loading_small.gif" class="loading" />
      <div id="totalDataInRHC"></div>
      <div id="pubDateInRHC"></div>
      <div id="relatedCitesInRHC"></div>
    </div>

    <div class="more clearfix"><a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" title="More information">More</a></div>
  </div>

  <div id="related" class="rhcBox_type2">
    <h6>Related Content</h6>
    <dl class="related">
      <dt>Related Articles on the Web</dt>
      <dd><a href="http://scholar.google.com/scholar?hl=en&lr=&q=related:http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1002726&btnG=Search">Google Scholar</a></dd>
      <dd id="pubMedRelatedLIInRHC" style="display:none;"><a id="pubMedRelatedURLInRHC">PubMed</a></dd>
    </dl>
    <div class="more clearfix"><a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" title="More information">More</a></div>
  </div>

  <div id="share" class="rhcBox_type2">
    <h6>Share this Article <a href="/static/help.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC#socialBookmarkLinks" class="replaced" id="info" title="More information"><span>info</span></a></h6>
    <ul>
      <li class="bookmarklets">

        <a href="http://www.reddit.com/submit?url=http://dx.plos.org/10.1371/journal.pcbi.1002726"
           target="_new"
           id="reddit">
          <img src="http://www.reddit.com/static/spreddit4.gif" alt="submit to reddit" border="0"/> </a>
        <g:plusone size="small" annotation="none" href=http://dx.plos.org/10.1371/journal.pcbi.1002726></g:plusone>
        <script type="text/javascript">
          (function () {
            var po = document.createElement('script');
            po.type = 'text/javascript';
            po.async = true;
            po.src = 'https://apis.google.com/js/plusone.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(po, s);
          })();
        </script>
        <a href="http://www.stumbleupon.com/submit?url=http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726" target="_new"> <img border=0 src="http://cdn.stumble-upon.com/images/16x16_su_solid.gif" alt="StumbleUpon" title="Add to StumbleUpon"></a>
        <script>function fbs_click() {u='http://dx.plos.org/10.1371/journal.pcbi.1002726';t='Spatially%20Pooled%20Contrast%20Responses%20Predict%20Neural%20and%20Perceptual%20Similarity%20of%20Naturalistic%20Image%20Categories';window.open('http://www.facebook.com/sharer.php?u='+encodeURIComponent(u)+'&t='+encodeURIComponent(t),'sharer','toolbar=0,status=0,width=626,height=436');return false;}</script><a href="http://www.facebook.com/share.php?u=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1002726" onclick="return fbs_click()" target="_blank"><img src="http://static.ak.fbcdn.net/images/share/facebook_share_icon.gif" alt="Facebook" title="Add to Facebook" /></a>       <!-- for mor info, see http://www.facebook.com/share_partners.php -->
        <script type="text/javascript">
          function bookmark_in_connotea(u) {
            a=false; x=window; e=x.encodeURIComponent; d=document;
            w=open('http://www.connotea.org/addpopup?continue=confirm&uri='+e(u),
                'add', 'width=600, height=400, scrollbars, resizable');
            void(x.setTimeout('w.focus()',200));
          }
        </script>
        <a style='cursor: pointer;' onclick='javascript:bookmark_in_connotea("http://dx.plos.org/10.1371/journal.pcbi.1002726");'><img src='/images/icon_connotea_16x16.gif' alt="Connotea" title="Add to Connotea"/></a>
        <a href="http://www.citeulike.org/posturl?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1002726&title=Spatially%20Pooled%20Contrast%20Responses%20Predict%20Neural%20and%20Perceptual%20Similarity%20of%20Naturalistic%20Image%20Categories" target="_new"><img src='/images/icon_citeulike_16x16.gif' alt="CiteULike" title="Add to CiteULike" /></a>
        <a href="http://www.mendeley.com/import/?url=http%3A%2F%2Fdx.plos.org%2F10.1371%2Fjournal.pcbi.1002726" title="Add to Mendeley" target="_new"><img src="/images/icon_mendeley_16x16.gif" alt="Bibliography"></a>
        <script type="text/javascript">
          var twtTitle  = 'Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories';
          var twtUrl    = 'http://dx.plos.org/10.1371/journal.pcbi.1002726';
          var twtTag    = '#PLOSCompBio:'
          var maxLength = 140 - (twtUrl.length + 1);
          if (twtTitle.length > maxLength) {
            twtTitle = twtTitle.substr(0, (maxLength - 17))+'...';  //truncates title to include space for twtTag and ellipsis
          }
          var twtLink = 'http://twitter.com/intent/tweet?text='+encodeURIComponent(twtTag + ' ' + twtTitle + ' ' + twtUrl);
          document.write('<a href="'+twtLink+'" target="_blank"><img src="/images/icon_twitter.gif" border="0" alt="Twitter icon" title="Tweet This!" /></a>');
        </script>

      </li>
      <li class="email icon"><a href="/article/emailArticle.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC?articleURI=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726" title="Email this article">Email this article</a></li>
    </ul>
  </div>
  <div id="articleTwitterStream" style="display:none">
    <script src="http://widgets.twimg.com/j/2/widget.js"></script>
    <script>
      new TWTR.Widget({
        version: 2,
        type: 'search',
        search: encodeURIComponent('10.1371/journal.pcbi.1002726'),
        interval: 6000,
        title: '',
        subject: 'From Twitter',
        width: 300,
        height: 300,
        theme: {
          shell: {
            background: '#667893',
            color: '#ffffff'
          },
          tweets: {
            background: '#ffffff',
            color: '#444444',
            links: '#006699'
          }
        },
        features: {
          scrollbar: true,
          loop: false,
          live: true,
          hashtags: true,
          timestamp: true,
          avatars: true,
          toptweets: true,
          behavior: 'all'
        }
      }).render().start();
    </script>
  </div>
</div>
<!-- end : right hand column -->
<form name="articleInfo" id="articleInfo" method="" action="">
<input type="hidden" name="isAuthor" value="true" />
<input type="hidden" name="authorIdList" value="" />
<input type="hidden" name="userIdList" value="" />
<input type="hidden" name="otherIdList" value="" />
<input type="hidden" name="annotationId" value="" />
<input type="hidden" name="isResearchArticle" value="true" />
</form>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.XML" value="156609"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.PDF" value="3208785"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g001.TIF" value="1082488"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g001.PNG_I" value="138029"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g001.PNG_S" value="21651"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g001.PNG_M" value="257711"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g001.PNG_L" value="906003"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g002.TIF" value="854582"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g002.PNG_I" value="49373"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g002.PNG_S" value="13061"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g002.PNG_M" value="122479"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g002.PNG_L" value="689371"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.e001.TIF" value="3206"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.e001.PNG" value="1736"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.e002.TIF" value="1930"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.e002.PNG" value="1155"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.e003.TIF" value="1258"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.e003.PNG" value="829"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.e004.TIF" value="3596"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.e004.PNG" value="1986"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g003.TIF" value="959384"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g003.PNG_I" value="55461"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g003.PNG_S" value="14707"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g003.PNG_M" value="129553"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g003.PNG_L" value="577995"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g004.TIF" value="443900"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g004.PNG_I" value="41307"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g004.PNG_S" value="11701"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g004.PNG_M" value="88166"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g004.PNG_L" value="263830"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g005.TIF" value="491334"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g005.PNG_I" value="111611"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g005.PNG_S" value="17974"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g005.PNG_M" value="105563"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g005.PNG_L" value="213380"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g006.TIF" value="608824"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g006.PNG_I" value="77226"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g006.PNG_S" value="17882"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g006.PNG_M" value="144730"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g006.PNG_L" value="229830"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g007.TIF" value="1715816"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g007.PNG_I" value="126566"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g007.PNG_S" value="21979"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g007.PNG_M" value="317924"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g007.PNG_L" value="1076698"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g008.TIF" value="537278"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g008.PNG_I" value="76710"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g008.PNG_S" value="18835"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g008.PNG_M" value="147870"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.g008.PNG_L" value="207061"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.s001.TIF" value="145039"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.s002.TIF" value="243083"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.s003.TIF" value="481788"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.s004.TIF" value="268418"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.s005.TIF" value="587942"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.s006.TIF" value="245908"/>
  <input type="hidden" class="assetSize" name="info:doi/10.1371/journal.pcbi.1002726.s007.MPG" value="491866"/>
<div id="articleContainer">

<span about="http://www.plos.org"  typeof="foaf:Organization" property="foaf:name" style="display:none; visibility:hidden;">PLOS</span>
<div id="researchArticle" class="content">
  <span rel="dc:publisher" href="http://www.plos.org"></span>
  <a id="top" name="top"></a>
<div id="contentHeader" xpathLocation="noSelect">
  <p>Open Access</p><p id="articleType">Research Article</p>
</div>

  <h1 xpathLocation="noSelect" property="dc:title" datatype="" rel="dc:type" href="http://purl.org/dc/dcmitype/Text">Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories</h1>
  <div property="dc:description" datatype="" style="display:none; visibility:hidden;">Author SummaryHumans excel in rapid and accurate processing of visual scenes. However, it is unclear which computations allow the visual system to convert light hitting the retina into a coherent representation of visual input in a rapid and efficient way. Here we used simple, computer-generated image categories with similar low-level structure as natural scenes to test whether a model of early integration of low-level information can predict perceived category similarity. Specifically, we show that summarized (spatially pooled) responses of model neurons covering the entire visual field (the population response) to low-level properties of visual input (contrasts) can already be informative about differences in early visual evoked activity as well as behavioral confusions of these categories. These results suggest that low-level population responses can carry relevant information to estimate similarity of controlled images, and put forward the exciting hypothesis that the visual system may exploit these responses to rapidly process real natural scenes. We propose that the spatial pooling that allows for the extraction of this information may be a plausible first step in extracting scene gist to form a rapid impression of the visual input.</div>
  <span property="dc:date" content="2012-10-18" datatype="xsd:date" rel="dc:identifier" href="http://dx.doi.org/10.1371/journal.pcbi.1002726"></span>
  <span property="dc:subject" content="Computational Biology"></span>
  <span property="dc:subject" content="Neuroscience"></span>
<form action="">
  <input type="hidden" name="journalDisplayName" id="journalDisplayName" value="PLOS Computational Biology" />
  <input type="hidden" name="crossRefPageURL" id="crossRefPageURL" value="/article/crossref/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" />
  <input type="hidden" name="twitterPageURL" id="twitterPageURL" value="/article/twitter/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" />
  <input type="hidden" name="metricsTabURL" id="metricsTabURL" value="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" />
  <input type="hidden" name="doi" id="doi" value="info:doi/10.1371/journal.pcbi.1002726" />
  <input type="hidden" name="articleTitleUnformatted" id="articleTitleUnformatted" value="Spatially%20Pooled%20Contrast%20Responses%20Predict%20Neural%20and%20Perceptual%20Similarity%20of%20Naturalistic%20Image%20Categories" />
  <input type="hidden" name="articlePubDate" id="articlePubDate" value="1350543600000" />
</form>
<div class="horizontalTabs" xpathLocation="noSelect">
  <ul id="tabsContainer">
        <li id="article" class="active"><a href="/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" class="tab" title="Article">Article</a></li>
        <li id="metrics"><a href="/article/metrics/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" class="tab" title="Metrics">Metrics</a></li>
        <li id="related"><a href="/article/related/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" class="tab" title="Related Content">Related Content</a></li>
        <li id="comments"><a href="/article/comments/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" class="tab" title="Comments">Comments: 0</a></li>
  </ul>
</div>
  <div id="articleMenuBox" xpathLocation="noSelect">
    <div id="articleMenu">
      <div class="wrap">
        <ul>
          <li class="annotation icon">To <strong>add a note</strong>, highlight some text. <a href="#" onclick="toggleAnnotation(this, 'public'); return false;" title="Click to turn notes on/off">Hide notes</a></li>
          <li class="discuss icon">
              <a href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pcbi.1002726">Make a general comment</a>
          </li>
        </ul>
        <div id="sectionNavTopBox" style="display:none;">
          <p><strong>Jump to</strong></p>
          <div id="sectionNavTop" class="tools"></div>
        </div>
      </div>
    </div>
      <div id="lightBox" xpathLocation="noDialog" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726')">
      </div>
  </div>




<p class="authors" xpathLocation="noSelect"><span rel="dc:creator"><span class="person" property="foaf:name" typeof="foaf:Person">Iris I. A. Groen</span></span><sup><a href="#aff1">1</a></sup><sup><a href="#cor1" class="fnoteref">*</a></sup>, <span rel="dc:creator"><span class="person" property="foaf:name" typeof="foaf:Person">Sennay Ghebreab</span></span><sup><a href="#aff1">1</a></sup><sup>,</sup><sup><a href="#aff2">2</a></sup>, <span rel="dc:creator"><span class="person" property="foaf:name" typeof="foaf:Person">Victor A. F. Lamme</span></span><sup><a href="#aff1">1</a></sup>, <span rel="dc:creator"><span class="person" property="foaf:name" typeof="foaf:Person">H. Steven Scholte</span></span><sup><a href="#aff1">1</a></sup></p><p class="affiliations" xpathLocation="noSelect"><a name="aff1" id="aff1"></a><strong>1</strong> Cognitive Neuroscience Group, Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands, <a name="aff2" id="aff2"></a><strong>2</strong> Intelligent Systems Lab Amsterdam, Institute of Informatics, University of Amsterdam, Amsterdam, The Netherlands</p><div class="abstract" xpathLocation="/article[1]/front[1]/article-meta[1]/abstract[1]"><a id="abstract0" name="abstract0" toc="abstract0" title="Abstract"></a><h2 xpathLocation="noSelect">Abstract&nbsp;<a href="#top">Top</a></h2><p xpathLocation="/article[1]/front[1]/article-meta[1]/abstract[1]/p[1]">The visual world is complex and continuously changing. Yet, our brain transforms patterns of light falling on our retina into a coherent percept within a few hundred milliseconds. Possibly, low-level neural responses already carry substantial information to facilitate rapid characterization of the visual input. Here, we computationally estimated low-level contrast responses to computer-generated naturalistic images, and tested whether spatial pooling of these responses could predict image similarity at the neural and behavioral level. Using EEG, we show that statistics derived from pooled responses explain a large amount of variance between single-image evoked potentials (ERPs) in individual subjects. Dissimilarity analysis on multi-electrode ERPs demonstrated that large differences between images in pooled response statistics are predictive of more dissimilar patterns of evoked activity, whereas images with little difference in statistics give rise to highly similar evoked activity patterns. In a separate behavioral experiment, images with large differences in statistics were judged as different categories, whereas images with little differences were confused. These findings suggest that statistics derived from low-level contrast responses can be extracted in early visual processing and can be relevant for rapid judgment of visual similarity. We compared our results with two other, well- known contrast statistics: Fourier power spectra and higher-order properties of contrast distributions (skewness and kurtosis). Interestingly, whereas these statistics allow for accurate image categorization, they do not predict ERP response patterns or behavioral categorization confusions. These converging computational, neural and behavioral results suggest that statistics of pooled contrast responses contain information that corresponds with perceived visual similarity in a rapid, low-level categorization task.</p>
</div><div class="abstract" xpathLocation="/article[1]/front[1]/article-meta[1]/abstract[2]"><a id="abstract1" name="abstract1" toc="abstract1" title="Author Summary"></a>
<h2 xpathLocation="noSelect">Author Summary&nbsp;<a href="#top">Top</a></h2>
<p xpathLocation="/article[1]/front[1]/article-meta[1]/abstract[2]/p[1]">Humans excel in rapid and accurate processing of visual scenes. However, it is unclear which computations allow the visual system to convert light hitting the retina into a coherent representation of visual input in a rapid and efficient way. Here we used simple, computer-generated image categories with similar low-level structure as natural scenes to test whether a model of early integration of low-level information can predict perceived category similarity. Specifically, we show that summarized (<em>spatially pooled</em>) responses of model neurons covering the entire visual field (<em>the population response</em>) to low-level properties of visual input (<em>contrasts</em>) can already be informative about differences in early visual evoked activity as well as behavioral confusions of these categories. These results suggest that low-level population responses can carry relevant information to estimate similarity of controlled images, and put forward the exciting hypothesis that the visual system may exploit these responses to rapidly process real natural scenes. We propose that the spatial pooling that allows for the extraction of this information may be a plausible first step in extracting scene gist to form a rapid impression of the visual input.</p>
</div>


<div class="articleinfo" xpathLocation="noSelect"><p><strong>Citation: </strong>Groen IIA, Ghebreab S, Lamme VAF, Scholte HS (2012) Spatially Pooled Contrast Responses Predict Neural and Perceptual Similarity of Naturalistic Image Categories. PLoS Comput Biol 8(10):
          e1002726.
            doi:10.1371/journal.pcbi.1002726</p><p><strong>Editor: </strong>Olaf Sporns, 
        Indiana University, United States of America
      </p><p></p><p><strong>Received:</strong> February 29, 2012; <strong>Accepted:</strong> August 2, 2012; <strong>Published:</strong> October 18, 2012</p><p><strong>Copyright:</strong> ©  Groen et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p><p><strong>Funding: </strong>This work is part of the Research Priority Program ‘Brain &amp; Cognition’ at the University of Amsterdam and was supported by an Advanced Investigator grant from the European Research Council (<a href="http://erc.europa.eu/">http://erc.europa.eu/</a>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p><p><strong>Competing interests:</strong> The authors have declared that no competing interests exist.</p><p><a name="cor1"></a>* E-mail: <a href="mailto:i.i.a.groen@uva.nl">i.i.a.groen@uva.nl</a></p></div>





<div id="section1" xpathLocation="/article[1]/body[1]/sec[1]"><a id="s1" name="s1" toc="s1" title="Introduction"></a><h3 xpathLocation="noSelect">Introduction&nbsp;<a href="#top">Top</a></h3><p xpathLocation="/article[1]/body[1]/sec[1]/p[1]">Complex natural images are categorized remarkably fast <a href="#pcbi.1002726-Potter1">[1]</a>, <a href="#pcbi.1002726-Greene1">[2]</a>, sometimes even faster than simple artificial stimuli <a href="#pcbi.1002726-FeiFei1">[3]</a>. For animal and non-animal scenes, differences in EEG responses are found within 150 ms <a href="#pcbi.1002726-Thorpe1">[4]</a> and a correct saccade is made within 120 ms <a href="#pcbi.1002726-Kirchner1">[5]</a>. This speed of processing is also found for other scene categories <a href="#pcbi.1002726-VanRullen1">[6]</a> and may require less attentional resources compared to artificial images <a href="#pcbi.1002726-Rousselet1">[7]</a>, <a href="#pcbi.1002726-Peelen1">[8]</a>. This suggests that relevant visual information is rapidly and efficiently extracted from early visual responses to natural scenes. However, the neural computations involved in this process are not known.</p>
<p xpathLocation="/article[1]/body[1]/sec[1]/p[2]">Importantly, natural images differ from other image types such as white noise in low-level properties (e.g., sparseness), leading to the suggestion that the visual system has adapted to these low-level properties <a href="#pcbi.1002726-Field1">[9]</a>. This idea paved the way for optimal coding models for natural images <a href="#pcbi.1002726-Vinje1">[10]</a>, <a href="#pcbi.1002726-Schwartz1">[11]</a> and successful predictions of response properties of visual neurons <a href="#pcbi.1002726-Olshausen1">[12]</a>. Recent work identified statistical properties that differ even within the class of natural images, e.g. between natural scene parts <a href="#pcbi.1002726-Karklin1">[13]</a>, <a href="#pcbi.1002726-Frazor1">[14]</a> or natural image categories <a href="#pcbi.1002726-Torralba1">[15]</a>, showing that image statistics such as power spectra of spatial frequency content or distributions of local image features are informative about scene category.</p>
<p xpathLocation="/article[1]/body[1]/sec[1]/p[3]">The fact that it is mathematically possible to distinguish categories based on image statistics, however, does not imply that they are used for categorization in the brain. Image statistics may not be sufficiently reliable, or their computation may not be suitable for neural implementation <a href="#pcbi.1002726-Olshausen1">[12]</a>, <a href="#pcbi.1002726-Graham1">[16]</a>. We recently showed that statistics derived from the frequency histogram of local contrast – summarized by two parameters of a Weibull fit, <a href="#pcbi-1002726-g001"><strong>Fig. 1A</strong></a> – explain up to 50% of the variance of event-related potentials (ERPs) recorded from visual cortex <a href="#pcbi.1002726-Scholte1">[17]</a>. These parameters inform about the width and shape of the histogram, respectively, and appear to describe meaningful variability between images (<a href="#pcbi-1002726-g001"><strong>Fig. 1B</strong></a>). Importantly, we found that these parameters can be reliably approximated by linear summation of the output of localized difference-of-Gaussians filters modeled after X- and Y-type LGN cells, suggesting that this global information may be available to visual cortex directly from its early low-level contrast responses <a href="#pcbi.1002726-Scholte1">[17]</a>.</p>
<div class="figure" xpathLocation="/article[1]/body[1]/sec[1]/fig[1]"><a name="pcbi-1002726-g001" id="pcbi-1002726-g001" title="Click for larger image " href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g001" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g001');"><div class="expand-link" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g001');"></div><img xpathLocation="noSelect" border="1" src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.g001&amp;representation=PNG_I" align="left" alt="thumbnail" class="thumbnail"></a><p><strong xpathLocation="/article[1]/body[1]/sec[1]/fig[1]/label[1]"><a xpathLocation="noSelect" href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g001" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g001');"><span>Figure 1. </span></a> <span xpathLocation="/article[1]/body[1]/sec[1]/fig[1]/caption[1]/title[1]">Contrast histograms of natural images follow a Weibull distribution.</span></strong></p><p xpathLocation="/article[1]/body[1]/sec[1]/fig[1]/caption[1]/p[1]">(<strong>A</strong>), Three natural images with varying degrees of details and scene fragmentation. The homogenous, texture-like image of grass (upper row) contains many edges of various strengths; its contrast distribution approaches a Gaussian. The strongly segmented image of green leaves against a uniform background (bottom row) contains very few, strong edges that are highly coherent; its distribution approaches power law. Most natural images, however, have distributions in between (middle row). The degree to which images vary between these two extremes is reflected in the free parameters of a Weibull fit to the contrast histogram: β (beta) and γ (gamma). (<strong>B</strong>), For each of 200 natural scenes, the beta and gamma values were derived from fitting the Weibull distribution to their contrast histogram. Beta describes the width of the histogram: it varies with the distribution of local contrasts strengths. Gamma describes the shape of the histogram: it varies with the amount of scene clutter. Four representative pictures are shown in each corner of the parameter space. Images with a high degree of scene segmentation, e.g. a leaf on top of snow, are found in the lower left corner, whereas highly cluttered images are on the right. Images with more depth are located on the top, whereas flat images are found at the bottom. Images are from the McGill Calibrated Colour Image Database <a href="#pcbi.1002726-Olmos1">[86]</a>.</p>
<span xpathLocation="noSelect">doi:10.1371/journal.pcbi.1002726.g001</span><div class="clearer"></div><div class="figure-inline-download"><ul><li><strong>Download: </strong><a href="/article/info:doi/10.1371/journal.pcbi.1002726.g001/powerpoint">PowerPoint slide</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g001/largerimage">
                    larger image (<span id="info:doi/10.1371/journal.pcbi.1002726.g001.PNG_L"></span> PNG)</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g001/originalimage">
                    original image (<span id="info:doi/10.1371/journal.pcbi.1002726.g001.TIF"></span> TIFF)
                  </a></li></ul></div></div><p xpathLocation="/article[1]/body[1]/sec[1]/p[4]">Moreover, we found that output of contrast filters with a larger range of receptive field sizes captures additional image information <a href="#pcbi.1002726-Ghebreab1">[18]</a>. This is not surprising since objects in natural scenes appear at many distances and hence spatial scales <a href="#pcbi.1002726-Oliva1">[19]</a>. In the present implementation, the model first estimates at which scale relevant contrast information is present, as well as characteristics of the distribution of contrast strengths at those scales. This model, which approximates early visual population responses based on spatially pooled contrasts, was able to explain almost 80% of ERP variance to natural images <a href="#pcbi.1002726-Ghebreab1">[18]</a>.</p>
<p xpathLocation="/article[1]/body[1]/sec[1]/p[5]">These previous findings suggest that images with more similar contrast response statistics evoke more similar early visual activity. Could these responses already contain relevant information about the stimulus for rapid categorization? The two parameters appear to index meaningful information such as degree of clutter, depth and figure-ground segmentation <a href="#pcbi.1002726-Scholte1">[17]</a>, but how the two dimensions in <a href="#pcbi-1002726-g001"><strong>Fig. 1B</strong></a> influence perception has not been examined. The goal of the current study was thus to explore what type of visual information is contained in the variance of the earliest visual contrast responses that is so well described by these two parameters. Specifically, we were interested in whether these parameters cannot only predict variance in visual activity, but also ‘variance in perception’. In other words, do images with more similar contrast statistics also lead to more similar perceptual representations, and perhaps ultimately, to similar images being considered a single category?</p>
<p xpathLocation="/article[1]/body[1]/sec[1]/p[6]">We aimed to answer this question in a data-driven manner, by investigating 1) which images group by similarity early in visual processing and 2) whether this grouping matches with perceived similarity of those images. For the first part of this question, we obtained reliable evoked responses to individual images. The advantage of this approach relative to traditional ERP analysis (which is based on averaging many trials across individual images within an a priori determined condition) is that it provides much richer data <a href="#pcbi.1002726-vanRijsbergen1">[20]</a>–<a href="#pcbi.1002726-Groen1">[24]</a> that can be used for model selection. We used these single-image evoked responses to compute dissimilarities in ‘neural space’, similar to the pattern analysis approach used in fMRI <a href="#pcbi.1002726-Kriegeskorte1">[25]</a>, <a href="#pcbi.1002726-Kravitz1">[26]</a>. This allowed us to track, over the course of the ERP, to what extent the representation of an image is (dis)similar to all images in the data set.</p>
<p xpathLocation="/article[1]/body[1]/sec[1]/p[7]">For the second part of the question, we needed to obtain an image-specific behavioral judgment of perceived visual similarity. However, simply judging similarity of natural scenes is problematic, because these images obviously contain rich semantic content: there are many features of natural scenes that can be similar or dissimilar, which is likely to lead to different categorization strategies by different subjects. Also, it is uncertain to what extent specific semantic tags that are provided by the researcher (e.g. ‘openness’ or ‘naturalness’, <a href="#pcbi.1002726-Ross1">[27]</a>), can be uniformly interpreted as a relevant stimulus dimension that has a linear mapping to processing in early vision. Therefore, to explore the variance explained by contrast response statistics in a bottom-up way, we used stimuli that were simplified model images of natural scenes (‘dead leaves’, <a href="#pcbi-1002726-g002"><strong>Fig. 2A</strong></a>), which have similar low-level structure as natural scenes (e.g. 1/f power spectra) but are devoid of semantic content. These images are created by filling a frame with objects - much like fallen leaves can fill a forest floor – and are used in computer vision to study, for example, how the appearance and the distribution of these objects influences the low-level structure of natural scenes <a href="#pcbi.1002726-Hsiao1">[28]</a>. By manipulating properties of the objects in a controlled manner, we created distinct image categories, and then tested whether differences between these categories in contrast statistics matched with behaviorally perceived similarity by letting human observers perform a same-different categorization task on all combinations of image categories.</p>
<div class="figure" xpathLocation="/article[1]/body[1]/sec[1]/fig[2]"><a name="pcbi-1002726-g002" id="pcbi-1002726-g002" title="Click for larger image " href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g002" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g002');"><div class="expand-link" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g002');"></div><img xpathLocation="noSelect" border="1" src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.g002&amp;representation=PNG_I" align="left" alt="thumbnail" class="thumbnail"></a><p><strong xpathLocation="/article[1]/body[1]/sec[1]/fig[2]/label[1]"><a xpathLocation="noSelect" href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g002" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g002');"><span>Figure 2. </span></a> <span xpathLocation="/article[1]/body[1]/sec[1]/fig[2]/caption[1]/title[1]">Example stimuli and computation of contrast statistics.</span></strong></p><p xpathLocation="/article[1]/body[1]/sec[1]/fig[2]/caption[1]/p[1]">(<strong>A</strong>), Example images of each of the 16 categories used in the behavioral and EEG experiment. Images contained randomly placed disks that differed in distribution, opacity, depth and size. Each category contained 16 unique images. (<strong>B</strong>), Consecutive steps in computing various contrast statistics. Weibull statistics are computed by filtering the image with a range of contrast filters with LGN-like scale- and gain properties, after which for each image location, the filter containing the minimal reliable response is selected. Responses of all selected filters are summed in a histogram to which the Weibull function is fitted, from which the beta and gamma parameters are derived using maximum likelihood estimation. (<strong>C</strong>), Power spectra parameters (top row) are extracted by taking the Fourier transform, averaging across directions, and computing the intercept and slope values of a line fitted to the average power spectrum. Higher-order properties of the contrast distribution (bottom row) are computed by filtering with a single-scale center-surround filter, after which skewness and kurtosis of the resulting contrast distribution are derived. Weibull statistics (multiscale local contrast) presumably contain information present in Fourier parameters (scale statistics) as well as local contrast distribution parameters (distribution statistics).</p>
<span xpathLocation="noSelect">doi:10.1371/journal.pcbi.1002726.g002</span><div class="clearer"></div><div class="figure-inline-download"><ul><li><strong>Download: </strong><a href="/article/info:doi/10.1371/journal.pcbi.1002726.g002/powerpoint">PowerPoint slide</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g002/largerimage">
                    larger image (<span id="info:doi/10.1371/journal.pcbi.1002726.g002.PNG_L"></span> PNG)</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g002/originalimage">
                    original image (<span id="info:doi/10.1371/journal.pcbi.1002726.g002.TIF"></span> TIFF)
                  </a></li></ul></div></div><p xpathLocation="/article[1]/body[1]/sec[1]/p[8]">Specifically, we used the space formed by the two Weibull parameters to compute geometric distances between images in contrast statistics, and used these distances as quantitative predictors of dissimilarity <a href="#pcbi.1002726-Shepard1">[29]</a>–<a href="#pcbi.1002726-OpdeBeeck1">[31]</a>. We thus tested whether these parameters can predict the extent to which image categories induced dissimilar single-image EEG responses (experiment 1) and whether they match with perceptual categorization at the behavioral level (experiment 2). We predicted that images with very different Weibull statistics would appear less similar, i.e. be less often confused than images from categories with similar statistics.</p>
<p xpathLocation="/article[1]/body[1]/sec[1]/p[9]">By using controlled images that we quantified using a model originally derived from contrast responses to natural images, we aim to build a bridge between findings obtained with systematic manipulation of artificial stimuli and those obtained with more data-driven natural scene studies. For purpose of comparison, and to better understand which statistical information is captured by the Weibull parameters, we also tested two other global contrast statistics (<a href="#pcbi-1002726-g002"><strong>Fig. 2C</strong></a>). Following <a href="#pcbi.1002726-Oliva2">[32]</a> we calculated the intercept and slope of the average power spectrum to parameterize spatial frequency information, a commonly used measure of low-level information in scene perception. In addition, we followed <a href="#pcbi.1002726-Tadmor1">[33]</a> to derive the skewness and kurtosis of the contrast distribution for a range of spatial scales: these higher-order properties of distributions have previously been suggested (e.g. <a href="#pcbi.1002726-Brady1">[34]</a>, <a href="#pcbi.1002726-Kingdom1">[35]</a> to reflect low-level differences between images that are relevant for perceptual processing.</p>
<p xpathLocation="/article[1]/body[1]/sec[1]/p[10]">We find that Weibull statistics explain substantial variance in evoked response amplitude to the dead leaves images, predicting clustering-by-category of occipital ERP patterns within 100 ms of visual processing. In addition, they correlate with human categorization behavior: specific confusions were made between categories with similar Weibull statistics. By comparison, Fourier power spectra and skewness and kurtosis can be used for accurate classification of image category, but fail to predict neural clustering and behavioral categorization. These convergent results provide evidence for relevance of pooled contrast response statistics in rapid neural computation of perceptual similarity.</p>
</div>

<div id="section2" xpathLocation="/article[1]/body[1]/sec[2]"><a id="s2" name="s2" toc="s2" title="Materials and Methods"></a><h3 xpathLocation="noSelect">Materials and Methods&nbsp;<a href="#top">Top</a></h3>
<h4 xpathLocation="/article[1]/body[1]/sec[2]/sec[1]/title[1]">Ethics statement</h4>
<p xpathLocation="/article[1]/body[1]/sec[2]/sec[1]/p[1]">The experiments reported here were approved by the Ethical Committee of the Psychology Department at the University of Amsterdam; all participants gave written informed consent prior to participation and were rewarded with study credits or financial compensation (7 euro/hour).</p>


<h4 xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/title[1]">Stimuli</h4>
<p xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/p[1]">Gray-scale dead leaves images (512×512 pixels, bit depth 24) were generated using Matlab. Images contained randomly placed disks that were manipulated along 4 dimensions (opacity, depth, size and distribution) to create 16 categories. Disks were either opaque or transparent; intensity at the outer edges of the disk was either constant (leading to a 2D appearance) or decaying (3D appearance), and disk size was determined by drawing randomly from a range of small, medium or large diameters (exact settings as in <a href="#pcbi.1002726-Hsiao1">[28]</a>. Twelve categories were created by systematically varying these properties of power-law distributed disks. Four more categories were created using medium-diameter, exponentially distributed disks that could be 2D or 3D and opaque or transparent. For each category, 16 images were created using these category-specific settings: the random placement and use of ranges of diameter sizes ensured that each of these 16 images was unique. This procedure thus resulted in a total of unique 256 images, divided into 16 distinct categories, which were used for experimentation (<a href="#pcbi-1002726-g002"><strong>Fig. 2A</strong></a>).</p>
<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[1]/title[1]">Computation of contrast statistics.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[1]/p[1]">In the Weibull model, local contrast is computed at multiple spatial scales, after which a single optimal scale for each image location is selected. Subsequently, contrast responses are collected in a histogram that is summarized using a Weibull fit, yielding two statistical parameters: beta and gamma (<a href="#pcbi-1002726-g002"><strong>Fig. 2B</strong></a>). For comparison with other contrast statistics, we computed spatial frequency statistics (using power spectra) and higher-order statistics (third moments of the contrast distribution) for various receptive field sizes (<a href="#pcbi-1002726-g002"><strong>Fig. 2C</strong></a>). The computational steps of each method are described in detail below.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[2]/title[1]">Weibull contrast statistics.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[2]/p[1]">We computed image contrast according to the standard linear-nonlinear model. For the initial linear filtering step we used contrast filters modeled after well-known receptive fields of LGN-neurons <a href="#pcbi.1002726-Bonin1">[36]</a>. As described in detail in <a href="#pcbi.1002726-Ghebreab1">[18]</a> each location in the image was filtered using Gaussian second-order derivative filters spanning multiple octaves in spatial scale <a href="#pcbi.1002726-Croner1">[37]</a>. Based on our previous result <a href="#pcbi.1002726-Scholte1">[17]</a> that the beta parameter was best approximated by a linear summation of X-like receptive field size output, whereas the gamma parameter correlated highest with Y-like receptive field size contrast, two separate spatial scale octave ranges were applied to derive the two summary parameters in the present multi-scale model. For the beta parameter, a bank of filters with 5 octave scales (4, 8, 16, 32, 64) standard deviation in pixels was used; for the gamma parameter, the filter bank consisted of octave scales 5, 10, 20, 40 and 80. The output of each filter was normalized with a Naka-Rushton function with 5 semi-saturation constants between 0.15 and 1.6 to cover the spectrum from linear to non-linear contrast gain control in LGN.</p>
<p xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[2]/p[2]">From the population of gain- and scale-specific filters, one filter response was selected for each location in the image using minimum reliable scale selection <a href="#pcbi.1002726-Elder1">[38]</a>, a spatial scale control mechanism in which the smallest filter with output higher than what is expected to be noise for that specific filter is selected. The rationale behind this approach is that to arrive at a faithful scale-invariant contrast representation, the visual system selects spatial scale by minimizing receptive field size while simultaneously maximizing response reliability. Noise thresholds for each filter were determined in a separate set of stimuli (1800 natural images from the ImageNet natural scene database, <a href="#pcbi.1002726-Deng1">[39]</a>) and set to half a standard deviation of the average contrast present in that dataset for a given scale and gain. Applying the selected filter for each location in the image resulted in a 512×512 pixel contrast magnitude map, which was converted in a 256-bin histogram summarizing the contrast distribution of the image, to which the Weibull function was fitted by a maximum likelihood estimator (MLE). The Weibull function is given by:<br><a name="pcbi.1002726.e001" id="pcbi.1002726.e001"></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.e001&amp;representation=PNG"><span class="note" xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[2]/p[2]/disp-formula[1]/label[1]">(1)</span></span><br>where c is a normalization constant and μ, ß (beta) and γ (gamma) are the free parameters that represent the origin, scale and shape of the distribution, respectively. The value of the origin parameter μ is influenced by uneven illumination and generally close to zero for natural images. To achieve illumination invariance, this value was estimated and averaged out, leaving only the beta and gamma values as free parameters for each image.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[3]/title[1]">Fourier power statistics.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[3]/p[1]">A two-parameter Fourier statistic was derived for each image by computing the intercept and slope of a line fitted to its power spectrum. We determined the power spectrum of the largest concentric square portion of the image (in this case, the entire image), excluding its outer edges to prevent edge artifacts. The cropped image was transformed into the frequency domain using the Fast Fourier Transform. Slope and intercept were estimated from the regression line fitted to the log-log representation of the power law-dependence:<br><a name="pcbi.1002726.e002" id="pcbi.1002726.e002"></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.e002&amp;representation=PNG"><span class="note" xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[3]/p[1]/disp-formula[1]/label[1]">(2)</span></span><br>The rotationally averaged power-law spectrum <span class="inline-formula"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.e003&amp;representation=PNG"></span> is defined as<br><a name="pcbi.1002726.e004" id="pcbi.1002726.e004"></a><span class="equation"><img src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.e004&amp;representation=PNG"><span class="note" xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[3]/p[1]/disp-formula[2]/label[1]">(3)</span></span><br>where F<em><sub>I</sub> (f, θ)</em> is the Fourier transform spectrum of the input image <em>I</em>; <em>(f, θ)</em> are the cylindrical polar coordinates in Fourier space and 〈 〉<em><sub>θ</sub></em> denotes averaging over <em>θ</em>.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[4]/title[1]">Contrast distribution statistics.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[2]/sec[4]/p[1]">Following <a href="#pcbi.1002726-Tadmor1">[33]</a>, we used center-surround difference-of-Gaussian (DoG) filters to extract contrast values. Center receptive field sizes ranged between 2 and 4 pixels, and surround-to-center size ranged between 3 and 9, resulting in 21 different combinations of center size and surround-to-center ratio, referred to as receptive-field models. For each model, a scaling factor was used to set the integrated sensitivity of the surround to be 85% of that of the center. Per image, contrast responses were computed by convolving each pixel value with each of these 21 models separately. Responses were normalized using center-surround divisive normalization, where the difference in output of the center and surround is divided by their summed output. From the response distribution of responses across the image one skewness and one kurtosis value was derived for each image and for each receptive field model, resulting in 21 skewness and kurtosis values per image. Of these 21 values, results are reported for the skewness and kurtosis values that explained most EEG variance (center radius of 4 pixels with surround-center ratio 3); see next section (Experiment 1: EEG). This measure, computed exactly as reported in <a href="#pcbi.1002726-Tadmor1">[33]</a>, has two important distinctions with the Weibull model, namely 1) the method does not incorporate scale selection; each receptive field model has one specific receptive field size that is used across the entire image and 2) only one parameter (skewness or kurtosis) is used to describe the response distribution that results from contrast filtering, compared to the separate scale (beta) and shape (gamma) parameters used in the Weibull model.</p>



<h4 xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/title[1]">Experiment 1: EEG</h4>
<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[1]/title[1]">Experimental procedure.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[1]/p[1]">Nineteen subjects took part in this experiment. The dead leaves images were presented on a 19 inch Ilyama monitor, whose resolution was set at 1024×768 pixels with a frame rate of 60 Hz. Subjects were seated 90 cm from the monitor such that stimuli subtended 11×11° of visual angle. During EEG acquisition, a single image was presented in the center of the screen on a grey background for 100 ms, on average every 1500 ms (range 1000–2000 ms; <a href="#pcbi-1002726-g003"><strong>Fig. 3A</strong></a>). Each stimulus was presented twice, in two separate runs. Stimuli were presented intermixed with phase-scrambled versions of grayscale natural images; subjects were instructed to indicate which type of image they were shown. This instruction was intended to ensure that subjects attended to the stimuli: the required discrimination between the dead leaves and phase-scrambled natural images did not correspond to any distinction between the categories of dead leaves themselves. Examples of the two types of images were displayed prior to the experiment. Each run was subdivided in 8 blocks across which response mappings were counterbalanced. Stimuli were presented using the software package Presentation (<a href="http://www.neurobs.com">www.neurobs.com</a>).</p>
<div class="figure" xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[1]/fig[1]"><a name="pcbi-1002726-g003" id="pcbi-1002726-g003" title="Click for larger image " href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g003" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g003');"><div class="expand-link" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g003');"></div><img xpathLocation="noSelect" border="1" src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.g003&amp;representation=PNG_I" align="left" alt="thumbnail" class="thumbnail"></a><p><strong xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[1]/fig[1]/label[1]"><a xpathLocation="noSelect" href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g003" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g003');"><span>Figure 3. </span></a> <span xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[1]/fig[1]/caption[1]/title[1]">Methods and experimental design.</span></strong></p><p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[1]/fig[1]/caption[1]/p[1]">(<strong>A</strong>), Experimental set-up of experiment 1 (EEG experiment). Subjects were presented with individual images of dead leaves while EEG was recorded. Single-image evoked responses (ERPs) were computed for each electrode, by averaging two repeated presentations of each individual image. Regression analyses of ERP amplitude on contrast statistics were performed at each time sample and electrode. (<strong>B</strong>), Representational dissimilarity matrices (RDMs) were computed at each sample of the ERP. A single RDM displays Euclidean distance (red = high, blue = low) between multiple-electrode patterns of ERP amplitude between all pairs of stimuli at a specific moment in time. The (cartoon) inset demonstrates how dissimilarities can cluster by category: all images from one category are in consecutive rows and can be ‘similarly dissimilar’ to other categories. (<strong>C</strong>), Experimental set-up of experiment 2 (behavioral experiment). On each trial, subjects were presented with a pair of stimuli for 50 ms, followed by a mask after an interval of 100 ms. Subjects were presented 8 times with all possible pairings of stimuli and were instructed to indicate whether stimuli were the same or different. (<strong>D</strong>), Cartoon example of leave-one-out classification based on contrast statistics. One stimulus is selected in turn, after which the median (thumbnail) of the remaining stimuli of its category is computed, as well as the median of other categories (here, just one). Classification accuracy reflects how many stimuli are closer to the median of other categories instead of its own category in terms of distance in image statistics.</p>
<span xpathLocation="noSelect">doi:10.1371/journal.pcbi.1002726.g003</span><div class="clearer"></div><div class="figure-inline-download"><ul><li><strong>Download: </strong><a href="/article/info:doi/10.1371/journal.pcbi.1002726.g003/powerpoint">PowerPoint slide</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g003/largerimage">
                    larger image (<span id="info:doi/10.1371/journal.pcbi.1002726.g003.PNG_L"></span> PNG)</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g003/originalimage">
                    original image (<span id="info:doi/10.1371/journal.pcbi.1002726.g003.TIF"></span> TIFF)
                  </a></li></ul></div></div>
<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[2]/title[1]">EEG data acquisition.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[2]/p[1]">EEG Recordings were made with a Biosemi 64-channel Active Two EEG system (Biosemi Instrumentation BV, Amsterdam, NL, <a href="http://www.biosemi.com">www.biosemi.com</a>), with sintered Ag/AgCl electrodes at scalp positions including the standard 10-10 system along with intermediate positions and two additional occipital electrodes (I1 and I2), which replaced two frontal electrodes (F5 and F6). During recording, a CMS/DRL feedback loop was used as an active ground, followed by offline referencing to electrodes placed on the earlobes. The Biosemi hardware is completely DC-coupled, so no high-pass filter is applied during recording of the raw data. A Bessel low-pass filter was applied starting at 1/5<sup>th</sup> of the sample rate. Eye movements were monitored with a horizontal electro-oculogram (hEOG) placed lateral to both eyes and a vertical electro-oculogram (vEOG) positioned above and below the left eye, aligned with the pupil location when the participants looked straight ahead. Data was sampled at 256 Hz.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[3]/title[1]">EEG data preprocessing.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[3]/p[1]">The raw data was pre-processed using Brain Vision Analyzer by applying a high-pass filter at 0.1 Hz (12 dB/octave) and a low-pass filter at 30 Hz (24 dB/octave). Since this low-pass filter has a graded descent, it cannot be guaranteed that all high-frequency noise is removed; therefore, we additionally applied two notch filters at 50 (for line noise) and 60 Hz (for monitor noise). Deflections larger than 300 mV were automatically removed. Trials were segmented into epochs starting 100 ms before stimulus onset and ending at 500 ms after stimulus onset. These epochs were corrected for eye movements by removing the influence of ocular-generated EEG using a regression analysis based on the two horizontal and vertical EOG channels <a href="#pcbi.1002726-Gratton1">[40]</a>. Baseline correction was performed based on the data between −100 ms and 0 ms relative to stimulus onset; artifacts were rejected using maximal allowed voltage steps of 50 µV, minimal and maximal allowed amplitudes of −75 and 75 µV and a lowest allowed activity of 0.50 µV (median rejection rate across subjects was 7%, with a range of 1%–38%). The resulting event-related potentials (ERPs) were converted to Current Source Density (CSD) responses <a href="#pcbi.1002726-Perrin1">[41]</a>. This conversion results in a signal that is more localized in space, which has the advantage of more reliably reflecting activity of neural tissue underlying the recording electrode <a href="#pcbi.1002726-Nunez1">[42]</a>.</p>
<p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[3]/p[2]">Trials in which the same individual image was presented were averaged over the two runs, resulting in a single event-related potential (ERP) for each image and each subject. To address the concern that regression results (see below) might be artificially high due to averaging of ERPs over repetitions, we also conducted all analyses using first-trial estimates only; these are reported in <strong><a href="#pcbi.1002726.s004">Fig. S4</a></strong> and <strong><a href="#pcbi.1002726.s005">S5</a></strong>; the results were very similar to those obtained with repetition-averaged ERPs.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[4]/title[1]">Regression on single-image ERPs.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[4]/p[1]">To test whether differences between evoked neural responses could be predicted by differences in contrast statistics between images, we conducted regression analyses on the single-image ERPs (<a href="#pcbi-1002726-g003"><strong>Fig. 3A</strong></a>). The preprocessed ERPs were read into Matlab, where we conducted linear regression analyses of ERP amplitude on image parameters using the Statistics Toolbox. For each subject, each channel and each time-point, two image parameters (Weibull parameters; Fourier parameters; skewness/kurtosis) were entered together as linear regressors on ERP amplitude. This analysis results in a measure of model fit (r<sup>2</sup>) over time (each sample of the ERP) and space (each electrode) for each individual subject.</p>
<p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[4]/p[2]">To compare the results between different sets of statistics directly (within each subject), we used the Akaike information criterion (AIC, <a href="#pcbi.1002726-Akaike1">[43]</a> which measures the information contained in each set of predictors. In this procedure, we transformed the residual sum of squares (RSS) of the regression analysis based on each set of statistics into AIC-values using AIC = n*log(RSS/n)+2k where n = number of images and k is the number of predictors. AIC can be used for model selection given a set of candidate models of the same data, where the preferred model has minimum AIC-value <a href="#pcbi.1002726-Burnham1">[44]</a>.</p>
<p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[4]/p[3]">To test whether the various image parameters explained any unique variance, we ran an additional regression analysis using a full model in which all three sets of image statistics were entered simultaneously (resulting in a 6 parameter model). We compared the results obtained with the full model with models for which, in turn, each parameter was left out; by subtracting the r<sup>2</sup> values of each of these partial models from the full model, we quantified unique variance explained by individual predictors.</p>
<p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[4]/p[4]">To correct for multiple comparisons, the p-values associated with the regression results were FDR-corrected at α = 0.05, unless stated otherwise.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[5]/title[1]">Representational similarity analysis.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[5]/p[1]">To examine how variance between individual visual stimuli arises over time and space, we computed representational dissimilarity matrices (RDMs; <a href="#pcbi.1002726-Kriegeskorte1">[25]</a>) based on spatial patterns of evoked ERP amplitude. In this type of analysis, dissimilarity between patterns of activity evoked by individual images (measured as 1-correlation or Euclidean distance) is determined across multiple recording sites simultaneously (e.g., voxels in fMRI, <a href="#pcbi.1002726-Kriegeskorte2">[45]</a>). Here, we computed RDMs based on ERP amplitude at each time-point, using the spatial pattern of evoked activity across multiple electrode sites; we did this for each subject separately. Only electrodes showing substantial variance across the entire stimulus- and dataset were included (<strong><a href="#pcbi.1002726.s001">Fig. S1</a></strong>); these were I1, I2, Iz, O1, O2, Oz, POz, PO7, PO8, P6 and P8. Based on this multi-electrode data, we computed (per subject and time-point) for all pairs of images the Euclidean distance between their evoked ERP amplitude patterns. As a result, we obtained RDMs containing 256×256 ‘dissimilarity’ values at each time-point of the ERP (<a href="#pcbi-1002726-g003"><strong>Fig. 3B</strong></a>). Within one RDM, each cell reflects similarity in ERP amplitude patterns of the corresponding two images indicated by the row- and column number. We used Euclidean distance to quantify dissimilarity rather than the 1–correlation measure recommended for fMRI data <a href="#pcbi.1002726-Kriegeskorte2">[45]</a> because it corresponds more closely to the distance measure taken for the contrast statistics matrices (see below).</p>

<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[6]/title[1]">Comparison with distance matrices based on contrast statistics.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[3]/sec[6]/p[1]">To examine whether the dissimilarities between ERP patterns evoked by individual images could be predicted based on differences in contrast statistics, we computed pair-wise dissimilarity matrices based on the three sets of parameter values (Weibull statistics; Fourier statistics; distribution statistics). We computed the sum of the absolute differences between the (normalized) parameter values of each pair of images (reflecting distance in the parameter space formed by the image parameters, <a href="#pcbi-1002726-g001"><strong>Fig. 1B</strong></a>), resulting in one difference value between those two images. The matrices based on contrast statistics were compared with the RDMs based on the ERP data using a Mantel test for two-dimensional correlations <a href="#pcbi.1002726-Daniels1">[46]</a>, <a href="#pcbi.1002726-Mantel1">[47]</a>, denoted as r<sub>m</sub>. We computed these correlations for the average RDM across subjects as well as for single subjects RDMs. For the former, 95% confidence intervals for each correlation were assessed using a percentile bootstrap on the dissimilarity values <a href="#pcbi.1002726-Garthwaite1">[48]</a> with number of bootstraps = 10.000 (~40 * number of images).</p>



<h4 xpathLocation="/article[1]/body[1]/sec[2]/sec[4]/title[1]">Experiment 2: Behavior</h4>
<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[4]/sec[1]/title[1]">Behavioral data acquisition.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[4]/sec[1]/p[1]">Twelve participants took part in the behavioral experiment; none of them had participated in the EEG experiment. The dead leaves images were presented on a 19-inch Dell monitor with a resolution of 1280×1024 pixels and a frame rate of 60 Hz. On each trial, a fixation cross appeared at the center of the screen; after an interval of 500 ms, a pair of images was presented simultaneously for 50 ms, separated by a gap of 236 pixels (<a href="#pcbi-1002726-g003"><strong>Fig. 3C</strong></a>). A mask followed after 100 ms, and stayed on screen for 200 ms. Participants were seated approximately 90 cm from the monitor; the stimulus display subtended 27×11° of visual angle. Subjects were instructed to indicate if the images were from the same or a different category by pressing one of two designated buttons on a keyboard (‘z’ and ‘m’) that were mapped to the left or the right hand. They completed four blocks of 256 trials each. In each block, the 256 trials were determined as follows: of the 16 images per category, 15 were paired with a randomly drawn image from another category (different-category comparisons); the 16th was paired with a randomly drawn image from the other 15 of its own category (same-category comparisons). Images were drawn without replacement, such that each image occurred only once in each block (with exception of the images that were selected for the same-category comparisons, which therefore occurred more often). Every possible different-category comparison thus occurred twice per block, and the ratio of different-category vs. same-category comparisons was 15:1.</p>
<p xpathLocation="/article[1]/body[1]/sec[2]/sec[4]/sec[1]/p[2]">Before testing, subjects were informed that for most trials the stimuli were different, and that only some were the same, preventing them from adopting a balanced response (50-50) strategy. Also, subjects were shown a few example stimuli and performed 20 practice trials (none of which appeared in the main experiment) before starting the actual experiment. Masks were created by randomly placing four mini-blocks of 16×16 pixels from each of the 256 stimuli in a 512×512 frame. Unique masks were randomly assigned to each trial. The same mask was presented at the location of both stimuli. Stimuli were presented using the Matlab Psychophysics Toolbox <a href="#pcbi.1002726-Pelli1">[49]</a>, <a href="#pcbi.1002726-Brainard1">[50]</a>.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[4]/sec[2]/title[1]">Behavioral data analysis.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[4]/sec[2]/p[1]">In total, each possible combination of the 16 categories was presented 8 times in 4 consecutive blocks. Trials at which the subject failed to respond (&lt;1% for all subjects) within 1500 ms were discarded. Accuracy was determined by averaging across the four blocks. A mean confusion matrix was calculated by averaging accuracies across subjects separately for each specific combination of categories; we also calculated these matrices for each individual subject. We correlated both the mean confusion matrix and the individual matrices with classification accuracy based on contrast statistics (see below) using the Mantel test, resulting in one ‘mean’ and 12 individual correlation values. For these comparisons, the same-category comparisons were excluded (the Mantel test requires zero-values on the diagonal); they are included in the overall accuracy scores. Confidence intervals were determined using a percentile bootstrap (with number of bootstraps = 1000), which results in a 95% confidence interval along with the correlation.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[2]/sec[4]/sec[3]/title[1]">Classification analysis on contrast statistics.</h5><p xpathLocation="/article[1]/body[1]/sec[2]/sec[4]/sec[3]/p[1]">To compare the behavioral performance with distance in contrast statistics, we performed leave-one-out classification analyses based on the parameter values of each set of contrast statistics (Weibull statistics; Fourier statistics; skewness/kurtosis). We used a simple algorithm that determines a single measure of classification accuracy based on the amount of overlap between different categories in parameter values. This involved the following steps: First, the median parameter values of each category were calculated. In turn, one of the 256 stimuli was selected, after which a temporary median of other 15 stimuli of its own category was determined. Next, the difference between its parameter values (beta and gamma for Weibull statistics; intercept and slope for Fourier statistics; skewness and kurtosis for distribution statistics) and the temporary median of its own category was calculated, as well as the difference with the median of all other categories. If the difference with its own category was less than the difference with any other category, this stimulus was counted as a ‘hit’, otherwise it was assigned a ‘miss’ (a cartoon example is shown in <a href="#pcbi-1002726-g003"><strong>Fig. 3D</strong></a>). Classification accuracy was determined by counting the percentage of hits out of all comparisons. To determine significance, binomial density probabilities across all combinations in the dataset were calculated (the likelihood of a hit occurring rather than a miss) based on which an FDR-threshold was established that was used to correct the pair-wise classification accuracy values for multiple comparisons. Using the mean values for each category rather than the median to determine distances between images between yielded very similar results as those reported here.</p>


</div>

<div id="section3" xpathLocation="/article[1]/body[1]/sec[3]"><a id="s3" name="s3" toc="s3" title="Results"></a><h3 xpathLocation="noSelect">Results&nbsp;<a href="#top">Top</a></h3>
<h4 xpathLocation="/article[1]/body[1]/sec[3]/sec[1]/title[1]">Contrast statistics</h4>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[1]/p[1]">If we set out all 256 dead leaves images against the three sets of image statistics (Weibull parameters, Fourier parameters and skewness/kurtosis), stimuli cluster by category in all cases, with Fourier parameters leading to the most separable clusters (<a href="#pcbi-1002726-g004"><strong>Fig. 4A–C</strong></a>). There were considerable correlations between the various parameters (<a href="#pcbi-1002726-g004"><strong>Fig. 4D</strong></a>; individual correlations plots in <strong><a href="#pcbi.1002726.s002">Fig. S2</a></strong>). Skewness and kurtosis correlated highly (ρ = 0.91, p&lt;0.0001), but other significant correlations are observed as well, for example between Fourier slope and the Weibull beta parameter (ρ = 0.57, p&lt;0.0001) and also between the two Weibull parameters (ρ = 0.48, p&lt;0.001). A correlation of similar magnitude was also observed <a href="#pcbi.1002726-Scholte1">[17]</a> for natural scenes, supporting the notion that the dead leaves stimuli used here have similar low-level structure as natural stimuli.</p>
<div class="figure" xpathLocation="/article[1]/body[1]/sec[3]/sec[1]/fig[1]"><a name="pcbi-1002726-g004" id="pcbi-1002726-g004" title="Click for larger image " href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g004" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g004');"><div class="expand-link" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g004');"></div><img xpathLocation="noSelect" border="1" src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.g004&amp;representation=PNG_I" align="left" alt="thumbnail" class="thumbnail"></a><p><strong xpathLocation="/article[1]/body[1]/sec[3]/sec[1]/fig[1]/label[1]"><a xpathLocation="noSelect" href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g004" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g004');"><span>Figure 4. </span></a> <span xpathLocation="/article[1]/body[1]/sec[3]/sec[1]/fig[1]/caption[1]/title[1]">Stimuli set out against their respective contrast statistics.</span></strong></p><p xpathLocation="/article[1]/body[1]/sec[3]/sec[1]/fig[1]/caption[1]/p[1]">Each data-point reflects parameter values for a single image, color-coded by category. Individual images are displayed against their (<strong>A</strong>), Weibull parameters beta and gamma, (<strong>B</strong>), Fourier parameters intercept and (increasing negative) slope and (<strong>C</strong>), distribution properties skewness and kurtosis. In all cases, clustering by category based on parameter values is evident. (<strong>D</strong>), Non-parametric correlations between the six image parameters: Beta (B), Gamma (G), Fourier Intercept (Ic), Fourier Slope (S), Skewness (Sk) and Kurtosis (Ku).</p>
<span xpathLocation="noSelect">doi:10.1371/journal.pcbi.1002726.g004</span><div class="clearer"></div><div class="figure-inline-download"><ul><li><strong>Download: </strong><a href="/article/info:doi/10.1371/journal.pcbi.1002726.g004/powerpoint">PowerPoint slide</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g004/largerimage">
                    larger image (<span id="info:doi/10.1371/journal.pcbi.1002726.g004.PNG_L"></span> PNG)</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g004/originalimage">
                    original image (<span id="info:doi/10.1371/journal.pcbi.1002726.g004.TIF"></span> TIFF)
                  </a></li></ul></div></div><p xpathLocation="/article[1]/body[1]/sec[3]/sec[1]/p[2]">Interestingly, however, the ‘similarity spaces’ formed by each set of parameters are quite different between the various models. If Weibull parameters determine the axes of the similarity space (<a href="#pcbi-1002726-g004"><strong>Fig. 4A</strong></a>), highly cluttered images with many strong edges (e.g. 2D opaque stimuli with small disks) are located in the upper right corner (high gamma, high beta); images containing fewer edges (e.g. with larger disks) are found more on the left (low gamma); and most of the transparent stimuli, with weak edges, cluster together in the bottom of the space (low beta). For Fourier intercept and slope (<a href="#pcbi-1002726-g004"><strong>Fig. 4B</strong></a>), transparent categories are highly separated across the space: however, most images with strong edges end up in a similar part of the space (low slope, high intercept). Based on either skewness or kurtosis (<a href="#pcbi-1002726-g004"><strong>Fig. 4C</strong></a>), a few categories are distinct, but most tend to cluster together. These qualitative results suggest that all parameters are informative about clustering of image categories, but that they index different image properties.</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[1]/p[3]">Importantly, they give rise to different predictions about which categories should lead to similar evoked responses based on overlapping parameter values. We tested these predictions using the single-image ERP data.</p>


<h4 xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/title[1]">Experiment 1</h4>
<h5 xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/title[1]">Contrast statistics explain variance in occipital ERPs.</h5><p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/p[1]">Regression of single-image ERP amplitude (per subject, electrode and time-point) on contrast statistics showed that Weibull statistics explain a substantial amount of variance between individual images. Highest values were found at occipital channel Oz, where explained variance for all subjects reached a maximum between 100 and 210 ms after stimulus onset; maximal values ranged between r<sup>2</sup> = 0.12–0.80 (<a href="#pcbi-1002726-g005"><strong>Fig. 5A</strong></a>) and were highly significant (all p&lt;0.0001, FDR-corrected). For Fourier parameters (<a href="#pcbi-1002726-g005"><strong>Fig. 5B</strong></a>), somewhat lower values were found (max r<sup>2</sup> between 0.08–0.59, 100–210 ms; all p&lt;0.0001). For skewness and kurtosis (<a href="#pcbi-1002726-g005"><strong>Fig. 5C</strong></a>), explained variance was much lower and did not reach a consistent maximum during a specific time frame (max r<sup>2</sup> between 0.02–0.23 at 78–421 ms; maximal values were significant for 11 out of 19 subjects).</p>
<div class="figure" xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/fig[1]"><a name="pcbi-1002726-g005" id="pcbi-1002726-g005" title="Click for larger image " href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g005" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g005');"><div class="expand-link" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g005');"></div><img xpathLocation="noSelect" border="1" src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.g005&amp;representation=PNG_I" align="left" alt="thumbnail" class="thumbnail"></a><p><strong xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/fig[1]/label[1]"><a xpathLocation="noSelect" href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g005" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g005');"><span>Figure 5. </span></a> <span xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/fig[1]/caption[1]/title[1]">Regression analysis of EEG data: single subject results.</span></strong></p><p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/fig[1]/caption[1]/p[1]">Explained variance of ERP amplitude at channel Oz over time, for each individual subject (colored thin lines) and mean across subjects (black thick line), using as regressors either (<strong>A</strong>), Weibull parameters beta and gamma, (<strong>B</strong>), Fourier parameters intercept and slope and (<strong>C</strong>), skewness and kurtosis; single-trial results of these analyses can be found in <strong><a href="#pcbi.1002726.s004">Fig. S4</a></strong>. Insets display scalp plots of r<sup>2</sup> values for all electrodes at the time of maximal explained variance averaged over subjects (113 ms for Weibull/Fourier, 254 ms for skewness/kurtosis. (<strong>D</strong>), Grand average ERP amplitude (averaged over all subjects and all images) for an early and a late time-point of peak explained variance displayed in <strong>A–C</strong>.</p>
<span xpathLocation="noSelect">doi:10.1371/journal.pcbi.1002726.g005</span><div class="clearer"></div><div class="figure-inline-download"><ul><li><strong>Download: </strong><a href="/article/info:doi/10.1371/journal.pcbi.1002726.g005/powerpoint">PowerPoint slide</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g005/largerimage">
                    larger image (<span id="info:doi/10.1371/journal.pcbi.1002726.g005.PNG_L"></span> PNG)</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g005/originalimage">
                    original image (<span id="info:doi/10.1371/journal.pcbi.1002726.g005.TIF"></span> TIFF)
                  </a></li></ul></div></div><p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/p[2]">If we average the explained variance across subjects for each electrode separately at the time-points of maximal explained variance (113 ms for Weibull and Fourier statistics, 254 ms for skewness/kurtosis), we see (insets <a href="#pcbi-1002726-g005"><strong>Fig. 5A–C</strong></a>) that for all three sets of statistics, explained variance clusters around the midline occipital channels (Oz). Two weaker clusters were located near parietal electrodes, likely reflecting a dipole effect: both the early and late signals appear to originate from early visual areas (<a href="#pcbi-1002726-g005"><strong>Fig. 5D</strong></a>).</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/p[3]">These results demonstrate substantial differences in maximum explained variance between individual subjects. Inspection of the ERP recordings of each subject revealed a similarly large variability in subjects' signal-to-noise ratio (SNR, measured as the difference in ERP amplitude relative to pre-stimulus variability, reflecting the degree to which an evoked response is present). Indeed, the rank correlation between SNR and maximal explained variance by Weibull statistics was ρ = 0.69, p&lt;0.0014; see <strong><a href="#pcbi.1002726.s003">Fig. S3</a></strong>, which includes examples of subject-specific r<sup>2</sup> values alongside their single-image ERPs). This suggests that the observed variability in maximum explained variance is related to these subject-specific differences in SNR, which are in turn likely due to individual differences in cortical folding, scalp conductivity and recording conditions.</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/p[4]">In an alternative analysis performed on single-trial rather than single-image data (in which repeated presentations of the same stimulus were averaged, see <a href="#s2">Materials and Methods</a>), we found slightly lower explained variance for all models (maximal r<sup>2</sup> values: 0.71 for Weibull statistics, 0.52 for Fourier statistics, and 0.16 for skewness/kurtosis, respectively; see <strong><a href="#pcbi.1002726.s004">Fig. S4</a></strong>). Importantly, however, the relative differences between the sets of image parameters were fully consistent with those reported here.</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[1]/p[5]">Overall, the regression results show that Weibull contrast statistics, but also Fourier statistics, reliably predict activity evoked by individual dead leaves images at the individual subject level. To investigate differences between the contributions of the different image predictors, we ran several additional analyses that are described below.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[2]/title[1]">Comparisons between different image parameters.</h5><p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[2]/p[1]">In order to compare differences in explained variance for Weibull statistics compared to the other statistics (<a href="#pcbi-1002726-g006"><strong>Fig. 6A</strong></a>), we used Akaike's information criterion (AIC) to evaluate the relative ‘goodness of fit’ of each of the three sets of contrast statistics. AIC is computed from the residuals of regression analyses (see <a href="#s2">Materials and Methods</a>) and can be used for model selection given a set of candidate models of the same data, where the preferred model has minimum AIC-value. If we compare the mean AIC-value across individual subjects of Weibull, Fourier and skewness/kurtosis parameters over time, we find that the model fits start to diverge around 100 ms, with Weibull statistics leading to the lowest values (<a href="#pcbi-1002726-g006"><strong>Fig. 6B</strong></a>). It thus appears that Weibull parameters provide a better fit to the data than the other two sets of statistics. This could be related to the fact that the Weibull parameters characterize the histogram of contrast responses at a selected spatial scale, and may thus contain information reflected in both Fourier power spectra and higher-order properties of the contrast distribution. Therefore, we also computed AIC-values for intercept, slope, skewness and kurtosis combined into one regressor (<a href="#pcbi-1002726-g006"><strong>Fig. 6B</strong></a>, black line); the obtained values from this regression analysis are however still higher than those obtained from the Weibull parameters (significant differences between 117–140 ms, all t(19)&lt;−2.8, all p&lt;0.01). At the time-point of (mean) maximal explained variance (113 ms), the ordering of the different models in terms of AIC-values is consistent over subjects (<a href="#pcbi-1002726-g006"><strong>Fig. 6C</strong></a>): in all subjects, Weibull parameters lead to the best model fit, although differences are minimal for low SNR subjects. Interestingly, for subjects with high SNR, the distance between AIC-values for the Weibull model compared to the other contrast statistics appears to increase. These findings suggest that Weibull statistics capture additional variance relative to the other contrast statistics parameters considered here.</p>
<div class="figure" xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[2]/fig[1]"><a name="pcbi-1002726-g006" id="pcbi-1002726-g006" title="Click for larger image " href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g006" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g006');"><div class="expand-link" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g006');"></div><img xpathLocation="noSelect" border="1" src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.g006&amp;representation=PNG_I" align="left" alt="thumbnail" class="thumbnail"></a><p><strong xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[2]/fig[1]/label[1]"><a xpathLocation="noSelect" href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g006" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g006');"><span>Figure 6. </span></a> <span xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[2]/fig[1]/caption[1]/title[1]">AIC (Akaike information criterion) and unique explained variance analyses at channel Oz.</span></strong></p><p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[2]/fig[1]/caption[1]/p[1]">(<strong>A</strong>), Mean explained variance across single subjects for Weibull (red), Fourier (blue) and skewness/kurtosis (green), respectively; shaded areas indicate S.E.M. (<strong>B</strong>), Mean AIC-value across single subjects computed from the residuals of each of the three regression models, as well as an additional model (black) consisting of Fourier and skewness/kurtosis values combined, showing that Weibull parameters provide the best fit to the data (low AIC-value); shaded areas indicate S.E.M. (<strong>C</strong>), Single subject AIC-values for the models displayed in <strong>B</strong> at the time-point of maximal explained variance for Weibull and Fourier statistics (113 ms); subjects are sorted based on independently determined SNR ratio (reported in <strong><a href="#pcbi.1002726.s002">Fig. S2</a></strong>). (<strong>D</strong>), Unique explained variance by each set of contrast statistics. (<strong>E</strong>), Absolute, non-parametric correlations (Spearman's ρ) with ERP amplitude for the individual image parameters: Beta (B), Gamma (G), Fourier Intercept (Ic), Fourier Slope (S), distribution Skewness (Sk) and Kurtosis (Ku). Absolute values are plotted for convenience; shaded areas indicate S.E.M. (<strong>F</strong>), Unique explained variance by each individual parameter. Results for <strong>A–E</strong> based on single-trial rather than single-image data were highly similar (<strong><a href="#pcbi.1002726.s005">Fig. S5</a></strong>).</p>
<span xpathLocation="noSelect">doi:10.1371/journal.pcbi.1002726.g006</span><div class="clearer"></div><div class="figure-inline-download"><ul><li><strong>Download: </strong><a href="/article/info:doi/10.1371/journal.pcbi.1002726.g006/powerpoint">PowerPoint slide</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g006/largerimage">
                    larger image (<span id="info:doi/10.1371/journal.pcbi.1002726.g006.PNG_L"></span> PNG)</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g006/originalimage">
                    original image (<span id="info:doi/10.1371/journal.pcbi.1002726.g006.TIF"></span> TIFF)
                  </a></li></ul></div></div><p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[2]/p[2]">To demonstrate this in a different way, we computed the unique variance contributed by each set of contrast statistics (r<sup>2</sup><sub>unique</sub>) by comparing partial models with a full model consisting of all 6 parameters (see <a href="#s2">Materials and Methods</a>). Unique explained variance for each set of statistics was low (r<sup>2</sup><sub>unique</sub> for Weibull parameters reached a maximum of 0.07 at 109 ms; for Fourier, max r<sup>2</sup><sub>unique</sub> was 0.05 at 180 ms; for skewness/kurtosis, max r<sup>2</sup><sub>unique</sub> was 0.04 at 203 ms), but clearly highest for the Weibull parameters in an extended early time interval (~100–180 ms; <a href="#pcbi-1002726-g006"><strong>Fig. 6D</strong></a>). Given the substantial correlations between the various image parameters (reported in <a href="#pcbi-1002726-g004"><strong>Fig. 4D</strong></a>), we also tested the contribution of each parameter individually. From the correlations of individual parameters with ERP amplitude (<a href="#pcbi-1002726-g006"><strong>Fig. 6E</strong></a>), it can be readily seen that out of all parameters, the Weibull beta parameter correlates highest with the evoked activity in the early time-interval (max ρ = 0.57 at 121 ms, p&lt;0.001 in 18 out of 19 subjects, FDR-corrected); it also has highest unique explained variance (r<sup>2</sup><sub>unique</sub> reaching a max of 0.05 at 109 ms, <a href="#pcbi-1002726-g006"><strong>Fig. 6F</strong></a>), whereas the gamma parameter contributes unique variance somewhat later in time (max r<sup>2</sup><sub>unique</sub> was 0.04 at 164 ms), just before the Fourier parameters (a max of 0.03–0.04, around 175–180 ms).</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[2]/p[3]">Taken together, these additional analyses suggest that the differences in regression results between the various sets of contrast statistics reflect reliable and consistent differences in information about the stimulus carried by these statistics, with Weibull statistics resulting in the best fit to the differences observed in the neural data.</p>

<h5 xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/title[1]">Clustering-by-category of ERPs is predicted by Weibull statistics.</h5><p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/p[1]">The regression results indicate that the Weibull parameters are predictive of ERP amplitude, but do not reveal whether any categorical differences between ERPs are reflected in these parameters. To address this, we constructed representational dissimilarity matrices (RDMs) based on EEG activity. In this analysis, we computed RDMs of ERP amplitude using multiple electrodes as input (see <a href="#s2">Materials and Methods</a>) for each subject separately. This approach is akin to performing multi-voxel pattern analysis in fMRI and calculating the dissimilarity between these activity patterns, but now comparing ERP amplitude differences across electrodes instead of voxels. We computed one RDM for each time-point of the ERP and averaged RDMs over subjects.</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/p[2]">To demonstrate how these matrices can convey information about categorical properties of evoked responses, we selected the time-point at which maximal dissimilarities were found (<a href="#pcbi-1002726-g007"><strong>Fig. 7A</strong></a>; 101 ms after stimulus-onset). In this subject-averaged RDM (<a href="#pcbi-1002726-g007"><strong>Fig. 7B</strong></a>), we observe clustering by category: the matrix appears to consist of small blocks of 16×16 images that are minimally dissimilar amongst themselves (diagonal values), but that tend to differ from other categories (off-diagonal values). Moreover, differences between these blocks show that some categories are more dissimilar than others. Specifically, opaque categories (upper left quadrant) differ from one another and from transparent categories (lower left/upper right quadrant) whereas the transparent categories themselves tend to be minimally dissimilar (lower right quadrant).</p>
<div class="figure" xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/fig[1]"><a name="pcbi-1002726-g007" id="pcbi-1002726-g007" title="Click for larger image " href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g007" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g007');"><div class="expand-link" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g007');"></div><img xpathLocation="noSelect" border="1" src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.g007&amp;representation=PNG_I" align="left" alt="thumbnail" class="thumbnail"></a><p><strong xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/fig[1]/label[1]"><a xpathLocation="noSelect" href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g007" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g007');"><span>Figure 7. </span></a> <span xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/fig[1]/caption[1]/title[1]">Results of RDM analysis.</span></strong></p><p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/fig[1]/caption[1]/p[1]">(<strong>A</strong>), Maximum and mean Euclidean distance for the subject-averaged RDM: for both measures, highest dissimilarity between images was found at 101 ms after stimulus-onset. (<strong>B</strong>), Mean RDM across subjects at the moment of maximal Euclidean distance. Each cell of the matrix reflects the dissimilarity (red = high, blue = low) between two individual images, whose category is indexed on the x- and y-axis. (<strong>C</strong>), Dissimilarity matrices based on difference in contrast statistics between individual images. Color values indicate the summed difference between two individual images in beta and gamma (Weibull statistics), intercept and slope (Fourier statistics), skewness and kurtosis (distribution statistics). (<strong>D</strong>), Correlation between the RDM and each of the three dissimilarity matrices at each time-point. Highest correlation is found for Weibull statistics at 109 ms. Shaded areas reflect 95% confidence intervals obtained from a percentile bootstrap on the dissimilarity values.</p>
<span xpathLocation="noSelect">doi:10.1371/journal.pcbi.1002726.g007</span><div class="clearer"></div><div class="figure-inline-download"><ul><li><strong>Download: </strong><a href="/article/info:doi/10.1371/journal.pcbi.1002726.g007/powerpoint">PowerPoint slide</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g007/largerimage">
                    larger image (<span id="info:doi/10.1371/journal.pcbi.1002726.g007.PNG_L"></span> PNG)</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g007/originalimage">
                    original image (<span id="info:doi/10.1371/journal.pcbi.1002726.g007.TIF"></span> TIFF)
                  </a></li></ul></div></div><p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/p[3]">Next, we tested to what extent these category-specific differences between images in the ERP were predicted by contrast statistics. We calculated 256×256 distance matrices for each set of image parameters, in which we subtracted the parameter values of each image from the values of each other image (<a href="#pcbi-1002726-g007"><strong>Fig. 7C</strong></a>, see <a href="#s2">Materials and Methods</a>). For example, for the first cell in the upper left corner of the Weibull statistics distance matrix, we summed the difference in beta and gamma values between image 1 and 2 (<sub>bim1</sub>−b<sub>im2</sub>+g<sub>im1</sub>−g<sub>im2</sub>), for the cell next to it between image 1 and 3, etc. For the other two sets of statistics, beta and gamma were replaced by intercept and slope or skewness and kurtosis.</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/p[4]">By visual inspection alone, it is clear that distances between individual images in Weibull statistics are most similar to the ERP dissimilarities. Inter-matrix correlations (Mantel tests, <a href="#pcbi.1002726-Burnham1">[44]</a>) reveal that at nearly all time-points there is a substantially higher correlation of the RDM of the ERP signal with the distance matrix based on Weibull, relative to the other two statistics (<a href="#pcbi-1002726-g007"><strong>Fig. 7D</strong></a>). The highest correlations for Weibull and Fourier are found shortly after 100 ms (Weibull: r<sub>m</sub> = 0.67, 109 ms; Fourier: r<sub>m</sub> = 0.22, 113 ms) and are both significant after FDR-correction (p-values&lt;0.001), whereas the correlation between the RDMs and the skewness/kurtosis distance matrix does not reach significance. We also correlated the distance matrices based on contrast statistics with the subject-specific RDMs, confirming this result to be consistent over subjects; see <strong><a href="#pcbi.1002726.s006">Fig. S6</a></strong>. RDMs at all ERP time-points are provided in <strong><a href="#pcbi.1002726.s007">Video S1</a></strong> in the form of a short movie clip.</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[2]/sec[3]/p[5]">These results show that differences between image categories in ERP amplitude map onto differences in underlying Weibull statistics of individual images. Throughout the ERP, this model of low-level visual responses provides a better prediction of differences between images in neural response patterns than the other image parameters considered here. Moreover, the highest correlation between differences in Weibull statistics and ERP amplitude is near the time-point of maximal dissimilarity, where clustering by category in the ERP is clearly present. This clustering corresponds to the categorical organization in Weibull parameter space (<a href="#pcbi-1002726-g002"><strong>Fig. 2A</strong></a>), in which transparent categories were largely overlapping whereas stimuli with strong edges were more differentiated. In the next experiment, we asked whether this similarity space could not only predict early differences in ERP amplitude, but also behaviorally perceived similarity: do image categories with overlapping parameter values also look more alike?</p>



<h4 xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/title[1]">Experiment 2</h4>
<h5 xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/sec[1]/title[1]">Prediction of behavioral confusions.</h5><p xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/sec[1]/p[1]">Participants indicated for each possible combination of the 16 dead leaves categories whether these were the same or different category. Behavioral accuracy was high across all subjects (mean 93% correct, range 0.88–0.98), suggesting that subjects were well able to categorize these stimuli (<a href="#pcbi-1002726-g008"><strong>Fig. 8A</strong></a>). To generate specific predictions about categorical similarity based on contrast statistics, we conducted classification analyses using the distance between images in each of the three similarity spaces, testing how often proximity in parameter values resulted in classification of an image to another category than its own (see <a href="#s2">Materials and Methods</a> and <a href="#pcbi-1002726-g003"><strong>Fig. 3D</strong></a>). Mean classification accuracy based on distance in contrast statistics was high for all three sets of contrast statistics, with highest accuracy for the Fourier parameters (99%), subsequently for the Weibull parameters (94%) and finally for skewness/kurtosis (93%). Despite these high accuracies, errors were made in both behavior and classification: to test whether these errors occurred for specific combinations of categories, we summarized the average number of errors for each specific combination of categories in confusion matrices.</p>
<div class="figure" xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/sec[1]/fig[1]"><a name="pcbi-1002726-g008" id="pcbi-1002726-g008" title="Click for larger image " href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g008" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g008');"><div class="expand-link" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g008');"></div><img xpathLocation="noSelect" border="1" src="/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002726.g008&amp;representation=PNG_I" align="left" alt="thumbnail" class="thumbnail"></a><p><strong xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/sec[1]/fig[1]/label[1]"><a xpathLocation="noSelect" href="/article/info:doi/10.1371/journal.pcbi.1002726?imageURI=info:doi/10.1371/journal.pcbi.1002726.g008" onclick="return ambra.lightBox.show('info:doi/10.1371/journal.pcbi.1002726', 'info:doi/10.1371/journal.pcbi.1002726.g008');"><span>Figure 8. </span></a> <span xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/sec[1]/fig[1]/caption[1]/title[1]">Behavioral results and comparison with classification.</span></strong></p><p xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/sec[1]/fig[1]/caption[1]/p[1]">(<strong>A</strong>), Accuracy of behavioral categorization (open circles: single subjects, filled circle: mean) and of classification based on Weibull parameters, Fourier parameters or skewness and kurtosis. (<strong>B</strong>), Behavioral confusion matrix, displaying mean categorization accuracy for specific comparisons of categories. For each pair of categories the percentage of correct answers is displayed as a grayscale value. (<strong>C</strong>), Comparison of mean behavioral confusion matrix with classification results based on the three sets of contrast statistics. (<strong>D</strong>), Inter-matrix correlations of the classification errors for each set of statistics with the mean behavioral confusion matrix (left, mean) as well as those of individual participants (right, single subjects). For the mean correlation, error bars indicate 95% confidence intervals obtained using a percentile bootstrap on values within the mean confusion matrix.</p>
<span xpathLocation="noSelect">doi:10.1371/journal.pcbi.1002726.g008</span><div class="clearer"></div><div class="figure-inline-download"><ul><li><strong>Download: </strong><a href="/article/info:doi/10.1371/journal.pcbi.1002726.g008/powerpoint">PowerPoint slide</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g008/largerimage">
                    larger image (<span id="info:doi/10.1371/journal.pcbi.1002726.g008.PNG_L"></span> PNG)</a> |
                  <a href="/article/info:doi/10.1371/journal.pcbi.1002726.g008/originalimage">
                    original image (<span id="info:doi/10.1371/journal.pcbi.1002726.g008.TIF"></span> TIFF)
                  </a></li></ul></div></div><p xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/sec[1]/p[2]">From the mean behavioral confusion matrix (<a href="#pcbi-1002726-g008"><strong>Fig. 8B</strong></a>), it is clear that subjects systematically confused certain categories more often than others. Specifically, transparent two- and three- dimensional images (dark squares in lower right quadrant) are more often confused than their opaque counterparts, although there were also some specific errors within opaque categories (upper left quadrant). Few errors were made between transparent and opaque categories. Although mean classification performance based on the Fourier parameters is highest, it is clear that the pattern of classification errors based on Weibull statistics most resembles the pattern of categorical confusions in behavior (<a href="#pcbi-1002726-g008"><strong>Fig. 8C</strong></a>). As expected, the behavioral confusion matrix correlated significantly with classification errors made based on Weibull parameters (r<sub>m</sub> = 0.46, p&lt;0.001), whereas classification based on differences in Fourier parameters or skewness/kurtosis did not correlate with human performance (r<sub>m</sub> = 0.07, p = 0.21 and r<sub>m</sub> = −0.20, p = 0.03, respectively; although significant, a negative correlation indicates that classification errors are <em>opposed</em> to categorization errors made by human participants). Correlations of individual confusion matrices confirm this result across all subjects (<a href="#pcbi-1002726-g008"><strong>Fig. 8D</strong></a>; range individual Weibull r<sub>m</sub>-values 0.33–0.46, all p&lt;0.005, FDR-corrected).</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/sec[1]/p[3]">These results show that perceived similarity of dead leaves image categories can be predicted based on differences in statistics of low-level contrast responses. Whereas mean classification accuracy for all image parameters was high, the different image parameters yielded different predictions about expected errors if categorization were to be based on these values. In the case of Fourier statistics, classification predicted that subjects would hardly confuse any categories at all, whereas skewness/kurtosis classification predicted that other categories would be confused with each other than those that subjects actually judged as similar. Only the Weibull parameters correlated with specific errors made by human subjects during rapid categorization.</p>
<p xpathLocation="/article[1]/body[1]/sec[3]/sec[3]/sec[1]/p[4]">This suggests that out of the three similarity spaces presented in <a href="#pcbi-1002726-g004"><strong>Fig. 4</strong></a>, the arrangement of categories in Weibull space corresponds most closely to the actual perceptual similarity experienced by human subjects during a rapid categorization task.</p>


</div>

<div id="section4" xpathLocation="/article[1]/body[1]/sec[4]"><a id="s4" name="s4" toc="s4" title="Discussion"></a><h3 xpathLocation="noSelect">Discussion&nbsp;<a href="#top">Top</a></h3><p xpathLocation="/article[1]/body[1]/sec[4]/p[1]">Low-level contrast statistics, derived from pooling of early visual responses, can predict similarity of early visual evoked responses as well as perceptual similarity of model natural scene images. We show that Weibull statistics, derived from the output of contrast filters modeled after LGN receptive fields, correlate with perceived similarity of computationally defined dead leaves categories. These statistics explain a significant amount of variance in the early visual ERP signal and correlate with behavioral categorization performance. Based on differences in these statistics, we were able to predict specific dissimilarities in the neural signal as well as specific category confusions.</p>
<p xpathLocation="/article[1]/body[1]/sec[4]/p[2]">Interestingly, if we compare the results of experiment 1 and 2, we observe that subjects confused categories that were minimally dissimilar in ERP amplitude, which in turn were minimally different in Weibull statistics. Conversely, subjects accurately distinguished categories that were separable in their statistics, which was mirrored in high ERP dissimilarities. Also, correlations between Weibull statistics and neural responses were highest between 100 and 200 ms, well within the time frame that rapid categorization of natural images is thought to be constrained to <a href="#pcbi.1002726-Thorpe2">[51]</a>.</p>
<p xpathLocation="/article[1]/body[1]/sec[4]/p[3]">This work extends recent findings that statistical variations in low-level properties are important for understanding categorical generalization over single images <a href="#pcbi.1002726-Karklin1">[13]</a>. It has been demonstrated before that behavioral categorization can be predicted using computational modeling of low-level information: a neural network consisting of local filters that were first allowed to adapt to natural scene statistics could predict behavioral performance on an object categorization task <a href="#pcbi.1002726-Serre1">[52]</a>, and a computational model based on texture statistics accurately predicted human natural scene categorization performance <a href="#pcbi.1002726-Renninger1">[53]</a>. Here, we expand on these results by showing that a geometric ‘similarity space’ formed by low-level contrast statistics can predict a complex pattern of categorization confusions of model natural scene images.</p>

<h4 xpathLocation="/article[1]/body[1]/sec[4]/sec[1]/title[1]">Implications for processing of real natural scenes</h4>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[1]/p[1]">Whether low-level statistics are indeed actively exploited during scene or object categorization is a topic of considerable debate. Whereas some studies report that manipulation of low-level properties influences rapid categorization accuracy <a href="#pcbi.1002726-Johnson1">[54]</a>, <a href="#pcbi.1002726-Kaping1">[55]</a> as well as early EEG responses <a href="#pcbi.1002726-Hansen1">[56]</a>, <a href="#pcbi.1002726-Martinovic1">[57]</a>, other studies have shown that not all early visual activity is obliterated by equation of those properties <a href="#pcbi.1002726-Philiastides1">[58]</a>–<a href="#pcbi.1002726-Rousselet4">[60]</a> and, conversely, that early sensitivity to diagnostic information is revealed in stimuli that do not differ in low-level statistics <a href="#pcbi.1002726-vanRijsbergen1">[20]</a>, <a href="#pcbi.1002726-Schyns1">[61]</a>. We find that, at least for our set of simplified models of natural scene images, early differences in ERPs are correlated with low-level contrast statistics that are themselves also directly predictive of perceptual similarity.</p>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[1]/p[2]">It is however likely that the degree to which low-level properties are relevant for processing of natural image categories is highly dependent on stimulus type and context, even within actual natural scene stimuli: for example, low-level information may influence rapid detection of faces to a larger extent than objects <a href="#pcbi.1002726-Gaspar1">[22]</a> and the effects of low-level statistics on animal detection may interact with scene category (man-made vs. natural) <a href="#pcbi.1002726-Honey1">[62]</a>. In addition, the present work is very different from these previous reports in that our experiments did not require formation of a high-level representation but only a same-different judgment. There are also notable differences between our ERP effects and those obtained with standardized object/scene categories: our maximum explained variance was found at around 100 ms, whereas those studies report sensitivity starting at 120 ms and onwards <a href="#pcbi.1002726-BaconMac1">[63]</a>–<a href="#pcbi.1002726-Smith1">[66]</a>. Maximal sensitivity of evoked activity to faces and objects is found at lateral-occipital and parietal electrodes (PO, e.g. <a href="#pcbi.1002726-Philiastides1">[58]</a>), whereas our correlations are clustered around occipital electrode Oz. This suggests that the dead leaves images may mostly engage mid-level areas of visual processing, such as those sensitive to textural information, e.g. V2 <a href="#pcbi.1002726-Groen1">[24]</a>, <a href="#pcbi.1002726-Kastner1">[67]</a>–<a href="#pcbi.1002726-Freeman1">[69]</a>. Our results implicate that clustering of image similarities at this level of processing can, in principle, already predict perceptual similarity – in turn, these similarities can be derived from Weibull contrast statistics. Given that for natural scenes, the Weibull statistics explain similar amounts of variance in EEG activity as reported here, we can hypothesize that image similarities as predicted by Weibull statistics are also present in evoked activity to actual natural scenes.</p>


<h4 xpathLocation="/article[1]/body[1]/sec[4]/sec[2]/title[1]">Information contained in contrast statistics</h4>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[2]/p[1]">If Weibull statistics indeed approximate meaningful global information in natural images, which image features do they convey? By manipulating computational image categories in their perceptual appearance, we were able to get a better understanding of the information contained in the Weibull parameters. They appear to index the amount of clutter, i.e. are related to occlusion and object size. These properties may be relevant for natural scene categorization: a forest has a higher degree of clutter (high gamma) and lower mean edge strength (high beta) compared to a beach scene. An image containing a few strong edges (low beta) that are sparsely distributed (low gamma) has high probability of coinciding with a single salient object, for example a single bird against an empty sky, suggesting that these statistics may be relevant for object detection in natural scenes. Here, behavioral confusions (and corresponding dissimilarities in ERP signals) were found between stimuli without coherent edge information (transparent stimuli with either large or small disks), or that were highly cluttered (opaque stimuli with small disks) which were exactly the categories that overlapped in Weibull parameter values.</p>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[2]/p[2]">For comparison, we computed Fourier power spectra and higher-order properties of the contrast distribution (skewness and kurtosis), two sets of statistics that each index different sources of information in natural images: spatial frequency content and central moments of the contrast distribution, respectively. Deviations in the power spectra of natural images inform about variations in contrast across spatial scales: the slope and intercept parameters describe the ‘spectral signature’ of images <a href="#pcbi.1002726-Oliva2">[32]</a> which is diagnostic of scene category <a href="#pcbi.1002726-Torralba1">[15]</a>. Skewness and kurtosis were proposed to be relevant for texture perception <a href="#pcbi.1002726-Kingdom1">[35]</a>, <a href="#pcbi.1002726-Ruderman1">[70]</a> which in turn can be important for feature detection <a href="#pcbi.1002726-Renninger1">[53]</a>, <a href="#pcbi.1002726-Malik1">[71]</a> and the presence of featureless regions of images <a href="#pcbi.1002726-Brady1">[34]</a>, <a href="#pcbi.1002726-Landy1">[72]</a>. Our results confirm that both frequency content and central moments of the contrast distribution inform about image properties: both lead to accurate image classification. However, in the present study they did not predict neural and behavioral categorization patterns, suggesting that these statistics may not be plausible computations involved in visual processing of the dead leaves images.</p>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[2]/p[3]">Even though we used controlled, computationally defined image categories, it is still possible that an image property other that the contrast statistics tested here will provide a better prediction of the (neural and behavioral) data, for example one of the manipulations used to create the image categories (e.g., opacity). However, neither the observed clustering-by-category of ERPs in the RDM, nor the pattern of categorization errors in behavior mapped clearly onto one of the manipulations used to create the categories (e.g., opaque vs. transparent; as is visible in <a href="#pcbi-1002726-g007"><strong>Fig. 7B</strong></a>, there are also differences <em>within</em> opaque and transparent categories, and this complex pattern of clustering is only predicted by Weibull statistics).</p>


<h4 xpathLocation="/article[1]/body[1]/sec[4]/sec[3]/title[1]">Explaining the advantage of Weibull statistics</h4>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[3]/p[1]">Why is the Weibull model better than widely used contrast statistics in predicting early neural and perceptual similarity? Although higher order moments of distributions can be diagnostic of textural differences, they may in practice be difficult for the visual system to represent <a href="#pcbi.1002726-Kingdom1">[35]</a>. In addition, it has been suggested that rather than amplitude spectra, phase information derived from the Fourier transform <a href="#pcbi.1002726-Wichmann1">[73]</a>, <a href="#pcbi.1002726-Loschky1">[74]</a>, or the interaction between these two <a href="#pcbi.1002726-Gaspar2">[75]</a>, <a href="#pcbi.1002726-Joubert1">[76]</a> contains diagnostic scene information. The reason that higher-order statistics derived from the phase spectrum may contain perceptually relevant information <a href="#pcbi.1002726-Doi1">[77]</a> is that they carry edge information. In the Weibull model, contrasts, i.e. non-oriented edges, are explicitly computed (as the response of LGN-type neurons) and evaluated at multiple spatial scales. The model may thus be able to capture information contained both in power spectra (scale statistics) as well as central moments (distribution statistics). The Weibull parameters appear to reflect different aspects of low-level information: the beta parameter varies with the range of contrast strengths present in the image, reflecting overall <em>contrast energy</em>, whereas the gamma parameter varies with the degree of correlation between local contrast values, reflecting clutter or <em>spatial coherence</em>.</p>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[3]/p[2]">Obviously, the Weibull fit is still a mathematical construct. However, the two parameters can also be approximated in a more biologically plausible way: with our previous single-scale model <a href="#pcbi.1002726-Scholte1">[17]</a>, we demonstrated that simple summation of X- and Y-type LGN output corresponded strikingly well with the fitted Weibull parameters. Similarly, if the outputs of the multi-scale filter banks used here (reflecting the entire range of receptive field sizes of the LGN) are linearly summed, we again obtain values that correlate highly with the Weibull parameters obtained from the contrast histogram at minimal reliable scale (S. Ghebreab, H.S. Scholte, V.A.F. Lamme, A.W.M Smeulders, under review). This suggests that Weibull estimation can in fact be reduced to pooling of neuronal population responses by summation, which is a biologically realistic operation.</p>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[3]/p[3]">Why would summation of contrast responses of low-level neurons convey the same information as the Weibull parameters? This is likely a result of the structure of the world itself: distributions of contrast in natural images tend to range between power-law and Gaussian, which is the family of distributions that the Weibull function can capture <a href="#pcbi.1002726-Geusebroek1">[78]</a>. It appears that this statistic simply provides a good characterization of the dynamic range of the low-level input to the visual cortex when viewing natural images. Since our brain developed in a natural world, early visual processing may take advantage of this regularity in estimating global properties to arrive at a first impression of scene content.</p>


<h4 xpathLocation="/article[1]/body[1]/sec[4]/sec[4]/title[1]">Outlook</h4>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[4]/p[1]">The present results extend our previous findings <a href="#pcbi.1002726-Scholte1">[17]</a>, <a href="#pcbi.1002726-Ghebreab1">[18]</a> with natural images to other image types (computational categories) and to prediction of behavioral categorization. Interestingly, even though the subjects in experiment 1 (EEG) were not engaged in categorization of the dead leaves images, their results generalize to the behavioral categorization patterns that were found in experiment 2, suggesting that similarity of bottom-up responses measured in EEG - in a different person - can be predictive of the perceived similarity during categorization of these images. This observation is now restricted to computationally defined categories. An interesting question for future work is whether in construction of high-level categorical representations of natural stimuli - considered a computationally challenging task - the brain actively exploits the pattern of variability of the population response to low-level information, estimated from early receptive field output. Contrary to the classical view of the visual hierarchy (e.g., <a href="#pcbi.1002726-Riesenhuber1">[79]</a>) it has been proposed that a rapid, global percept of the input (gist) precedes a slow and detailed analysis of the scene <a href="#pcbi.1002726-Biederman1">[80]</a>–<a href="#pcbi.1002726-Hochstein1">[83]</a>. Natural image statistics provide a pointer to information that could be relevant for such a global percept <a href="#pcbi.1002726-Simoncelli1">[84]</a>, <a href="#pcbi.1002726-Geisler1">[85]</a>. However, the mechanism by which global information can be rapidly extracted from low-level properties is not directly evident from natural image statistics alone. As explained above, in our model, the statistics are derived from a biologically realistic substrate (the response of early visual contrast filters). We suggest that to build a realistic model of natural image categorization, it is essential to understand how statistics derived from very early, simple low-level responses can contribute to gist extraction.</p>
<p xpathLocation="/article[1]/body[1]/sec[4]/sec[4]/p[2]">In conclusion, our findings suggest that global information based on low-level contrast can be available very early in visual processing and that this information can be relevant for judgment of perceptual similarity of controlled image categories.</p>

</div>

<div id="section5" xpathLocation="/article[1]/body[1]/sec[5]"><a id="s5" name="s5" toc="s5" title="Supporting Information"></a><h3 xpathLocation="noSelect">Supporting Information&nbsp;<a href="#top">Top</a></h3><a name="pcbi.1002726.s001" id="pcbi.1002726.s001"></a><p><strong xPathLocation="noSelect"><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1002726.s001">Figure S1. </a></strong></p><p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[1]/caption[1]/p[1]">Selection of electrodes (Iz, I1, I2, Oz, O1, O2, POz, PO7, PO8, P6, P8) that were used as input to compute RDMs (dissimilarity matrices). Selection was based on standard deviation in ERP amplitude across the whole data set (all subjects and all images). Each line corresponds to a single electrode: only electrodes whose standard deviations crossed the dashed line were selected.</p>
<p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[1]/caption[1]/p[2]">(TIF)</p>
<a name="pcbi.1002726.s002" id="pcbi.1002726.s002"></a><p><strong xPathLocation="noSelect"><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1002726.s002">Figure S2. </a></strong></p><p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[2]/caption[1]/p[1]">Correlations of individual image parameters Weibull beta (<strong>A</strong>) and gamma (<strong>B</strong>) with Fourier intercept, Fourier slope, skewness and kurtosis values.</p>
<p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[2]/caption[1]/p[2]">(TIF)</p>
<a name="pcbi.1002726.s003" id="pcbi.1002726.s003"></a><p><strong xPathLocation="noSelect"><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1002726.s003">Figure S3. </a></strong></p><p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[3]/caption[1]/p[1]">Left: Correlation between subject-specific signal-to-noise ratio (SNR) and maximal explained variance (across all electrodes). SNR was computed by 1) per electrode, averaging the mean ERP amplitude across the 256 images over all post-stimulus time-points, 2) dividing the absolute value of this average by the standard deviation of all pre-stimulus time-points and 3) averaging the resulting SNR values over electrodes. The SNR-values thus reflect the degree to which stimulus-related ERP amplitude is present relative to baseline fluctuations. Right: two examples of evoked responses (CSD-transformed) for the 256 individual stimuli and corresponding explained variance values at channel Oz. Top: example of high SNR single-subject data; an ERP is clearly visible in individual trials; explained variance based on contrast statistics is high. Bottom: example of low SNR single-subject data; an evoked response is hardly discernable in the individual trials; explained variance based on contrast statistics is low. This result elegantly shows that if there is no evoked response present in the EEG signal, there is no stimulus-related variance to be explained by differences in contrast statistics.</p>
<p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[3]/caption[1]/p[2]">(TIF)</p>
<a name="pcbi.1002726.s004" id="pcbi.1002726.s004"></a><p><strong xPathLocation="noSelect"><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1002726.s004">Figure S4. </a></strong></p><p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[4]/caption[1]/p[1]">Explained variance values at channel Oz as reported in <a href="#pcbi-1002726-g005"><strong>Fig. 5A–C</strong></a>, but now computed based on non-averaged single-trial ERPs (compared to single-image ERPs that are averaged over repeats). As regressors, we used either (<strong>A</strong>), Weibull beta and gamma, (<strong>B</strong>), Fourier intercept and slope and (<strong>C</strong>), skewness and kurtosis. Colored thin lines: r<sup>2</sup> values for individual subjects. Black thick line: mean r<sup>2</sup> across subjects.</p>
<p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[4]/caption[1]/p[2]">(TIF)</p>
<a name="pcbi.1002726.s005" id="pcbi.1002726.s005"></a><p><strong xPathLocation="noSelect"><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1002726.s005">Figure S5. </a></strong></p><p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[5]/caption[1]/p[1]">AIC and unique variance analyses at channel Oz as reported in <a href="#pcbi-1002726-g006"><strong>Fig. 6</strong></a>, but now computed based on non-averaged single-trial ERPs (compared to single-image ERPs that are averaged over repeats). (<strong>A</strong>), Mean explained variance across subjects for Weibull (red), Fourier (blue) and skewness/kurtosis (green); shaded areas indicate S.E.M. (<strong>B</strong>), Mean AIC-value across single subjects computed from the residuals of each of the three regression models, as well as an additional model (black) consisting of Fourier and skewness/kurtosis values combined, shaded areas indicate S.E.M. (<strong>C</strong>), Single subject AIC-values at the time-point of maximal explained variance for Weibull and Fourier statistics (113 ms); subjects are sorted based on SNR ratio (reported in <strong><a href="#pcbi.1002726.s002">Fig. S2</a></strong>). (<strong>D</strong>), Unique explained variance by each set of contrast statistics. (<strong>E</strong>), Absolute, non-parametric correlations (Spearman's ρ) with ERP amplitude for the individual image parameters: Beta (B), Gamma (G), Fourier Intercept (Ic), Fourier Slope (S), distribution Skewness (Sk) and Kurtosis (Ku). Absolute values are plotted for convenience; shaded areas indicate S.E.M. (<strong>F</strong>), Unique explained variance by each individual image parameter.</p>
<p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[5]/caption[1]/p[2]">(TIF)</p>
<a name="pcbi.1002726.s006" id="pcbi.1002726.s006"></a><p><strong xPathLocation="noSelect"><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1002726.s006">Figure S6. </a></strong></p><p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[6]/caption[1]/p[1]">Single-subject correlations of dissimilarity matrices (RDMs) of ERPs with distance matrices based on the three sets of contrast statistics: (<strong>A</strong>), Weibull parameters, (<strong>B</strong>), Fourier parameters and (<strong>C</strong>), skewness and kurtosis.</p>
<p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[6]/caption[1]/p[2]">(TIF)</p>
<a name="pcbi.1002726.s007" id="pcbi.1002726.s007"></a><p><strong xPathLocation="noSelect"><a href="/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pcbi.1002726.s007">Video S1. </a></strong></p><p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[7]/caption[1]/p[1]">Representational dissimilarity matrices at each sample in time of the ERP, starting 50 ms before until 350 ms after stimulus-onset. Dissimilarity between stimuli is measured as Euclidean distance (red = maximal, blue = minimal values in entire data set) between ERP patterns across occipital electrodes (see <a href="#s2">Materials and Methods</a>). Categories are labeled on the x- and y-axis; each cell of the matrix indexes the dissimilarity between two individual stimuli. Differences between images suddenly emerge around 90 ms after stimulus-onset and disappear again about 60 ms later. These differences cluster in 16×16 blocks, suggesting that categorical information is present in this time period. Later in time, weaker differences arise, but not as large as before, suggesting that category-specific dissimilarities between stimuli are evoked early in time.</p>
<p xpathLocation="/article[1]/body[1]/sec[5]/supplementary-material[7]/caption[1]/p[2]">(MPG)</p>
</div>





<div xpathLocation="noSelect"><a id="ack" name="ack" toc="ack" title="Acknowledgments"></a><h3 xpathLocation="noSelect">Acknowledgments&nbsp;<a href="#top">Top</a></h3>
<p xpathLocation="/article[1]/back[1]/ack[1]/p[1]">We thank Judith Tankink for EEG data collection and Romke Rouw and Olympia Colizoli for helpful comments on earlier versions of this manuscript.</p>
</div><div class="contributions"><a id="authcontrib" name="authcontrib" toc="authcontrib" title="Author Contributions"></a><h3 xpathLocation="noSelect">Author Contributions&nbsp;<a href="#top">Top</a></h3><p xpathLocation="noSelect">Conceived and designed the experiments: SG VAFL HSS. Performed the experiments: IIAG. Analyzed the data: IIAG. Contributed reagents/materials/analysis tools: SG HSS. Wrote the paper: IIAG SG VAFL HSS.</p></div><div xpathLocation="noSelect"><a id="references" name="references" toc="references" title="References"></a><h3 xpathLocation="noSelect">References&nbsp;<a href="#top">Top</a></h3><ol class="references" xpathLocation="noSelect"><li xpathLocation="noSelect"><a name="pcbi.1002726-Potter1" id="pcbi.1002726-Potter1"></a>Potter MC (1975) Meaning in visual search. Science 187: 965–966.
        doi:
        <a href="http://dx.doi.org/10.1126/science.1145183">10.1126/science.1145183</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928095">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Greene1" id="pcbi.1002726-Greene1"></a>Greene MR, Oliva A (2009) The briefest of glances: the time course of natural scene understanding. Psych Sci 20: 464–472.
        doi:
        <a href="http://dx.doi.org/10.1111/j.1467-9280.2009.02316.x">10.1111/j.1467-9280.2009.02316.x</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928097">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-FeiFei1" id="pcbi.1002726-FeiFei1"></a>Fei-Fei L, VanRullen R, Koch C, Perona P (2002) Rapid natural scene categorization in the near absence of attention. Proc Natl Acad Sci U S A 99: 9596–9601. <a class="find" href="/article/findArticle.action?citedArticleID=5928099">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Thorpe1" id="pcbi.1002726-Thorpe1"></a>Thorpe S, Fize D, Marlot C (1996) Speed of processing in the human visual system. Nature 381: 520–522.
        doi:
        <a href="http://dx.doi.org/10.1038/381520a0">10.1038/381520a0</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928101">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Kirchner1" id="pcbi.1002726-Kirchner1"></a>Kirchner H, Thorpe SJ (2006) Ultra-rapid object detection with saccadic eye movements: visual processing speed revisited. Vision Res 46: 1762–1776.
        doi:
        <a href="http://dx.doi.org/10.1016/j.visres.2005.10.002">10.1016/j.visres.2005.10.002</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928103">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-VanRullen1" id="pcbi.1002726-VanRullen1"></a>VanRullen R, Thorpe S (2001) The time course of visual processing: from early perception to decision-making. J Cogn Neurosci 13: 454–461.
        doi:
        <a href="http://dx.doi.org/10.1162/08989290152001880">10.1162/08989290152001880</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928105">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Rousselet1" id="pcbi.1002726-Rousselet1"></a>Rousselet GA, Fabre-Thorpe M, Thorpe SJ (2002) Parallel processing in high-level categorization of natural images. Nature Neurosci 5: 629–630.
        doi:
        <a href="http://dx.doi.org/10.1038/nn866">10.1038/nn866</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928107">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Peelen1" id="pcbi.1002726-Peelen1"></a>Peelen MV, Fei-Fei L, Kastner S (2009) Neural mechanisms of rapid natural scene categorization in human visual cortex. Nature 460: 94–97.
        doi:
        <a href="http://dx.doi.org/10.1038/nature08103">10.1038/nature08103</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928109">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Field1" id="pcbi.1002726-Field1"></a>Field DJ (1987) Relations between the statistics of natural images and the response properties of cortical cells. J Opt Soc Am A 4: 2379–2394.
        doi:
        <a href="http://dx.doi.org/10.1364/JOSAA.4.002379">10.1364/JOSAA.4.002379</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928111">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Vinje1" id="pcbi.1002726-Vinje1"></a>Vinje WE, Gallant JL (2000) Sparse coding and decorrelation in primary visual cortex during natural vision. Science 287: 1273–1276.
        doi:
        <a href="http://dx.doi.org/10.1126/science.287.5456.1273">10.1126/science.287.5456.1273</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928113">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Schwartz1" id="pcbi.1002726-Schwartz1"></a>Schwartz O, Simoncelli EP (2001) Natural signal statistics and sensory gain control. Nature Neurosci 4: 819–825. <a class="find" href="/article/findArticle.action?citedArticleID=5928115">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Olshausen1" id="pcbi.1002726-Olshausen1"></a>Olshausen BA, Field DJ (1996) Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature 381: 607–610.
        doi:
        <a href="http://dx.doi.org/10.1038/381607a0">10.1038/381607a0</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928117">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Karklin1" id="pcbi.1002726-Karklin1"></a>Karklin Y, Lewicki MS (2009) Emergence of complex cell properties by learning to generalize in natural scenes. Nature 457: 83–86.
        doi:
        <a href="http://dx.doi.org/10.1038/nature07481">10.1038/nature07481</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928119">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Frazor1" id="pcbi.1002726-Frazor1"></a>Frazor RA, Geisler WS (2006) Local luminance and contrast in natural images. Vision Res 46: 1585–1598.
        doi:
        <a href="http://dx.doi.org/10.1016/j.visres.2005.06.038">10.1016/j.visres.2005.06.038</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928121">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Torralba1" id="pcbi.1002726-Torralba1"></a>Torralba A, Oliva A (2003) Statistics of natural image categories. Network 14: 391–412.
        doi:
        <a href="http://dx.doi.org/10.1088/0954-898X/14/3/302">10.1088/0954-898X/14/3/302</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928123">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Graham1" id="pcbi.1002726-Graham1"></a>Graham N (1979) Does the brain perform a Fourier analysis of the visual scene? Trends Neurosci 2: 207–208.
        doi:
        <a href="http://dx.doi.org/10.1016/0166-2236(79)90082-1">10.1016/0166-2236(79)90082-1</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928125">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Scholte1" id="pcbi.1002726-Scholte1"></a>Scholte HS, Ghebreab S, Waldorp L, Smeulders AWM, Lamme VAF (2009) Brain responses strongly correlate with Weibull image statistics when processing natural images. J Vis 9: 1–15.
        doi:
        <a href="http://dx.doi.org/10.1167/9.4.29">10.1167/9.4.29</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928127">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Ghebreab1" id="pcbi.1002726-Ghebreab1"></a>Ghebreab S, Smeulders AWM, Scholte HS, Lamme VAF (2009) A biologically plausible model for rapid natural image identification. Adv Neural Inf Process Syst 1–9. <a class="find" href="/article/findArticle.action?citedArticleID=5928129">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Oliva1" id="pcbi.1002726-Oliva1"></a>Oliva A, Torralba A (2006) Building the gist of a scene: the role of global image features in recognition. Prog Brain Res 155: 23–36. <a class="find" href="/article/findArticle.action?citedArticleID=5928131">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-vanRijsbergen1" id="pcbi.1002726-vanRijsbergen1"></a>van Rijsbergen NJ, Schyns PG (2009) Dynamics of trimming the content of face representations for categorization in the brain. PLoS Comput Biol 5: e1000561.
        doi:
        <a href="http://dx.doi.org/10.1371/journal.pcbi.1000561">10.1371/journal.pcbi.1000561</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928133">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Rousselet2" id="pcbi.1002726-Rousselet2"></a>Rousselet GA, Pernet CR (2011) Quantifying the time course of visual object processing using ERPs: It's time to up the game. Front Psych 2: 1–6.
        doi:
        <a href="http://dx.doi.org/10.3389/fpsyg.2011.00107">10.3389/fpsyg.2011.00107</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928135">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Gaspar1" id="pcbi.1002726-Gaspar1"></a>Gaspar CM, Rousselet GA, Pernet CR (2011) Reliability of ERP and single-trial analyses. Neuroimage 58: 620–629.
        doi:
        <a href="http://dx.doi.org/10.1016/j.neuroimage.2011.06.052">10.1016/j.neuroimage.2011.06.052</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928137">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Kahn1" id="pcbi.1002726-Kahn1"></a>Kahn DA, Harris AM, Wolk DA (2010) Temporally distinct neural coding of perceptual similarity and prototype bias. J Vis 10: 1–12.
        doi:
        <a href="http://dx.doi.org/10.1167/10.10.12">10.1167/10.10.12</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928139">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Groen1" id="pcbi.1002726-Groen1"></a>Groen IIA, Ghebreab S, Lamme VAF, Scholte HS (2012) Low-level contrast statistics are diagnostic of invariance of natural textures. Front Comput Neurosci 6: 34.
        doi:
        <a href="http://dx.doi.org/10.3389/fncom.2012.00034">10.3389/fncom.2012.00034</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928141">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Kriegeskorte1" id="pcbi.1002726-Kriegeskorte1"></a>Kriegeskorte N, Mur M, Bandettini P (2008) Representational similarity analysis - connecting the branches of systems neuroscience. Front Syst Neurosci 2: 4.
        doi:
        <a href="http://dx.doi.org/10.3389/neuro.06.004.2008">10.3389/neuro.06.004.2008</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928143">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Kravitz1" id="pcbi.1002726-Kravitz1"></a>Kravitz DJ, Peng CS, Baker CI (2011) Real-world scene representations in high-level visual cortex: it's the spaces more than the places. J Neurosci 31: 7322–7333.
        doi:
        <a href="http://dx.doi.org/10.1523/JNEUROSCI.4588-10.2011">10.1523/JNEUROSCI.4588-10.2011</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928145">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Ross1" id="pcbi.1002726-Ross1"></a>Ross MG, Oliva A (2010) Estimating perception of scene layout properties from global image features. J Vis 10: 1–25.
        doi:
        <a href="http://dx.doi.org/10.1167/10.1.2">10.1167/10.1.2</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928147">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Hsiao1" id="pcbi.1002726-Hsiao1"></a>Hsiao WH, Millane RP (2005) Effects of occlusion, edges, and scaling on the power spectra of natural images. J Opt Soc Am A 22: 1789–1797.
        doi:
        <a href="http://dx.doi.org/10.1364/JOSAA.22.001789">10.1364/JOSAA.22.001789</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928149">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Shepard1" id="pcbi.1002726-Shepard1"></a>Shepard RN (1964) Attention and the metric structure of the stimulus space. J Math Psych 1: 54–87.
        doi:
        <a href="http://dx.doi.org/10.1016/0022-2496(64)90017-3">10.1016/0022-2496(64)90017-3</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928151">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Drucker1" id="pcbi.1002726-Drucker1"></a>Drucker DM, Aguirre GK (2009) Different spatial scales of shape similarity representation in lateral and ventral LOC. Cereb Cortex 19: 2269–2280.
        doi:
        <a href="http://dx.doi.org/10.1093/cercor/bhn244">10.1093/cercor/bhn244</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928153">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-OpdeBeeck1" id="pcbi.1002726-OpdeBeeck1"></a>Op de Beeck HP, Wagemans J, Vogels R (2008) The representation of perceived shape similarity and its role for category learning in monkeys: a modeling study. Vision Res 48: 598–610.
        doi:
        <a href="http://dx.doi.org/10.1016/j.visres.2007.11.019">10.1016/j.visres.2007.11.019</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928155">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Oliva2" id="pcbi.1002726-Oliva2"></a>Oliva A, Torralba A (2001) Modeling the shape of the scene: A holistic representation of the spatial envelope. Int J Comput Vis 42: 145–175. <a class="find" href="/article/findArticle.action?citedArticleID=5928157">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Tadmor1" id="pcbi.1002726-Tadmor1"></a>Tadmor Y, Tolhurst DJ (2000) Calculating the contrasts that retinal ganglion cells and LGN neurones encounter in natural scenes. Vision Res 40: 3145–3157.
        doi:
        <a href="http://dx.doi.org/10.1016/S0042-6989(00)00166-8">10.1016/S0042-6989(00)00166-8</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928159">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Brady1" id="pcbi.1002726-Brady1"></a>Brady N, Field DJ (2000) Local contrast in natural images: normalisation and coding efficiency. Perception 29: 1041–1055.
        doi:
        <a href="http://dx.doi.org/10.1068/p2996">10.1068/p2996</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928161">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Kingdom1" id="pcbi.1002726-Kingdom1"></a>Kingdom FAA, Hayes A, Field DJ (2001) Sensitivity to contrast histogram differences in synthetic wavelet-textures. Vision Res 41: 585–598.
        doi:
        <a href="http://dx.doi.org/10.1016/S0042-6989(00)00284-4">10.1016/S0042-6989(00)00284-4</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928163">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Bonin1" id="pcbi.1002726-Bonin1"></a>Bonin V, Mante V, Carandini M (2005) The suppressive field of neurons in lateral geniculate nucleus. J Neurosci 25: 10844–10856.
        doi:
        <a href="http://dx.doi.org/10.1523/JNEUROSCI.3562-05.2005">10.1523/JNEUROSCI.3562-05.2005</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928165">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Croner1" id="pcbi.1002726-Croner1"></a>Croner LJ, Kaplan E (1995) Receptive fields of P and M ganglion cells across the primate retina. Vision Res 35: 7–24.
        doi:
        <a href="http://dx.doi.org/10.1016/0042-6989(94)E0066-T">10.1016/0042-6989(94)E0066-T</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928167">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Elder1" id="pcbi.1002726-Elder1"></a>Elder JH, Zucker SW (1998) Local scale control for edge detection and blur estimation. IEEE Trans Pattern Anal Mach Intell 20: 699–716. <a class="find" href="/article/findArticle.action?citedArticleID=5928169">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Deng1" id="pcbi.1002726-Deng1"></a>Deng J, Dong W, Socher R, Li L-J, Li K, et al.. (2009) ImageNet: A large-scale hierarchical image database. In: Proceedings of the Conference on Computer Vision and Pattern Recognition; 2–25 June 2009; Miami, FL, 248–255. CPVR 2009. </li><li xpathLocation="noSelect"><a name="pcbi.1002726-Gratton1" id="pcbi.1002726-Gratton1"></a>Gratton G, Coles MGH, Donchin E (1983) A new method for off-line removal of ocular artifact. Electroencephalogr Clin Neurophysiol 55: 468–484.
        doi:
        <a href="http://dx.doi.org/10.1016/0013-4694(83)90135-9">10.1016/0013-4694(83)90135-9</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928173">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Perrin1" id="pcbi.1002726-Perrin1"></a>Perrin F (1989) Spherical splines for scalp potential and current density mapping. Electroencephalogr Clin Neurophysiol 72: 184–187.
        doi:
        <a href="http://dx.doi.org/10.1016/0013-4694(89)90180-6">10.1016/0013-4694(89)90180-6</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928175">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Nunez1" id="pcbi.1002726-Nunez1"></a>Nunez PL, Srinivasan R (2006) The neurophysics of EEG. 2nd ed,. Oxford, UK: Oxford University Press. </li><li xpathLocation="noSelect"><a name="pcbi.1002726-Akaike1" id="pcbi.1002726-Akaike1"></a>Akaike H (1973) Information theory and an extension of the maximum likelihood principle. In: Petrov BN, Csaki F, editors. Second International Symposium on Information Theory. Budapest: Akademiai Kiado. pp. 267–281. </li><li xpathLocation="noSelect"><a name="pcbi.1002726-Burnham1" id="pcbi.1002726-Burnham1"></a>Burnham KP, Anderson DR (2004) Multimodel Inference: Understanding AIC and BIC in Model Selection. Sociol Methods Res 33: 261–304.
        doi:
        <a href="http://dx.doi.org/10.1177/0049124104268644">10.1177/0049124104268644</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928181">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Kriegeskorte2" id="pcbi.1002726-Kriegeskorte2"></a>Kriegeskorte N, Mur M, Ruff DA, Kiani R, Bodurka J, et al. (2008) Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron 60: 1126–1141.
        doi:
        <a href="http://dx.doi.org/10.1016/j.neuron.2008.10.043">10.1016/j.neuron.2008.10.043</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928183">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Daniels1" id="pcbi.1002726-Daniels1"></a>Daniels HE (1944) The relation between measures of correlation in the universe of sample permutations. Biometrika 33: 129–135.
        doi:
        <a href="http://dx.doi.org/10.2307/2334112">10.2307/2334112</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928185">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Mantel1" id="pcbi.1002726-Mantel1"></a>Mantel N, Valand RS (1970) A technique of nonparametric multivariate analysis. Biometrics 26: 547–558.
        doi:
        <a href="http://dx.doi.org/10.2307/2529108">10.2307/2529108</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928187">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Garthwaite1" id="pcbi.1002726-Garthwaite1"></a>Garthwaite PH (1996) Confidence intervals from randomization tests. Biometrics 52: 1387–1393.
        doi:
        <a href="http://dx.doi.org/10.2307/2532852">10.2307/2532852</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928189">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Pelli1" id="pcbi.1002726-Pelli1"></a>Pelli DG (1997) The VideoToolbox software for visual psychophysics: Transforming numbers into movies. Spat Vis 10: 437–442.
        doi:
        <a href="http://dx.doi.org/10.1163/156856897X00366">10.1163/156856897X00366</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928191">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Brainard1" id="pcbi.1002726-Brainard1"></a>Brainard DH (1997) The Psychophysics Toolbox. Spat Vis 10: 433–436.
        doi:
        <a href="http://dx.doi.org/10.1163/156856897X00357">10.1163/156856897X00357</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928193">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Thorpe2" id="pcbi.1002726-Thorpe2"></a>Thorpe S (2009) The speed of categorization in the human visual system. Neuron 62: 168–170.
        doi:
        <a href="http://dx.doi.org/10.1016/j.neuron.2009.04.012">10.1016/j.neuron.2009.04.012</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928195">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Serre1" id="pcbi.1002726-Serre1"></a>Serre T, Oliva A, Poggio T (2007) A feedforward architecture accounts for rapid categorization. Proc Natl Acad Sci U S A 104: 6424–6429.
        doi:
        <a href="http://dx.doi.org/10.1073/pnas.0700622104">10.1073/pnas.0700622104</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928197">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Renninger1" id="pcbi.1002726-Renninger1"></a>Renninger LW, Malik J (2004) When is scene identification just texture recognition? Vision Res 44: 2301–2311  doi:<a href="http://dx.doi.org/10.1016/j.visres.2004.04.006">10.1016/j.visres.2004.04.006</a>. </li><li xpathLocation="noSelect"><a name="pcbi.1002726-Johnson1" id="pcbi.1002726-Johnson1"></a>Johnson JS, Olshausen BA (2003) Timecourse of neural signatures of object recognition. J Vis 3: 499–512.
        doi:
        <a href="http://dx.doi.org/10.1167/3.7.4">10.1167/3.7.4</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928201">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Kaping1" id="pcbi.1002726-Kaping1"></a>Kaping D, Tzvetanov T, Treue S (2007) Adaptation to statistical properties of visual scenes biases rapid categorization. Vis Cogn 15: 12–19.
        doi:
        <a href="http://dx.doi.org/10.1080/13506280600856660">10.1080/13506280600856660</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928203">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Hansen1" id="pcbi.1002726-Hansen1"></a>Hansen BC, Jacques T, Johnson AP, Ellemberg D (2011) From spatial frequency contrast to edge preponderance: the differential modulation of early visual evoked potentials by natural scene stimuli. Vis Neurosci 28: 221–237.
        doi:
        <a href="http://dx.doi.org/10.1017/S095252381100006X">10.1017/S095252381100006X</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928205">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Martinovic1" id="pcbi.1002726-Martinovic1"></a>Martinovic J, Mordal J, Wuerger SM (2011) Event-related potentials reveal an early advantage for luminance contours in the processing of objects. J Vis 11: 1–15.
        doi:
        <a href="http://dx.doi.org/10.1167/11.7.1">10.1167/11.7.1</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928207">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Philiastides1" id="pcbi.1002726-Philiastides1"></a>Philiastides MG, Ratcliff R, Sajda P (2006) Neural representation of task difficulty and decision making during perceptual categorization: A timing diagram. J Neurosci 26: 8965–8975.
        doi:
        <a href="http://dx.doi.org/10.1523/JNEUROSCI.1655-06.2006">10.1523/JNEUROSCI.1655-06.2006</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928209">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Rousselet3" id="pcbi.1002726-Rousselet3"></a>Rousselet GA, Husk JS, Bennett PJ, Sekuler AB (2008) Time course and robustness of ERP object and face differences. J Vis 8: 1–18.
        doi:
        <a href="http://dx.doi.org/10.1167/8.12.3">10.1167/8.12.3</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928211">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Rousselet4" id="pcbi.1002726-Rousselet4"></a>Rousselet GA, Husk JS, Bennett PJ, Sekuler AB (2005) Spatial scaling factors explain eccentricity effects on face ERPs. J Vis 5: 755–763.
        doi:
        <a href="http://dx.doi.org/10.1167/5.10.1">10.1167/5.10.1</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928213">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Schyns1" id="pcbi.1002726-Schyns1"></a>Schyns PG, Thut G, Gross J (2011) Cracking the code of oscillatory activity. PLoS Biol 9: e1001064.
        doi:
        <a href="http://dx.doi.org/10.1371/journal.pbio.1001064">10.1371/journal.pbio.1001064</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928215">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Honey1" id="pcbi.1002726-Honey1"></a>Honey C, Kirchner H, VanRullen R (2008) Faces in the cloud: Fourier power spectrum biases ultrarapid face detection. J Vis 8: 1–13.
        doi:
        <a href="http://dx.doi.org/10.1167/8.12.9">10.1167/8.12.9</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928217">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-BaconMac1" id="pcbi.1002726-BaconMac1"></a>Bacon-Macé N, Macé MJ-M, Fabre-Thorpe M, Thorpe SJ (2005) The time course of visual processing: backward masking and natural scene categorisation. Vision Res 45: 1459–1469.
        doi:
        <a href="http://dx.doi.org/10.1016/j.visres.2005.01.004">10.1016/j.visres.2005.01.004</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928219">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Philiastides2" id="pcbi.1002726-Philiastides2"></a>Philiastides MG, Sajda P (2006) Temporal characterization of the neural correlates of perceptual decision making in the human brain. Cereb Cortex 16: 509–518.
        doi:
        <a href="http://dx.doi.org/10.1093/cercor/bhi130">10.1093/cercor/bhi130</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928221">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Rousselet5" id="pcbi.1002726-Rousselet5"></a>Rousselet GA, Pernet CR, Bennett PJ, Sekuler AB (2008) Parametric study of EEG sensitivity to phase noise during face processing. BMC Neurosci 9: 98.
        doi:
        <a href="http://dx.doi.org/10.1186/1471-2202-9-98">10.1186/1471-2202-9-98</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928223">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Smith1" id="pcbi.1002726-Smith1"></a>Smith ML, Fries P, Gosselin F, Goebel R, Schyns PG (2009) Inverse mapping the neuronal substrates of face categorizations. Cereb Cortex 19: 2428–2438.
        doi:
        <a href="http://dx.doi.org/10.1093/cercor/bhn257">10.1093/cercor/bhn257</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928225">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Kastner1" id="pcbi.1002726-Kastner1"></a>Kastner S, Weerd PD, Ungerleider LG (2000) Texture segregation in the human visual cortex: A functional MRI study. J Neurophys 83: 2453–2457. <a class="find" href="/article/findArticle.action?citedArticleID=5928227">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Scholte2" id="pcbi.1002726-Scholte2"></a>Scholte HS, Jolij J, Fahrenfort JJ, Lamme VAF (2008) Feedforward and recurrent processing in scene segmentation: electroencephalography and functional magnetic resonance imaging. J Cogn Neurosci 20: 2097–2109.
        doi:
        <a href="http://dx.doi.org/10.1162/jocn.2008.20142">10.1162/jocn.2008.20142</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928229">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Freeman1" id="pcbi.1002726-Freeman1"></a>Freeman J, Simoncelli EP (2011) Metamers of the ventral stream. Nature Neurosci 14: 1195–1201.
        doi:
        <a href="http://dx.doi.org/10.1038/nn.2889">10.1038/nn.2889</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928231">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Ruderman1" id="pcbi.1002726-Ruderman1"></a>Ruderman D (1994) The statistics of natural images. Network 5: 517–548.
        doi:
        <a href="http://dx.doi.org/10.1088/0954-898X/5/4/006">10.1088/0954-898X/5/4/006</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928233">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Malik1" id="pcbi.1002726-Malik1"></a>Malik J, Perona P (1990) Preattentive texture discrimination with early vision mechanisms. J Opt Soc Am A 7: 923–932.
        doi:
        <a href="http://dx.doi.org/10.1364/JOSAA.7.000923">10.1364/JOSAA.7.000923</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928235">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Landy1" id="pcbi.1002726-Landy1"></a>Landy MS, Graham N (2004) Visual perception of texture. In: Chalupa LM, Werner JS, editors. The visual neurosciences. Cambridge, MA; MIT Press. pp. 1106–1118. </li><li xpathLocation="noSelect"><a name="pcbi.1002726-Wichmann1" id="pcbi.1002726-Wichmann1"></a>Wichmann FA, Braun DI, Gegenfurtner KR (2006) Phase noise and the classification of natural images. Vision Res 46: 1520–1529.
        doi:
        <a href="http://dx.doi.org/10.1016/j.visres.2005.11.008">10.1016/j.visres.2005.11.008</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928239">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Loschky1" id="pcbi.1002726-Loschky1"></a>Loschky LC, Larson AM (2008) Localized information is necessary for scene categorization, including the natural/man-made distinction. J Vis 8: 1–9.
        doi:
        <a href="http://dx.doi.org/10.1167/8.1.4">10.1167/8.1.4</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928241">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Gaspar2" id="pcbi.1002726-Gaspar2"></a>Gaspar CM, Rousselet GA (2009) How do amplitude spectra influence rapid animal detection? Vision Res 49: 3001–3012.
        doi:
        <a href="http://dx.doi.org/10.1016/j.visres.2009.09.021">10.1016/j.visres.2009.09.021</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928243">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Joubert1" id="pcbi.1002726-Joubert1"></a>Joubert OR, Rousselet GA, Fabre-Thorpe M, Fize D (2009) Rapid visual categorization of natural scene contexts with equalized amplitude spectrum and increasing phase noise. J Vis 9: 2.1–16.
        doi:
        <a href="http://dx.doi.org/10.1167/9.1.2">10.1167/9.1.2</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928245">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Doi1" id="pcbi.1002726-Doi1"></a>Doi E, Lewicki MS (2005) Relations between the statistical regularities of natural images and the response properties of the early visual system. In: Proceedings of the Workshop of Special Interest Group of Pattern Recognition and Perception Model (SIG P&amp;P); 28 July 2005; Kyoto University 2005. Japanese Cognitive Science Society, pp. 1–8. </li><li xpathLocation="noSelect"><a name="pcbi.1002726-Geusebroek1" id="pcbi.1002726-Geusebroek1"></a>Geusebroek J, Smeulders AWM (2005) A six-stimulus theory for stochastic texture. Int J Comput Vis 62: 7–16. <a class="find" href="/article/findArticle.action?citedArticleID=5928249">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Riesenhuber1" id="pcbi.1002726-Riesenhuber1"></a>Riesenhuber M, Poggio T (1999) Hierarchical models of object recognition in cortex. Nature Neurosci 2: 1019–1025. <a class="find" href="/article/findArticle.action?citedArticleID=5928251">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Biederman1" id="pcbi.1002726-Biederman1"></a>Biederman I (1972) Perceiving real world scenes. Science 177: 77–80.
        doi:
        <a href="http://dx.doi.org/10.1126/science.177.4043.77">10.1126/science.177.4043.77</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928253">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Oliva3" id="pcbi.1002726-Oliva3"></a>Oliva A, Schyns PG (1997) Coarse blobs or fine edges? Evidence that information diagnosticity changes the perception of complex visual stimuli. Cogn Psychol 34: 72–107.
        doi:
        <a href="http://dx.doi.org/10.1006/cogp.1997.0667">10.1006/cogp.1997.0667</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928255">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Oliva4" id="pcbi.1002726-Oliva4"></a>Oliva A (2005) Gist of the Scene. In: Itti L, Rees G, Tsotsos JK, editors. The Encyclopedia of neurobiology of attention. San Francisco, CA: Elsevier. pp. 251–257. </li><li xpathLocation="noSelect"><a name="pcbi.1002726-Hochstein1" id="pcbi.1002726-Hochstein1"></a>Hochstein S, Ahissar M (2002) View from the top: Hierarchies and reverse hierarchies in the visual system. Neuron 36: 791–804. <a class="find" href="/article/findArticle.action?citedArticleID=5928259">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Simoncelli1" id="pcbi.1002726-Simoncelli1"></a>Simoncelli EP, Olshausen BA (2001) Natural image statistics and neural representation. Annu Rev Neurosci 24: 1193–1216. <a class="find" href="/article/findArticle.action?citedArticleID=5928261">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Geisler1" id="pcbi.1002726-Geisler1"></a>Geisler WS (2008) Visual perception and the statistical properties of natural scenes. Annu Rev Psychol 59: 167–192.
        doi:
        <a href="http://dx.doi.org/10.1146/annurev.psych.58.110405.085632">10.1146/annurev.psych.58.110405.085632</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928263">
                      Find this article online
                    </a></li><li xpathLocation="noSelect"><a name="pcbi.1002726-Olmos1" id="pcbi.1002726-Olmos1"></a>Olmos A, Kingdom FAA (2004) A biologically inspired algorithm for the recovery of shading and reflectance images. Perception 33: 1463–1473.
        doi:
        <a href="http://dx.doi.org/10.1068/p5321">10.1068/p5321</a>.
       <a class="find" href="/article/findArticle.action?citedArticleID=5928265">
                      Find this article online
                    </a></li></ol></div>

</div></div>
<div style="display:none">
<div dojoType="ambra.widget.RegionalDialog" id="AnnotationDialog" style="padding:0;margin:0;">
  <div class="dialog annotate">
    <div class="tipu" id="dTipu"></div>
    <div class="comment">
      <h5><span>Post Your Note (For Public Viewing)</span></h5>
      <div class="posting pane">
        <form name="createAnnotation" id="createAnnotation" method="post" action="">
          <input type="hidden" name="target" value="info:doi/10.1371/journal.pcbi.1002726" />
          <input type="hidden" name="startPath" value="" />
          <input type="hidden" name="startOffset" value="" />
          <input type="hidden" name="endPath" value="" />
          <input type="hidden" name="endOffset" value="" />
          <input type="hidden" name="commentTitle" id="commentTitle" value="" />
          <input type="hidden" name="comment" id="commentArea" value="" />
          <input type="hidden" name="ciStatement" id="statementArea" value="" />
          <input type="hidden" name="isCompetingInterest" id="isCompetingInterest" value="false" />
          <input type="hidden" name="noteType" id="noteType" value="" />
          <fieldset>
            <legend>Compose Your Note</legend>
            <span id="submitMsg" class="error" style="display:none;"></span>
            <table class="layout">
              <tr>
                <td>
                  <label for="cNoteType">This is a </label><select name="cNoteType" id="cNoteType"><option value="note">note</option><option value="correction">correction</option></select>
                  <span id="cdls" style="visibility:hidden;margin-left:0.3em; white-space:nowrap;"><a href="/static/commentGuidelines.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC?target=info%3Adoi%2F10.1371%2Fjournal.pcbi.1002726#corrections">What are corrections?</a></span>
                  <label for="cTitle"><span class="none">Enter your note title</span><!-- error message text <em>A title is required for all public notes</em>--></label>
                  <input type="text" name="cTitle" id="cTitle" value="Enter your note title..." class="title" alt="Enter your note title..." />
                  <label for="cArea"><span class="none">Enter your note</span><!-- error message text <em>Please enter your note</em>--></label>
                  <textarea name="cArea" id="cArea" value="Enter your note..." alt="Enter your note...">Enter your note...</textarea>
                  <input type="hidden" name="isPublic" value="true" />
                </td>
                <td>&nbsp;</td>
                <td class="coi">
                  <fieldset>
                    <legend>Declare any competing interests.</legend>
                    <ul>
                      <li><label><input id="isCompetingInterestNo" type="radio" checked="checked" name="competingInterest" value="false"  /> No, I don't have any competing interests to declare.</label></li>
                      <li><label><input id="isCompetingInterestYes" type="radio" name="competingInterest" value="true"  /> Yes, I have competing interests to declare (enter below):</label></li>
                    </ul>
                    <textarea name="ciStatementArea" id="ciStatementArea" disabled value="Enter your competing interests..." alt="Enter your competing interests...">Enter your competing interests...</textarea>
                  </fieldset>
                </td>
              </tr>
              <tr>
                <td colspan="3" class="buttons">
                  <input type="button" value="Cancel" title="Click to close and cancel" id="btn_cancel"/>
                  <input type="button" value="Submit" title="Click to post your note publicly" id="btn_post" class="primary"/>
                </td>
              </tr>
              <tr>
                <td colspan="3">
                  <p>Notes and Corrections can include the following markup tags:</p>
                  <p><strong>Emphasis:</strong> ''<em>italic</em>''&nbsp;&nbsp;'''<strong>bold</strong>'''&nbsp;&nbsp;'''''<strong><em>bold italic</em></strong>'''''</p>
                  <p><strong>Other:</strong> ^^<sup>superscript</sup>^^&nbsp;&nbsp;~~<sub>subscript</sub>~~</p>
                </td>
              </tr>
            </table>
          </fieldset>
        </form>
      </div>
    </div>
    <div class="tip" id="dTip"></div>
  </div>
</div>
<div dojoType="ambra.widget.ContextAction" id="ContextActionDialog" class="contextActionDialog">
  <div class="dialog context">
    <div class="tipu" id="caTipu"></div>
    <div class="contextActionContent">
      <h5><img src="/images/tooltip_addannotation.gif" /> Add a note to this text.</h5>
      Please follow our <a href="/static/commentGuidelines.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">guidelines for notes and comments</a> and review our <a href="/static/competing.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">competing interests policy</a>. Comments that do not conform to our guidelines will be promptly removed and the user account disabled. The following must be avoided:
      <ul>
        <li>Remarks that could be interpreted as allegations of misconduct</li>
        <li>Unsupported assertions or statements</li>
        <li>Inflammatory or insulting language</li>
      </ul>
      <form name="contextActionForm" id="contextActionForm" class="clearfix buttons" method="post" action="">
        <input type="button" name="Continue" value="Continue" id="ContextActionDialogContinueButton" onmouseup="ambra.displayAnnotationContext.startComment(event);" title="Add a note to this text" class="primary"/>
        <input type="button" name="Cancel" value="Cancel" id="ContextActionDialogCancelButton" onclick="return false;" onmouseup="ambra.displayAnnotationContext.cancelContext(event);" title="Close this Window"/>
      </form>
    </div>
    <div class="tip" id="caTip"></div>
  </div>
</div>
<div dojoType="ambra.widget.ContextAction" id="ContextActionDialogNotLogged" class="contextActionDialog">
  <div class="dialog context">
    <div class="tipu" id="canlTipu"></div>
    <div class="contextActionContent">
      <h5><img src="/images/tooltip_addannotation.gif" /> Add a note to this text.</h5>
      You must be logged in to add a note to an article.
      You may log in by <a onmousedown="ambra.displayAnnotationContext.disconnect(event);" href="/user/secure/secureRedirect.action?goTo=%2Farticle%2FfetchArticle.action%3FarticleURI%3Dinfo%253Adoi%252F10.1371%252Fjournal.pcbi.1002726">clicking here</a> or <a href="#" onclick="return false;" onmouseup="ambra.displayAnnotationContext.cancelContext(event);">cancel this note</a>.
    </div>
    <div class="tip" id="canlTip"></div>
  </div>
</div>
<div dojoType="ambra.widget.ContextAction" id="ContextActionDialogBadSelection" class="contextActionDialog">
  <div class="dialog context">
    <div class="tipu" id="canBDTipu"></div>
    <div class="contextActionContent">
      <h5 class="annotation icon"><img src="/images/tooltip_addannotation.gif" /> Add a note to this text.</h5>
      You cannot annotate this area of the document. <a href="#" onclick="return false;" onmouseup="ambra.displayAnnotationContext.cancelContext(event);">Close</a>
    </div>
    <div class="tip" id="canBDTip"></div>
  </div>
</div>
<div dojoType="ambra.widget.ContextAction" id="ContextActionDialogBadRangeSelection" class="contextActionDialog">
  <div class="dialog context">
    <div class="tipu" id="canbrTipu"></div>
    <div class="contextActionContent">
      <h5><img src="/images/tooltip_addannotation.gif" /> Add a note to this text.</h5>
      You cannot create an annotation that spans different sections of the document; please adjust your selection.<br/>
      <a href="#" onclick="return false;" onmouseup="ambra.displayAnnotationContext.cancelContext(event);">Close</a>
    </div>
    <div class="tip" id="canbrTip"></div>
  </div>
</div>
<div dojoType="ambra.widget.RegionalDialog" id="CommentDialog" style="padding:0;margin:0;">
  <div class="dialog preview">
    <div class="tipu" id="cTipu"></div>
    <div class="btn close" id="btn_close" title="Click to close"><a title="Click to close">Close</a></div>
    <div id="cmtContainer" class="comment">
      <h6 id="viewCmtTitle"></h6>
      <div class="detail" id="viewCmtDetail"></div>
      <div class="contentwrap" id="viewComment"></div>
      <div class="contentwrap" id="viewCIStatement"></div>
      <div class="detail" id="viewLink">
        <!--<a href="#" class="commentary icon" title="Click to view full thread and respond">View all responses</a>
        <a href="#" class="respond tooltip" title="Click to respond to this posting">Respond to this</a>-->
      </div>
    </div>
    <div class="tip" id="cTip"></div>
  </div>
</div>
<div dojoType="ambra.widget.RegionalDialog" id="CommentDialogMultiple" style="padding:0;margin:0;">
  <div class="dialog multiple preview">
    <div class="tipu" id="mTipu"></div>
    <div class="btn close" id="btn_close_multi" title="Click to close"><a title="Click to close">Close</a></div>
    <ol id="multilist"></ol>
    <br/>
    <div id="multidetail"></div>
    <div class="tip" id="mTip"></div>
  </div>
</div>


<div dojoType="dijit.Dialog" id="Rating">
  <div class="dialog annotate">
    <div class="tipu" id="dTipu"></div>
    <div class="comment">
      <h5><span>Rate This Article</span></h5>
      <div class="instructions">Please follow our <a  href="/static/ratingGuidelines.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">guidelines for rating</a> and review our <a href="/static/competing.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC">competing interests policy</a>. Comments that do not conform to our guidelines will be promptly removed and the user account disabled. The following must be avoided:
        <ol>
          <li>Remarks that could be interpreted as allegations of misconduct</li>
          <li>Unsupported assertions or statements</li>
          <li>Inflammatory or insulting language</li>
        </ol>
      </div>
      <div class="posting pane">
        <form name="ratingForm" id="ratingForm" method="post" action="">
          <input type="hidden" name="articleURI" value="info:doi/10.1371/journal.pcbi.1002726" />  
          <input type="hidden" name="commentTitle" id="commentTitle" value="" />
          <input type="hidden" name="comment" id="commentArea" value="" />
          <input type="hidden" name="ciStatement" id="statementArea" value="" />
          <input type="hidden" name="isCompetingInterest" id="isCompetingInterest" value="" />
          <fieldset>
            <legend>Compose Your Annotation</legend>
            <span id="submitRatingMsg" class="error" style="display:none;"></span>
            <table class="layout">
              <tr>
                <td rowspan="2">
                  <label for="insight">Insight</label>
                  <ul class="star-rating rating edit" title="Rate insight" id="rateInsight">
                    <li class="current-rating pct0"></li>
                    <li><a href="javascript:void(0);" title="Bland" class="one-star" onclick="ambra.rating.setRatingCategory(this, 'insight', 1);">1</a></li>
                    <li><a href="javascript:void(0);" title="" class="two-stars" onclick="ambra.rating.setRatingCategory(this, 'insight', 2);">2</a></li>
                    <li><a href="javascript:void(0);" title="" class="three-stars" onclick="ambra.rating.setRatingCategory(this, 'insight', 3);">3</a></li>
                    <li><a href="javascript:void(0);" title="" class="four-stars" onclick="ambra.rating.setRatingCategory(this, 'insight', 4);">4</a></li>
                    <li><a href="javascript:void(0);" title="Profound" class="five-stars" onclick="ambra.rating.setRatingCategory(this, 'insight', 5);">5</a></li>
                  </ul>    
                  <input type="hidden" name="insight" title="insight" value="" />
                  <label for="reliability">Reliability</label>
                  <ul class="star-rating rating edit" title="Rate reliability" id="rateReliability">
                    <li class="current-rating pct0"></li>
                    <li><a href="javascript:void(0);" title="Tenuous" class="one-star" onclick="ambra.rating.setRatingCategory(this, 'reliability', 1);">1</a></li>
                    <li><a href="javascript:void(0);" title="" class="two-stars" onclick="ambra.rating.setRatingCategory(this, 'reliability', 2);">2</a></li>
                    <li><a href="javascript:void(0);" title="" class="three-stars" onclick="ambra.rating.setRatingCategory(this, 'reliability', 3);">3</a></li>
                    <li><a href="javascript:void(0);" title="" class="four-stars" onclick="ambra.rating.setRatingCategory(this, 'reliability', 4);">4</a></li>
                    <li><a href="javascript:void(0);" title="Unassailable" class="five-stars" onclick="ambra.rating.setRatingCategory(this, 'reliability', 5);">5</a></li>
                  </ul>    
                  <input type="hidden" name="reliability" title="reliability" value="" />
                  <label for="style">Style</label>
                  <ul class="star-rating rating edit" title="Rate style" id="rateStyle">
                    <li class="current-rating pct0"></li>
                    <li><a href="javascript:void(0);" title="Crude" class="one-star" onclick="ambra.rating.setRatingCategory(this, 'style', 1);">1</a></li>
                    <li><a href="javascript:void(0);" title="" class="two-stars" onclick="ambra.rating.setRatingCategory(this, 'style', 2);">2</a></li>
                    <li><a href="javascript:void(0);" title="" class="three-stars" onclick="ambra.rating.setRatingCategory(this, 'style', 3);">3</a></li>
                    <li><a href="javascript:void(0);" title="" class="four-stars" onclick="ambra.rating.setRatingCategory(this, 'style', 4);">4</a></li>
                    <li><a href="javascript:void(0);" title="Elegant" class="five-stars" onclick="ambra.rating.setRatingCategory(this, 'style', 5);">5</a></li>
                  </ul>    
                  <input type="hidden" name="style" title="style" value="" />
                  <label for="cTitle"><span class="none">Enter your comment title</span><!-- error message text <em>A title is required for all public annotations</em>--></label>
                  <input type="text" name="cTitle" id="cTitle" value="Enter your comment title..." class="title" alt="Enter your comment title..." />
                  <label for="cArea"><span class="none">Enter your comment</span><!-- error message text <em>Please enter your annotation</em>--></label>
                  <textarea name="cArea" id="cArea" value="Enter your comment..." alt="Enter your comment...">Enter your comment...</textarea>
                </td>
                <td rowspan="2">&nbsp;</td>
                <td class="coi">
                  <fieldset>
                    <legend>Declare any competing interests.</legend>
                    <ul>
                      <li><label><input id="isCompetingInterestNo" type="radio" name="competingInterest" value="false"  /> No, I don't have any competing interests to declare.</label></li>
                      <li><label><input id="isCompetingInterestYes" type="radio" name="competingInterest" value="true"  /> Yes, I have competing interests to declare (enter below):</label></li>
                    </ul>
                    <textarea name="ciStatementArea" id="ciStatementArea" disabled value="Enter your competing interests..." title="Enter your competing interests...">Enter your competing interests...</textarea>
                  </fieldset>
                </td>
              </tr>
              <tr>
                <td class="buttons">
                  <input type="button" value="Cancel" title="Click to close and cancel" id="btn_cancel_rating"/>
                  <input type="button" value="Submit" title="Click to post your annotation publicly" id="btn_post_rating" class="primary"/>
                </td>
              </tr>
              <tr>
                  <td colspan="3"><p>Ratings can include the following markup tags:</p>
                  <p><strong>Emphasis:</strong> ''<em>italic</em>''&nbsp;&nbsp;'''<strong>bold</strong>'''&nbsp;&nbsp;'''''<strong><em>bold italic</em></strong>'''''</p>
                  <p><strong>Other:</strong> ^^<sup>superscript</sup>^^&nbsp;&nbsp;~~<sub>subscript</sub>~~</p></td>
              </tr>
            </table>
          </fieldset>
        </form>
      </div>
    </div>
  </div>
</div>
<div dojoType="ambra.widget.LoadingCycle" id="LoadingCycle" class="loadingCycler">
  <img src="/images/loading.gif" width="58" height="58" title="Loading..." />
</div>
<div dojoType="dijit.Dialog" id="LightBox">

    <!-- start main window wrapper-->
    <div class="figure-window-wrapper">

      <!--start film strip Wrapper-->
      <div class="figure-window-nav-wrapper">
        <div id="figure-window-nav"></div>
      </div><!--end figure-window-nav-wrapper-->

      <!-- start figure container-->
      <div class="figure-window-container">

        <!-- start/end figure viewer-->
        <div id="figure-window-viewer">
          <img id="figure-window-img" class="large"/>
        </div>

        <!-- start/end figure doi-->
        <div id="figure-window-doi"></div>

        <!--start figure title-box-->
        <div id="figure-window-title-box" class="figure-window-title-less">
          <span id="figure-window-title"></span>
          <div id="figure-window-more" class="figure-window-more" onclick="return ambra.lightBox.showMoreOrLess();"></div>
          <div id="figure-window-description"></div>
        </div><!--end figure title-box-->

        <!--start figure download-->
        <div class="figure-window-download">
          <ul>
            <li class="download icon"><strong>Download:</strong>
              <a id="figure-window-ppt" title="Click to download PowerPoint slide of this image">PowerPoint
                slide</a> |
              <a id="figure-window-large" title="Click to download a larger version of this image">larger image
                (<span id="figure-window-large-size"></span>)</a> |
              <a id="figure-window-tiff" title="Click to download a original version of this image">original image
                (<span id="figure-window-tiff-size"></span>)</a>
            </li>
          </ul>
        </div><!--end figure download-->

        <!--start/end of close, context, next and previous button-->
        <div class="figure-window-close" onclick="return ambra.lightBox.hide();"></div>
        <div class="figure-window-context" onclick="return ambra.lightBox.showInContext();"></div>
        <div id="figure-window-previous" onclick="return ambra.lightBox.showPrevious();"></div>
        <div id="figure-window-next" onclick="return ambra.lightBox.showNext();"></div>

      </div><!--end figure-window-container-->

    </div><!--end figure-window-wrapper-->

</div><!--end dijit.Dialog-->
</div>
</div>
<!-- end : main contents -->

</div>
<!-- end : container -->	

<!-- begin : footer -->
<div id="ftr">
  <p><span>All site content, except where otherwise noted, is licensed under a <a href="http://creativecommons.org/licenses/by/2.5/" title="Creative Commons Attribution License 2.5" tabindex="200">Creative Commons Attribution License</a>.</span></p>
  <ul>
    <li><a href="/static/privacy.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" title="PLOS Privacy Statement" tabindex="501">Privacy Statement</a></li>
    <li><a href="/static/terms.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" title="PLOS Terms of Use" tabindex="502">Terms of Use</a></li>
    <li><a href="http://www.plos.org/advertise/" title="Advertise With PLOS" tabindex="503">Advertise</a></li>
    <li><a href="http://www.plos.org/about/media-inquiries/" title="PLOS Embargo Policy" tabindex="504">Media Inquiries</a></li>
    <li><a href="http://www.plos.org/publish/order-reprints/" title="PLOS in Print" tabindex="505">PLOS in Print</a></li>
    <li><a href="/static/sitemap.action;jsessionid=27798ADAA937AC4537C756CBCF05EBCC" title="Site Map" tabindex="506">Site Map</a></li>
    <li><a href="http://www.plos.org" title="PLOS.org" tabindex="507">PLOS.org</a></li>
  </ul>
  <div class="powered">
  <ul>
    <li><a href="/static/releaseNotes.action" title="Ambra | Release Notes">Ambra 2.4.9</a></li>
    <li>Managed Colocation provided by <a href="http://www.isc.org/" title="Internet Systems Consortium">Internet Systems Consortium</a>.</li>
  </ul>
  </div>
</div>
<!-- end : footer -->
<script type="text/javascript">
  var _namespace="";
    var loggedIn = false;

  var almHost = "http://alm.plos.org";
  var solrHost = "http://api.plos.org/search";
  var pubGetHost = "http://pubget.com/widgetizer/links_hash";
  var appContext = "";

  // Safari v3.1.1 "console.debug" issue (http://trac.dojotoolkit.org/ticket/6849) workaround
  if (/3[\.0-9]+ Safari/.test(navigator.appVersion)) {
    window.console = {
      origConsole: window.console,
      log: function(s){ this.origConsole.log(s); },
      info: function(s){ this.origConsole.info(s); },
      error: function(s){ this.origConsole.error(s); },
      warn: function(s){ this.origConsole.warn(s); }
    };
  }

  var djConfig = {
    // don't debug for IE - as dojo's firebug lite module is error prone in IE
    isDebug: false,
    parseOnLoad: true
  };

  var bamGAID = 'UA-338393-1';
  var bamGAVPV = location.pathname + location.search + location.hash;
</script>
<script type="text/javascript" src="/javascript/dojo/dojo/dojo.js?v=pzlwVCPPo+oJC12upPgJrJ5EG_k"></script>
<script type="text/javascript" src="/javascript/dojo/dojo/ambra.js?v=Y8JM0CVD9YwjC99M_jvbcRTiGhY"></script>
<script type="text/javascript" src="/javascript/init_global.js?v=fF03pfG3r+Nkj_czZ8ORg8o5sNM"></script>
<script type="text/javascript" src="/javascript/init_article.js?v=qzBD_a7nKbeMM2xNUtB3vckN+Zc"></script>
<script type="text/javascript" src="/javascript/init_ratings.js?v=seKykuS0X33CQGS7Lj+8FtBy5Uw"></script>
<script type="text/javascript" src="/javascript/init_lightBox.js?v=vJRLPy7RRX8Zp31Ogcm3i+TA4f8"></script>
<script type="text/javascript" src="/javascript/init_article_body.js?v=YgFei2xCi50CahwaPbSsxUxUsmw"></script>
<script type="text/javascript" src="/javascript/init_article_rhc.js?v=djtbTMOagorKE6ZPrkCwufAfWWI"></script>
<script type="text/javascript" src="/javascript/alm.js?v=1tH9JZ8uDtXspXssGJDr8O+jBOs"></script>
<script type="text/javascript" src="/javascript/reporting/articleViewsCumulative.js?v=RQ+gCrcAl0dBeLbmokxgQZTGYYA"></script>
<script type="text/javascript" src="/javascript/check_twitter.js?v=Hc0sH1W9xIu0qhqxWPD4c1u50K4"></script>
<script type="text/javascript" src="/javascript/init_googleAnalytics.js?v=yCD4h9PHGI0dsefmuWDlMn7pHog"></script>

</body>
</html>

