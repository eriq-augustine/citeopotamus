On Scalable and Efficient Distributed Failure Detectors
Indranil Gupta Department of Computer Science Cornell University Ithaca, NY 14853, USA
gupta@cs.cornell.edu
Tushar D. Chandra IBM T.J. Watson Research Center P.O. Box 704 Yorktown Heights, NY, USA
tushar@us.ibm.com
German S. Goldszmidt IBM T.J. Watson Research Center P.O. Box 704 Yorktown Heights, NY, USA
gsg@us.ibm.com
ABSTRACT
Process groups in distributed applications and services rely on failure detectors to detect process failures completely, and as quickly, accurately, and scalably as possible, even in the face of unreliable message deliveries. In this paper, we look at quantifying the optimal scalability, in terms of network load, (in messages per second, with messages having a size limit) of distributed, complete failure detectors as a function of application-specified requirements. These requirements are 1) quick failure detection by some non-faulty process, and 2) accuracy of failure detection. We assume a crash-recovery (non-Byzantine) failure model, and a network model that is probabilistically unreliable (w.r.t. message deliveries and process failures). First, we characterize, under certain independence assumptions, the optimum worst-case network load imposed by any failure detector that achieves an application's requirements. We then discuss why traditional hearth eating schemes are inherently unscalable according to the optimal load. We also present a randomized, distributed, failure detector algorithm that imposes an equal expected load per group member. This protocol satisfies the application defined constraints of completeness and accuracy, and speed of detection on an average. It imposes a network load that differs from the optimal by a sub-optimality factor that is much lower than that for traditional distributed heartbeating schemes. Moreover, this sub-optimality factor does not vary with group size (for large groups).
Keywords
Distributed systems, Failure detectors, Efficiency, Accuracy, Scalability.
1. INTRODUCTION
Failure detectors are a central component in fault-tolerant distributed systems based on process groups running over unreliable, asynchronous networks eg., group membership protocols [3], supercomputers, computer clusters [13], etc.
Permission to make digital or hard copics of all or pan of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. podc 01 Newport Rhode Island USA Copyright ACM 2001 1-58113-383-9 /01/08 ..$5.00
The ability of the failure detector to detect process failures completely and efficiently, in the presence of unreliable messaging as well as arbitrary process crashes and recoveries, can have a major impact on the performance of these systems. "Completeness" is the guarantee that the failure of a group member is eventually detected by every non-faulty group member. "Efficiency" means that failures are detected quickly, as well as accurately (i.e., without too many mistakes).
The first work to address these properties of failure detectors was by Chandra and Toueg [5j. The authors showed why it is impossible for a failure detector algorithm to de-terministically achieve both completeness and accuracy over an asynchronous unreliable network. This result has lead to a flurry of theoretical research on other ways of classifying failure detectors, but more importantly, has served as a guide to designers of failure detector algorithms for real systems. For example, most distributed applications have opted to circumvent the impossibility result by relying on failure detector algorithms that guarantee completeness de-terministically while achieving efficiency only probabilistically [1, 2, 4, 6, 7, 8, 14).
The recent emergence of applications for large scale distributed systems has created a need for failure detector algorithms that minimize the network load (in bytes per second, or equivalently, messages per second with a limit on maximum message size) used, as well as the load imposed on participating processes [7, 14], Failure detectors for such settings thus seek to achieve good scalability in addition to efficiency, while still (deterministically) guaranteeing completeness.
Recently, Chen et at. [6] proposed a comprehensive set of metrics to measure the Quality of Service (QoS) of complete and efficient failure detectors. This paper presented three primary metrics to quantify the performance of a failure detector at one process detecting crash-recovery failures of a single other process over an unreliable network. The authors proposed failure detection time, and recurrence time and duration times of mistaken detection as the primary metrics for complete and efficient failure detectors. However, the paper neither deal with the optimal relation among these metrics, nor focussed on distributed or scalable failure detectors.
In this paper, we first address the question of quantifying the optimum worst-case network load (in messages per sec-
170
ond, with a limit on messages sizes) needed by a complete distributed failure detector protocol to satisfy the efficiency requirements as specified by ike application. We are concerned with distributed failure detectors working in a group of uniquely identifiable processes, which are subject to failures and recoveries, and communicate over an unreliable network. We deal with complete failure detectors that satisfy application-defined efficiency constraints of 1) (quickness) detection of any group member failure by some non-faulty member within a time bound, and 2) (accuracy) probability (within this time bound) of no other non-faulty member detecting a given non-faulty member as having failed.
The first (quickness) requirement merits further discussion. Many systems, such as multi-domain server farm clusters [7, 13] and virtual synchrony implementations [3] rely on a single or a few central computers to aggregate failure detection information from across the system. These computers are then responsible for disseminating that information across the entire system. In such systems, efficient detection of a failure depends on the time the failure is first detected by a non-faulty member. Even in the absence of a central server, notification of a failure is typically communicated, by the first member to detect it, to the entire group via a (possibly unreliable) broadcast [3j. Thus, although achieving completeness is important, efficient detection of a failure is more often related with the time to the first detection, by another non-faulty member, of the failure.
We derive the optimal worst-case network load (in messages per second, with a limit on maximum message size) imposed on the network by a complete failure detector satisfying the above application-defined constraints. We then discuss why the traditional and popular distributed heartbeating failure detection schemes (eg., [7, 14]) do not achieve these optimal scalability limits. Finally, we present a randomized distributed failure detector that can be configured to meet the application-defined constraints of completeness and accuracy, and expected speed of detection. With reasonable assumptions on the network unreliability (member and message failure rates of up to 15%), the worst-case network load imposed by this protocol has a sub-optimality factor that is much lower than that of traditional distributed heartbeat schemes. This sub-optimality factor does not depend on group size (in large groups), but only on the application-specified efficiency constraints and the network unreliability probabilities. Furthermore, the average load imposed per member is independent of the group size.
In arriving at these results, we will assume that message loss and member failures can each be characterized by probabilistic distributions, independent across messages and failures. While the practicality of these assumptions in real networks will probably be subject to criticism, these assumptions are necessary in order to take this first step towards quantifying and achieving scalable and efficient failure detectors. Besides, we believe that these independence assumptions are partially justified because of 1) the randomized nature of the new failure detector algorithm, and 2) the large temporal separation between protocol periods, typically O(seconds) in practice (mitigating much of the correlation among message loss probability distributions).
The rest of the paper is organized as follows. Section 2 briefly summarizes previous work in this area. In Section 3, we formally describe the process group model assumed in this paper. Section 4 presents a discussion of how an application can specify efficiency requirements to a failure detector, and quantifies the optimal worst-case network load a failure detector must impose, in order to meet these requirements. Section 5 presents the new randomized failure detector protocol. We conclude in section 6.
2. PREVIOUS WORK
Chandra and Toueg [5] were the first to formally address the completeness and accuracy properties of failure detectors, Subsequent work has focused on different properties and classifications of failure detectors. This area of literature has treated failure detectors as oracles used to solve the Distributed Consensus/Agreement problem [9], which is un-solvable in the general asynchronous network model. These classifications of failure detectors are primarily based on the weakness of the model required to implement them, in order to solve the Distributed Consensus/Agreement problem [11]-
Proposals for implementable failure detectors have sometimes assumed network models with weak unreliability semantics eg., timed-asynchronous model [8], quasi-synchronous model [2|, partial synchrony model [12], etc. These proposals have treated failure detectors only as a tool to efficiently reach agreement, ignoring their efficiency from an application designer's viewpoint. For example, most failure detectors such as [12] provide eventual guarantees, while applications are typically concerned about real timing constraints.
In most real-life distributed systems, the failure detection service is implemented via variants of the "Heartbeat mechanism" [1, 2, 4, 6, 7, 8, 14], which have been popular as they guarantee the completeness property. However, all existing heartbeat approaches have shortcomings. Centralized heartbeat schemes create hot-spots that prevent them from scaling. Distributed heartbeat schemes offer different levels of accuracy and scalability depending on the exact heartbeat dissemination mechanism used, but we show that they are inherently not as efficient and scalable as claimed.
Probabilistic network models have been used to analyze heartbeat failure detectors in [4, 6], but only with a single process detecting failures of a single other process. [6] was the first paper to propose metrics for non-distributed heartbeat failure detectors in the crash-recovery model. These metrics were not inclusive of scalability concerns.
Our work differs from all this prior work in that it is the first to approach the design of failure detectors from a distributed application developer's viewpoint. We quantify the performance of a failure detector protocol as the network load it requires to impose on the network, in order to satisfy the application-defined constraints of completeness, and quick and accurate detection1. We also present an efficient and scalable distributed failure detector. The new failure detector incurs a constant expected load per process, thus
1We will state these application-defined requirements for-
mally in Section 4.
171
avoiding the hot-spot problem of centralized heartbeating schemes.
3. MODEL
We consider a large group of n (>■ 1) members2. This set of potential group members is fixed a priori. Group members have unique identifiers. Each group member maintains a list, called a view, containing the identities of all other group members (faulty or otherwise). Our protocol specification and analysis assumes that this maximal group membership is always the same at all members, but our results can be extended to a model with dynamically changing membership and members with incomplete views, using methodologies similar to [10|.
Members may suffer crash (non-Byzantine) failures, and recover subsequently. Unlike other papers on failure detectors (eg., [14]) that consider a member as faulty if they are perturbed and sleep for a time greater than some pre-specifled duration, our notion of failure considers that a member is faulty if and only if it has really crashed. Perturbations at members that might lead to message losses are accounted for in the message loss rate pmi (which we will define shortly).
Whenever a member recovers from a failure, it does so into a new incarnation that is distinguishable from all its earlier incarnations. At each member, an integer in non-volatile storage, that is incremented every time the member recovers, suffices to serve as the member's incarnation number. The members in our group model thus have crash-recovery semantics with incarnation numbers distinguishing different failures and recoveries. When a member M, crashes (fails), it does so in its current incarnation (say its i'th incarnation). We say that such a failure is "detected" at exactly the first instant of time that some other non-faulty member detects either 1) failure of Mi in incarnation greater than or equal to /, or 2) recovery of Mi in an incarnation strictly greater
than I.
We characterize the member failure probability by a parameter pf. pf is the probability that a random group member is faulty at a random time. Member crashes are assumed to be independent across members.
We assume no synchronization of clocks across group members. We only require that each individual member's clock drift rate (from some fixed clock rate) remains constant.
Members communicate using unicast (point-to-point) messaging on an asynchronous, fault-prone network. Since we are interested in characterizing the network bandwidth utilized, we will assume that maximal message sizes are a constant, containing at most a few bytes of data (assuming a bound on the size of message identifiers and headers, as is typical in IP packets).
Each message sent out on the network fails to be delivered at its recipient (due to network congestion, buffer overflow at the sender or receiver due to member perturbations, etc.) with probability Pmi € (0,1). The worst-case message prop-
2 All of which are either processes, or servers, or network adaptors etc.
agation delay (from sender to receiver through the network) for any delivered message is assumed to be so small compared to the application-specified detection time (typically 0( several seconds )) that henceforth, for all practical purposes, we can assume that each message is either delivered immediately at the recipient with probability (1 — pm;), or never reaches the recipient.3
This message loss distribution is also assumed to be independent across messages. Message delivery losses could, in fact, be correlated in such a network. However, if application-specified failure detection times are much larger than message propagation and congestion repair times in the network, messages exchanged by the failure detector will have considerable temporal separation. This reduces the correlation among the loss distributions of different messages. Randomized selection of message destinations in the new failure detector also weakens such message loss correlation.
In the rest of the paper, we use the shorthands qj and qmt
instead of (1 — pf) and (1 — p-m.i) respectively,
4. SCALABLE AND EFFICIENT FAILURE DETECTORS
The first formal characterization of the properties of failure detectors was offered in [5|, which laid down the following properties for distributed failure detectors in process groups:
•	{Strong/Weak} Completeness: crash-failure of any group member is detected by {all/some} non-faulty members4,
•	Strong Accuracy: no non-faulty group member 5 is
declared as failed by any other non-faulty group member.
[5] also showed that a perfect failure detector i.e., one which satisfies both Strong Completeness and Strong Accuracy, is sufficient to solve distributed Consensus, but is impossible to implement in a fault-prone network.
Subsequent work on designing efficient failure detectors has attempted to trade off the Completeness and Accuracy properties in several ways. However, the completeness properties required by most distributed applications have lead to the popular use of failure detectors that guarantee Strong Completeness always, even if eventually [1, 2, 4, 5, 6, 7, 8, 14]. This of course means that such failure detectors cannot guarantee Strong Accuracy always, but only with a probability less than 1. For example, all-to-all (distributed)
sThis assumption is made for simplicity. In fact, the opti-mality results of section 4 hold if pmi is assumed to be the probability of message delivery within T time units after its send. The randomized protocol of section 5 and its analysis can be extended to hold if pmi is the probability of message delivery within a sixth of the protocol period.
4Rccollcct that in our model, since members recover with unique incarnations, detection of a member's failure or recovery also implies detection of failure of all it's previous incarnations.
5in its current incarnation
172
heartbeating schemes have been popular because they guarantee Strong Completeness (since a faulty member will stop sending heartbeats), while providing varying degrees of accuracy.
We have explained in Section 1 why in many distributed applications, although the failure of a group member must eventually be known to all non-faulty members, it is important to have the failure detected quickly by some non-faulty member (and not necessarily all non-faulty members). In other words, the quickness of failure detectors depends on the time from a member failure to Weak Completeness with respect to that failure, although Strong Completeness is a necessary property.
The requirements imposed by an application (or its designer) on a failure detector protocol can thus be formally specified and parameterized as follows:
1.	Completeness: satisfy eventual Strong Completeness for member failures.
2.	Efficiency:
(a)	Speed: every member failure is detected by some non-faulty group member within T time units after its occurrence (7~ 3> worst-case message round trip time).
(b)	Accuracy: at any time instant, for every non-faulty member Mi not yet detected as failed, the probability that no other non-faulty group member will (mistakenly) detect Mi as faulty within the next T time units is at least (1 — VM{T)}.
T and VM(T) are thus parameters specified by the application (or its designer). For example, an application designer might specify T = 3 seconds, and VM(3 seconds) — 10_a.
To measure the scalability of a failure detector algorithm, we use the worst-case network load it imposes - this is denoted as L. Since several messages may be transmitted simultaneously even from one group member, we define:
Definition 1. The worst-case network load L of a failure detector protocol is the maximum number of messages transmitted by any run of the protocol within any time interval of length T, divided by T.
We also require that the failure detector impose a uniform expected send and receive load at each member due to this traffic.
The goal of a near-optimal failure detector algorithm is thus to satisfy the above requirements (Completeness, Efficiency) while guaranteeing:
• Scale: the worst-case network load L imposed by the algorithm is close to the optimal possible, with equal expected load per member.
That brings us to the question - what is the optimal worst-case network load, call it L*, that is needed to satisfy the above application-defined requirements - Completeness, Speed (T), Accuracy (VM(T)) ? We are able to answer this question in the network model discussed earlier when the group size n is very large (3> 1), and 'PM('T) is very small ( < pmi).
theorem 1. Any distributed failure detector algorithm for a group of size n 1) thai determimstically satisfies the Completeness, Speed, Accuracy requirements above, for given values of T andVM(T) pmt). imposes a minimal worst-case network load (messages per time unit, as defined above) of:
log(VM(T)) log(pmi) ■ T
Furthermore, there is a failure detector that achieves this minimal worst-case bound while satisfying the Completeness, Speed, Accuracy requirements.
L* is thus the optimal worst-case network load required to satisfy the Completeness, Speed, Accuracy requirements.
proof. We prove the first part of the theorem by showing that each non-faulty group member could transmit up to MW1) messages in a time interval of length 7*.
Consider a group member Mi at a random point in time t. Let Mi not be detected as failed yet by any other group member, and stay non-faulty until at least time t + T. Let to be the maximum number of messages sent by Mi, in the time interval [t, t + T\, in any possible run of the failure detector protocol starting from time t.
Now, at time t, the event that "all messages sent by M, in the time interval [t,t+T] are lost" happens with probability at least p^Jj. Occurrence of this event entails that it is indistinguishable to the set of the rest of the non-faulty group members (i.e., members other than Mi) as to whether Mi is faulty or not. By the speed requirement, this event would then imply that Mf is detected as failed by some non-faulty group member between t and t + T.
Thus, the probability that at time t, a given non-faulty member Mi that is not yet detected as faulty, is detected as failed by some other non-faulty group member within the next T time units, is at least p^i- By the Accuracy requirement, we have p™t < PM(T), which implies that m >	•
A failure detector that satisfies the completeness, speed, Accuracy requirements and meets the L* bound works as follows. It uses a highly available, non-faulty server as a group leader6. Every other group member sends ['"fp^^^^l "I am alive" messages to this server every T time units. The
8The Bet of central computers, that collect failure information and disseminate it to the system, can be designated as the server.
173
server declares a member as failed when it does not receive any "I am alive" message from it for T time units7. □
Corollary: The optimal bound of Theorem 1 applies to the crash-stop model as well.
Proof: By exactly the same arguments as in the proof of Theorem 1.	□
Definition S. The sub-optimality factor of a failure detector algorithm that imposes a worst-case network load L, while satisfying the Completeness and Efficiency requirements, is defined as jr.
In the traditional distributed Heartbeating failure detection algorithms, every group member periodically transmits a "heartbeat" message (with an incremented countcr) to every other group member. A member M, is declared as failed by a non-faulty member M, when Mj does not receive heartbeats from Mi for some consecutive heartbeat periods (this duration being the detection time T).
Distributed heartbeating schemes have been the most popular implementation of failure detectors because they guarantee Completeness - a failed member will not send any more heartbeat messages. However, the accuracy and scalability guarantees of heartbeating algorithms differ, depending entirely on the actual mechanism used to disseminate heartbeats.
In the simplest implementation, each member Mi transmits a few "I am alive" messages to each group member it knows of, every T time units. The worst-case number of messages transmitted by each member per unit time is 0(n), and the worst-case total network load L is 0{n!). The sub-optimality factor (i.e., j^) varies as 8{n), for any values of pmj, p/ and VM(T).
The Gossip-style failure detection service, proposed by van Renesse et al. [14], uses a mechanism where every tnn„„v time units, each member gossips a 9(n) list of the latest heartbeat counters (for all group members) to a few other randomly selected group members. The authors show that under this scheme, a new heartbeat count typically takes an average time of 0{log(n) • tj^sip] to reach an arbitrary other group member. The Speed requirement thus leads us to choose tgosaip —	The worst-case network
load imposed by the Gossip-style heartbeat scheme is thus 0[t ] ~	The sub-optimality factor varies as
0[n ■ log(n)\, for any values of pmi, Pf and PM(T).
In fact, distributed heartbeating schemes do not meet the optimality bound of Theorem 1 because they inherently attempt to communicate a failure notification to all group members. As we have seen above, this is an overkill for systems that can rely on a centralized coordinated set of
7This implementation, which is essentially a centralized heartbeat mechanism, is undesirable as it requires a highly available server and has bad load balancing (does not satisfy the Scale property).
servers to disseminate failure information. These systems require only some other non-faulty member to detect a given failure.
Other heartbeating schemes, such as Centralized heartbeating (as discussed in the proof of Theorem 1) and heartbeating along a logical ring of group members [7], can be configured to meet the optimal load L*, but have problems such as creating hot-spots (centralized heartbeating) or unpredictable failure detection times in the presence of multiple simultaneous faults at larger group sizes (heartbeating in a ring).
5. A RANDOMIZED DISTRIBUTED FAILURE DETECTOR PROTOCOL
In the preceding sections, we have characterized the optimal worst-case load imposed by a distributed failure detector that satisfies the completeness, Speed and ACCURACY requirements, for application specified values of T and VM(T) (Theorem 1). We have then studied why traditional heartbeating schemes are inherently not scalable.
In this section, we relax the speed condition to detect a failure within an expected (rather than exact, as before) time bound of T time units after the failure. We then present a randomized distributed failure detector algorithm that guarantees Completeness with probability 1, detection of any member failure within an expected time T from the failure, and an Accuracy probability of (1 — VM(T)). The protocol imposes an equal expected load per group member, and a worst-case (and average case) network load L that differs from the optimal L* of Theorem 1 by a sub-optimality factor (i.e., jfv) that is independent of group size n (!S> 1). In such large groups, at reasonable values of member and message delivery failure rates pj and pmi, this sub-optimality factor is much lower than the sub-optimality factors of the traditional distributed heartbeating schemes discussed in the previous section.
5.1 New Failure Detector Algorithm
The failure detector algorithm uses two parameters: protocol period T' (in time units) and integer fc, which is the size of failure detection subgroups. We will show how the values of these parameters can be configured from the required values of T and VMiT), and the network parameters p/,pmi. Parameters T' and k are assumed to be known a priori at all group members. Note that this does not need clocks to be synchronized across members, but only requires each member to have a steady clock rate to be able to measure T'.
The algorithm is formally described in Figure 1. At each non-faulty member Mi, steps (1-3) are executed once every T' time units (which we call a protocol period), while steps (4,5,6) are executed whenever necessary. The data contained in each message is shown in parentheses after the message. If sequence numbers are allowed to wrap around, the maximal message size is bounded from above.
Figure 2 illustrates the protocol steps initiated by a member Mi, during one protocol period of length T' time units. At the start of this protocol period at Mi, a random member
174
Integer pr; /* Local period number */ Every T' time units at Mi:
0.	pr pr + 1
1.	Select random member Mj from view Send a ping(Mi, Mj,pr) message to Afj
Wait for the worst-case message round-trip time for an ack(Mi, Af,, pr) message
2.	If have not received &n ack(Afi, Mj,pr) message yet
Select k members randomly from view Send each of them a pmg-req(M4, Mj, pr) message Wait for an ack(Afj, Af^, pr) message until the end of period pr
3.	If have not received an ack (Aft, Mj , pr) message yet
Declare M?- as failed
Anytime at Mi:
4 On receipt of a ping-req(Afm, Mj, pr) (Mj ^ Mt) Send a ping(Mi, Mj, Mmt pr) message to Mj On receipt of an ack(Mt, Mj, Mmi pr) message from Mj Send an ack(Mm, Mj,pr) message to received to Mm
Anytime at Mi :
5.	On receipt of a ping(Mm, Mj.Mj.pr) message from member Mm
Reply with an ack(Mm, Mi, Mi, pr) message to Mm Anytime at Mr-
6.	On receipt of a pingM,,pr) message from member M„,
Reply with an ack(Mm, Mi, pr) message to Mm
Figure 1: Protocol steps at a group member Mi. Data in each message is shown in parentheses after the message. Each message also contains the current incarnation number of the sender.
is selected, in this case Mj, and a ping message sent to it. If Mi does not receive a replying ack from Mj within some time-out (determined by the message round-trip time, which is T), it selects k members at random and sends to each a ping-req message. Each of the non-faulty members among these k which receives the ping-req message subsequently pings Mj and forwards the ack received from Mj, if any, back to Mi. In the example of Figure 2, one of the k members manages to complete this cycle of events as Mj is up, and Mj does not suspect Mj as faulty at the end of this protocol period.
In the above protocol, member Mi uses a randomly selected subgroup of k members to out-source ping-req messages, rather than sending out k repeat ping messages to the target Mj. The effect of using the randomly selected subgroup is to distribute the decision on failure detection across a subgroup of (fc + 1) members. Although we do not analyze it in this paper, it can be shown that the new protocol's properties are preserved even in the presence of some degree of variation of message delivery loss probabilities across group members. Sending k repeat ping messages may not satisfy this property. Our analysis in Section 5.2 shows that the cost (in terms of sub-optimality factor of network load) of using a (k + l)-sized subgroup is not too significant.
5.2 Analysis
In this section, we calculate, for the above protocol, the expected detection time of a member failure, as well as the probability of an inaccurate detection of a non-faulty
TIME
choose random M
choose k random members
Figure 2: Example protocol period at Mi. This shows all the possible messages that a protocol period may initiate. Some message contents excluded for simplicity.
member by some other (at least one) non-faulty member. This will lead to calculation of the values of T' and k, for the above protocol, as a function of parameters specifying application-specified requirements and network unreliability, i.e., T, PM(T),pf, pml.
For any group member Mj, faulty or otherwise,
Pr [at Least one non-faulty member chooses to ping Mj (directly) in a time interval 7'']
=
~ 1 — e Vf (since n>l)
Thus, the expected time between a failure of member Mj and its detection by some non-faulty member is
1	eqf
E[T] - T'
1 - e~"f
r ■
e"f - 1
(1)
This gives us a configurable value for J* as a function of
T,pf.
Now, denote
C(j>f) =

e"s - 1
At any given time instant, a non-faulty member Mj will be detected as faulty by another non-faulty member Mi within the next T time units if Mi chooses to ping Mj within the next T time units and does not receive any acks, directly or indirectly from transitive ping-req's, from Mj. Then, VM(T), the probability of inaccurate failure detection of member Mj within the next T time units, is simply the probability that there is at least one such member Mi in the group.
A random group member Mi is non-faulty with probability
175
qf, and the probability of such a member choosing to ping Mj within a time interval T is £ ■ C(p/). Given this, the probability that such a Mi receives back no acks, direct or indirect, according to the protocol of section 5.1 equals
((i-i)'(i-rtf)
Therefore,
VM(T) = 1 - [1 - & C(pf) ■ (1 -<&,) ■ (1 -qf-n
(since tl 1) (since VM(T) « 1)
4 \Am—1
This gives us
log\
VM(T)

log{ 1 - qf ■ q^)
(2)
Thus, the new randomized failure detector protocol can be configured using equations (1) and (2) to satisfy the Speed and Accuracy requirements with parameters e[T], VM(T). Moreover, given a member Mj that has failed (and stays failed), every other non-faulty member Mi will eventually choose to ping Mj in some protocol period, and discover Mj as having failed. Hence,
Then, from Theorem 1 and equations (1),(2),
JL
L*
log[
'PM(T)
[2+4.


"f }J
log( 1 - qf q^t) f''!	log(p„Li)
le«/-l log(VM(T))>
(3)
L thus differs from the optimal L* by a factor that is independent of the group size n. Furthermore, (3) can be written as a linear function of _j03(yM(7J)) as:
S(p/'pml) + -log(VM{T)) '
where g(j>j,pmt) is: [4-
log{pmi)
.If
log(l-qf-q^t)
and f(j>f,Pmi) is:
[{2-4
log(gf • (1 - <?L) ■ tttzt) log( 1 - q; - q*nL)
} x (~log(pm,))
(4a)
(4b)
eqf - l1 (4c)
Theorem 3. The sub-optimality factor jy of the protocol of Figure 1. is independent of group size n \). Further-
THEOREM 2. This randomized failure detector protocol:
(a)	satisfies eventual Strong Completeness, i.e.. the completeness requirement,
(b)	can be configured via equations (1) and (2) to meet the requirements of (expected) Speed, and Accuracy, and
(c)	has a uniform, expected send/receive load at all group members.
Proof. From the above discussion and equations (1), (2)- □
1- if /(PfiPml) < 0;
(a)	jy is monotonically increasing with —logi'PM(T)), and
(b)	As VM(T) - 0+, £ a(p/ipmi)-
2. if f(pf,pml) > 0,
(a)	in monotonically decreasing with —IngCP M("7")) . and
(b)	As VMiT) o+ £ - g(Pf>Pm,)+
Proof. From equations (4a) through (4c). □
Finally, we upper-bound the worst-case and expected network load (L, E[i] respectively) imposed by this failure detector protocol.
The worst-case network load occurs when, every T' time units, each member initiates steps (1-6) in the algorithm of Figure 1. Steps (1,6) involve at most 2 messages, while steps (2-5) involve at most 4 messages per ping-req target member. Therefore, the worst-case network load imposed by this protocol (in messages/time unit) is
L = n-[2 + 4-fc]-^
We next calculate the average network load imposed by the new failure detector algorithm. Every T" time units, each non-faulty member (numbering (n • qf) on an average) executes steps (1-3), in the algorithm of Figure 1. Steps (1,6) involve at most 2 messages, while steps (2-5) (which are executed only if no ack is received from the target of the ping of step (1) - this happens with probability (1 — qj ■ q'frLi)) involve at most 4 messages per non-faulty ping-req target member. Therefore, the average network load imposed by this protocol (in messages/time unit) is
176
Then, from Theorem 1 and equations (1),(2),
iog{-sum
m
L-
eq' log(pm{) ,

e*t - 1 Zo5(PM(T)) j
(5)
Even E[jL] can be upper-bounded from the optimal L* by a factor that is independent of the group size n.
Do the values of and	go very high compared to
the ideal value of 1.0 ? The answer is a 'No' when values of pf, Pmi are low, yet reasonable. Figure 3(a) shows the variation of as in equation (3), at low but reasonable values of pf,pmi, and VM(T). This plot shows that the sub-optimality factor of the network load imposed by the new failure detector rises as pmi and pf increase, or VM{T) decreases, but is bounded above by the function g[pf, pmi), at all values of "PM{T). This happens because f(pj,pml) < 0 at such low values of p/ and pmi, as seen from Figure 3(b) - Theorem 3.1 thus applies here. From figure 3(a), the function g(pf,pmi) (bottom-most surface), does not attain too high values (staying below 26 for the values shown). Thus the performance of the new failure detector algorithm is good for reasonable assumptions on the network unreliability.
Figure 3(c) shows that the upper bound on ^^ stays very low (below 8) for values of Pf and pmi up to 15%. Moreover, as VM(T) is decreased, the bound on ^ actually decreases. This curve reveals the advantage of using randomization in the failure detector. Unlike traditional distributed heartbeating algorithms, the average case network load behavior of the new protocol is much lower than the worst-case network load behavior.
Figure 3 reveals that for values of p/ and pmi below 15%, the for the new randomized failure detector stays below 26, and stays below 8. Further, as is evident from equations (3) and (5), the variation of these sub-optimality factors does not depend on the group size (at large group sizes). Compare this with the sub-optimality factors of distributed heartbeating schemes discussed in Section 4, which are typically at least 0[rij.
In reality, message loss rates and process failure rates could vary from time to time. The parameters p/ and pmi, needed to configure protocol parameters T' and k. may be difficult to estimate. However, Figure 3 shows that assuming reasonable bounds on these message loss rates/failure rates and using these bounds to configure the failure detector suffices. In other words, configuring protocol parameters with Pf.Pmi — 15% will ensure that the failure detector preserves the application specified constraints (T, 'PM(T)) while imposing a network load that differs from the optimal worst-case load L" by a factor of at most 26 in the worst-case, and 8 in the average case, as long as the message loss/process failure rates do not exceed 15% (this load is lower when loss or failure rates are lower).
(L/L*)
ln(Pm(T))—10 tn(Pm(T)>—-30 g(pf.pml)
0.03

(a) Variation of jj (according to equation (3)) versus Pmi,Pf, at different values of 'PM(T). For low values of p„a and p/, g(p/,pml) is an upper bound on j--.
0-8
pmi
0.4 pf 0.6
(b) Values olpf,pmi for which f(pf,pmt) is positive or negative.
in(Pm(T))—io
r)J—3o
ln(Pm(T):
(E[L]/L-> <
0.03 O.
of u09~ TV--/ 0 03
(c) Variation of versus Pmi,Pf (according to equation (5)).
Figure 3: Performance of new failure detector algorithm
177
5.3 Future Work and Optimizations
At Cornell University, we are currently testing performance of a scalable distributed membership service that uses the new randomized failure detection algorithm.
Extending the above protocol to the crash-stop model inherent to dynamic groups involves several protocol extensions. Every group member join, leave or failure detection entails a broadcast to the non-faulty group members in order to update their view. Further, this broadcast may not be reliable.
Implementing this protocol over a group spanning several subnets requires that the load on the connecting routers or gateways be low. The protocol currently imposes an 0(n) load (in bytes per second) on such routers during every protocol period. Reducing this load inevitably leads to compromising some of the Efficiency properties of the protocol, as pings are sent less frequently across subnets.
The protocol can also be optimized to trade off worse Scale properties for better Accuracy properties. One such optimization is to follow a failure detection (by an individual non-faulty member through the described protocol) by multicast of a suspicion of that failure, waiting for some time before turning this suspicion into a declaration of a member failure. With such a suspicion multicast in place, protocol periods at different non-faulty group members, targeting this suspected member, can be correlated to improve the Accuracy properties. This would also reduce the effect of correlated message failures on the frequency of mistaken failure declarations.
A disadvantage of the protocol is that since messages are restricted to contain at most a few bytes of data, large message headers mean higher overheads per message. The protocol also precludes optimizations involving piggy-backed messages, primarily due to the random selection of ping targets.
The discussion in this paper also points us to several new and interesting questions.
Is it possible to design a failure detector algorithm that, for an asynchronous network setting, satisfies Completeness, Efficiency, Scale requirements, and the Speed requirement (section 4) with a deterministic bound on time to detection of a failure (7~), rather than as an average case as we have done in this paper ?8 Notice that this is not difficult to achieve in a synchronous network setting (by modifying the new failure detector algorithm to choose ping targets in a deterministic and globally known manner during every protocol period).
We also leave as an open problem the specification and realization of optimality load conditions for a failure detector with the Speed timing parameter T set as the time to achieve Strong Completeness for any group member failure (rather than just Weak Completeness).
8 Heartbeating along a logical ring among group members (eg., [7]) seems to provide a solution to this question. However, as pointed out before, ring heartbeating has unpredictable failure detection times in the presence of multiple simultaneous failures.
Of course, it would be ideal to extend all such results to models that assume some degree of correlation among message losses, and perhaps even member failures.
6.	CONCLUDING COMMENTS
In this paper, we have looked at designing complete, scalable, distributed failure detectors from timing and accuracy parameters specified by the distributed application. We have restricted ourselves to a simple, probabilistically lossy, network model. Under certain independence assumptions, we have first quantified the optimal worst-case network load (messages per second, with a limit on maximal message size) required by a complete failure detector algorithm in a process group over such a network, derived from application-specified constraints of 1) detection time of a group member failure by some non-faulty group member, and 2) probability (within the detection time period) of no other non-faulty member detecting a given non-faulty member as having failed. We have then shown why the popular distributed heartbeating failure detection schemes inherently do not satisfy this optimal scalability limit.
Finally, we have proposed a randomized failure detector algorithm that imposes an equal expected load on all group members. This failure detector can be configured to satisfy the application-specified requirements of completeness and accuracy, and speed of failure detection (on average). Our analysis of the protocol shows that it imposes a worst-case network load that differs from the optimal by a sub-optimality factor greater than 1. For very stringent accuracy requirements (VM(T) as low as e-30), reasonable message loss probabilities and process failure rates in the network (up to 15% each), the sub-optimality factor is not as large as that of traditional distributed heartbeating protocols. Further, this sub-optimality factor does not vary with group size, when groups are large.
We are currently involved in implementing and testing the behavior of this protocol in dynamic group membership scenarios. This involves several extensions and optimizations to the described protocol.
Acknowledgments
We thank all the members of the Oceano group for their feedback. We are also immensely grateful to the anonymous reviewers and Michael Kalantar for their suggestions towards improving the quality of the paper.
7.	REFERENCES
[1]	M. K. Aguilera, W. Chen, and S. Toueg. Heartbeat: a timeout-free failure detector for quiescent reliable communication. In Proceedings of 11th International Workshop on Distributed Algorithms (WDAG'97), pages 126-140, September 1997.
[2]	C. Almeida and P. Verissimo. Timing failure detection and real-time group communication in real-time systems. In Proceedings of 8th Euromicro Workshop on Real-Time Systems, June 1996.
[3| K. P. Birman, The process group approach to reliable distributed computing. Communications of the A CM, 36(12):37-53, December 1993.
178
14| R. Bollo, J.-R L. Narzul, M. Raynal, and F. Tronel. Probabilistic analysis of a group failure detection protocol. In Proceedings of 4th International Workshop on Object-Oriented Real-Time Dependable Systems, 1998.
[5]	T. D. Chandra and S. Toueg. Unreliable failure detectors for reliable distributed systems. Journal of the ACM, 43(2):225-267, March 1996.
[6]	W. Chen, S. Toueg, and M. K. Aguilera. On the quality of service of failure detectors. In Proceedings of 30th International Conference on Dependable Systems and Networks (ICDSN/FTCS-30), June 2000.
[7]	S. A. Fakhouri, G. S. Goldszmidt, 1. Gupta,
M. Kalantar, and J. A. Pershing. Gulfstream - a system for dynamic topology management in multi-domain server farms. Technical Report RC 21954, IBM T.J. Watson Research Center, February 2001.
[8]	C. Fetzer and F. Cristian. Fail-awareness in timed asynchronous systems. In Proceedings of 15th Annual A CM Symposium on Principles of Distributed Computing (PODC'96), pages 314-321a, May 1996.
[9]	M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of distributed Consensus with one faulty process. Journal of the ACM, 32(2):374-382, April 1985.
[10]	I. Gupta, R. van Renesse, and K. P. Birman. A probabilistically correct leader election protocol for large groups. In Proceedings of 14th International Symposium on Distributed Computing (DISC 2000), LNCS-1914, pages 89-103, October 2000.
[11]	J. M. Helary and M. Hurfin. Solving Agreement problems with failure detectors; a survey. Annals of Telecommunications, 52(9-l0):447-464, September-October 1997.
[12]	M. Larrea, A. Fernandez, and S. Arevalo. Optimal implementation of the weakest failure detector for salving Consensus. In Proceedings of 19th Annual ACM-SIGOPS Symposium on Principles of Distributed Computing (PODC 2000), July 2000.
[13]	G. Pfister. In search of Clusters, the Ongoing Battle in Lowly Parallel Computing. Prentice Hall, 1998.
[14]	R. van Renesse, Y. Minsky, and M. Hayden. A gossip-style failure detection service. In Proceedings of International Conference and Distributed SysteTns Platforms and Open Distributed Processing (IFIP), 1998.
179