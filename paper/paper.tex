% Paper for CSC580 final project.
\documentclass[10pt, conference, compsocconf]{IEEEtran}

\usepackage{float}
%\usepackage{url}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{color}

\begin{document}

\title{Citeopotamus}

% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\author{\IEEEauthorblockN{Eriq Augustine, Aldrin Montana, Ryan Verdon}
\\
\IEEEauthorblockA{Department of Computer Science\\
Cal Poly, San Luis Obispo\\
 \textsf{\{eaugusti, amontana, rverdon\}@calpoly.edu}
}
}

\maketitle

\thispagestyle{empty}
\pagestyle{empty}

\section{Introduction}\label{sec:introduction}
The problem of recommending citations for a technical paper is known and has
many approaches to attempt to solve it~\cite{cite1, cite2, cite3, cite4, cite5,
cite6, cite7, cite8}. Solutions range from graph based social network
approaches to language models to match meaning in both the paper and the
recommended text. But the problem of recommending where inside the paper to put
citations is unknown.  The ideal solution would recommend an exact spot in the
paper to put a specific citation. To solve this problem there exist two
separate pieces. First, given a location of a citation the solution must be
able to examine the references for the paper and choose the most appropriate
citations.  Second, find locations where citations should go. We leave the
second problem to be done in the future while we focus on solving the first in
this paper.

The rest of the paper is laid out as follows. Section \ref{sec:contributions}
is about what we are contributing. The following section describes out testing
dataset. We then talk about our architecture we created to solve the problem.
Then we go into how we parse papers and discuss the methods for choosing the
best citation. Then we go into future work, related work, and lastly our
conclusion.

\section{Contributions}\label{sec:contributions}
Our contributions consist of a dataset to test new solutions and methods to
recommend citations given a location for a citation.

\section{Dataset}\label{sec:dataset}
%overview
The dataset for which we report results was collected from \textit{PLoS
Computational Biology}~\cite{plos}. PLoS (Public Library of Science) is a
nonprofit publisher that aims to accelerate progress in science. PLoS has
several journals--ONE, Biology, Medicine, Genetics, Computational Biology,
Pathogens, and Neglected Tropical Diseases--available to the public. For our
purposes we collected articles from PLoS Computational Biology, the
computational biology journal published by PLoS. We only collected articles
from this journal simply for the reason that as computer scientists it is the
most likely to be similar to computer science publications in style and in
content. We initially were interested in a corpus of computer science
publications, however we were unable to find a source that would allow us to
programmatically collect computer science publications and articles.

%different components of the data
From PLoS Computational Biology we collected a set of \textit{root papers}, the
papers whose citations we attempt to correctly predict. For each root paper we
attempted to retrieve all \textit{reference papers}, or papers listed in the
reference section of the root paper. For each reference paper, we retrieve
metadata of the paper--title and authors--and the abstract text. We prefer to
get the full text of a reference paper, but unfortunately we were unable to
retrieve these, and so we settled for paper abstracts. One of our main
objectives for the data, which we mostly satisfied, was to collect what we
refer to as \textit{complete papers}. A complete paper is a paper for which we
could retrieve metadata and abstracts for all reference papers.

In order to simplify our implementation, we maintain a particular structure of
the articles collected from PLoS. This structure dictates how the articles are
stored on disk (files with a particular format) and what data must be
possessed. This structure is further described in sections
\ref{sec:text_structure} and \ref{sec:dir_structure} below.

\section{Architecture}\label{sec:architecture}
The Citeopotamus system is broken into two independent parts, the scrapper and citer.
The general architecture can be seen in Figure \ref{fig:arch}.

\begin{figure*}[ht]
   \includegraphics[width=\textwidth px]{images/arch.eps}
        \caption{The Citeopotamus architecture overlaid on a hippopotamus.}
        \label{fig:arch}
\end{figure*}

\subsection{Scraper}
TODO(aldrin)

\subsubsection{Structure of Paper Text}
%TODO(aldrin): Expand here if necessary
If the paper text was generated from OCR, then the structure of the text was left up to the OCR engine.
However, papers scraped from PLOS were in HTML and extra formatting can be infered.

All text inside top-level $p$ tags can be infered to be an entire paragraph and was placed on a single line.
The references section was seperated from the rest of the paper by a single line with only ``REFERENCES'' on it.

\subsection{Structured Paper Directory Format}
The output of the scraper is a directory that contains information about the root paper and all the reference that were collected.
To make all the information easily accessible to anyone without imposing any arbitrary restrictions, the structure of the
data is enforced by directory structure.

The root directory is named using the root paper's title. Inside the root directory is a ``references'' directory.
Inside the ``references'' directory are sub-directories named with the number of that citation. A missing directory
means that the scraper could not collect that reference's information.

Every directory that represents a paper will have at most four files:
\begin{enumerate}
   \item meta.txt -- Contains the title, authors, tags, categories, and abstract. This is the only file required for every paper directory.
   \item paper.pdf -- The pdf version of the paper.
   \item paper.txt -- The text version of the paper. This file is required for the root paper.
   \item refs.txt -- Only the references that appear at the bottom of the paper. This information is duplicated in paper.txt, but placed here for convenience.
\end{enumerate}

A sample directory structure can be seen in Figure \ref{fig:tree}.

\begin{figure}[ht]
   \includegraphics[width=\columnwidth px]{images/tree.eps}
        \caption{The structure of a paper directory.}
        \label{fig:tree}
\end{figure}

\subsection{Citer}\label{sec:archCiter}
The citer begins by attempting to parse a directory structure into a structured collection of papers.
The citer then pre-processed each paper and generates a set of contexts (see Section \ref{sec:context}) for each citation in each
root paper.

After all of the contexts are generated, those contexts are fed into the Method Pipeline. In this pipeline are all the methods (see Section \ref{sec:methods})
that operate on the contexts to try and generate citations. Each citation in put through each method one at a time. If any method emits a value, then the pipeline
it short circuited and the emitted value is used as the citation. Because of this short circuiting behavior, the methods in the pipeline are ordered by approximate
precision. That is, all of the methods that rarely miss (regardless of how many citations they correctly place) are moved to the front of the pipeline.
As a citation travels through the pipeline, its citation becomes more and more uncertain.

\section{Parsing}\label{sec:parsing}
Because of its recursive nature, parsing the directory sreucture itself is rather simple.
Parsing the text of a paper however, can be quite complex.

\subsection{OCR}
Before any parsing can be done a paper, it must first be converted to a plain text (UTF-8) format.
Since PLOS provides most of their papers in an HTML format, this only had to be done offline for a few
papers. After looking at many alternatives, we chose to use ABBYY Finereader 10\cite{abbyy}. This
is a comercial OCR product, and requires a lisence to use.

ABBYY Finereader clearly outperformed all of the other OCR products we tried. It is especially
good at maintaining and recognizing formatting. Many other OCR products failed to correctly parse
two-column formats and thus generated unusable output.

\subsection{Transformations}
Before parsing the text of the paper, we had to make some global transformation on the paper.
Most of these transformations were done with simple regular expressions.

\subsubsection{Multiple Citations}
It is not unheard of for authors to make multiple citations within the same brackets: $$[1, 2]$$
These have to be parsed out and replaced with individual citations next to eachother: $$[1][2]$$

\subsubsection{Ranges}
It is very common for authors to cite entire ranges at a time: $$[1] - [3]$$
This is especially seen when an author is referencing background material. These ranges must be expanded out to
the fill listof citations: $$[1][2][3]$$

\subsubsection{False Positive Sentence Enders}\label{sec:fpEnders}
Being able to split up paragraphs into sentences is very important to the Citeopotamus system.
We first tried to use NLTK's \textit{PunktSentenceTokenizer}. However, this did not seem to work well.
It seemed to have a problem with some of the symbols used in acedemic papers, such as the square brackets around
citations. To combat the punctuation tokenizing problem, we decided to only spolit sentnce on periods, question marks, and
exclamations points. Additionally, we replaced common false positive sentence enters i.e.
$$i.e. => \_ie\_$$
$$dr. => \_dr\_$$
$$0.05 => \_0\_05\_$$

Using this tactic, there is a chance that a real sentence ender is replaced. However, this will still
include the original sentence in the result and the original sentence will still be located closer to the
citation that the incorrectly included sentence. Missing a significant portion of a sentence would be a much more
grevious error because then the context required to identify the citation may be missing.

\subsection{Contexts}\label{sec:context}
Contexts are the core of the Citeopotamus system. A ``context'' is just the words surrounding the citation.
Deciding which words to include in a context is very important. We use three different types of contexts that are meant for different
levels of precision. The contexts that include more words tends to be less precise and will therefore appear later in the pipeline.
However, these larger contexts have access to more information than the smaller ones and can sometimes makes guesses that are impossible
(outside of random) to the smaller contexts.

\subsubsection{Paragraph}
The largest of the contexts. We do not consider anything larger because it is very rare for an author to include information about a citation
outside of the paragrapgh that the citation is in (not including parsing or OCR formatting errors). Paragraphs are easy to parse because the
scraper puts one paragraph per line.

\subsubsection{Sentence}
Sentence level contexts contain just the sentence that the citation was main in. As discussed in Section \ref{sec:fpEnders},
sentence context is parsed just by splitting the paragraph on sentence ending punctuation after false positives have been replaced.

\subsubsection{Clause}
Clause context is the smallest and most complex context. The purpose of this context is to capture the clause that the citation was made in.
There are two parts to parsing this context: looking for parenthesis and walking backwards in the sentence.

We will use the following actual sentence as a running example:
\begin{quote}
In previous work, the Guyton models were modularized and re-implemented in Fortran, C++ (M2SL \textbf{[]}), and Simulink \textbf{[]}.
\end{quote}

When trying to find the context of the first citaion, we recognize that it is surrounded by parenthesis.
We take the parenthesis as a hint the that content inside the parenthesis is more related to the citation than the surrounding words.

If there are no parethesis surrounding the citaion (as with the second citation), then we begin walking backwards in the sentence from the citation.
We will continue walking backwards in the sentence until we find:
\begin{enumerate}
   \item The beginning of the sentence.
   \item Another citation.
   \item A character that may break a clause: '[', ']', '.', ',', ';', ':', '!', '"', '?', or '-'.
\end{enumerate}

We do not include the words that appear after the citation because we want to avoid a clause context expanding into a sentence context and
in English, information about a reference is usually presented before the citation.

Our example would generate two clause contexts:
\begin{quote}
M2SL
\end{quote}

\begin{quote}
and Simulink
\end{quote}

Note that both cases generate contexts that contain the name of the product featured in the paper and no erronous inforamtion. Whereas the
sentence context not only contains both produces, but a slew of other seemingly important words: ``Guyton'', ``Fortran'', and ``C++''.

\subsection{References}

\section{Methods}\label{sec:methods}

\subsection{History}

\section{Evaluation}

\subsection{Adjacent Citations}
%[][]; [], []

\section{Results}\label{sec:results}

\section{Future Work}\label{sec:future}
TODO(eriq): THis section is bad.
We believe this work can be continued on in a couple obvious directions. The most obvious is to tackle the second part of the problem we mentioned early in the paper. That is the case where the solution must automatically look through the document to find places that need citations. The next most obvious extension is to improve our methods for recommending citations. New ways of comparing the context of a location for a citation and all of the papers available for citations are needed. Something that would be interesting to examine is summarizing every possible citation paper and treat the summarization as another abstract to use in our pipeline. The hardest and most interesting extension
of this work would be to tie the solution in with a citation recommendation solution. We feel that would be an awesome advance in paper writing.

\subsection{Improvements}

\subsubsection{Parsing and Scraping}
Parsing is vital to the Citeopotamus system. If a context is inproperly parsed, then everything else around that citation falls apart.
A spot check of papers that perform poorly (under 10\%) reveals that many of these these papers have many incorectly parsed contexts.
Some of these errors come from an error in parsing like missing a false positive sentence ender. However, most of these errors appear to
come from difficulties scraping the paper. We believe that irregularities in the paper's HTML causes poor formatting of the resulting
plain text paper and therefore causes parsing errors.

\subsubsection{Pipeline Uncertianty}
As mentioned in Section \ref{sec:archCiter}, the further a citation moves into the pipeline the more uncertain its guessed citation becomes.
Incorporating this uncertainty into the information provided to the user can be very useful in a real-world system.

\subsection{Future Product}

\section{Related Work}\label{sec:related}
We were unable to find any related work on recommending citations given a spot for a citation to go. Unlike the problem of recommending citations for a paper. Copious amounts of work already exists on the topic. With several different approaches. One of the most common was using models to compare a paper and the rest of the dataset. Some of the models used includes translation models, probabilistic models, and several forms of LDAs.\cite{cite1, cite2, cite3} The next most common group of solutions was to use citations graphs to recommend missing citations.\cite{cite6} One group extended the notion of graphs and included metadata to help filter out results.\cite{cite4} One of the most 
interesting and unique approaches was to not look at co-citations but to look at co-accesses via logs like HTTP access records.\cite{cite7}
Another interesting approach used collaborative filtering to let communities help decide citations.\cite{cite8} For more related work in
citation recommendation we recommend the survey by McNee et al. they go into a lot of depth on the problem and common pitfalls.\cite{cite5}

\section{Conclusion}\label{sec:conclusion}

\section{Thanks}
We thanks the folks at PLOS for providing the source of our dataset. Without them this would of been a lot harder.

\bibliographystyle{acm}
\bibliography{refs}

\end{document}
